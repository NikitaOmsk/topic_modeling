{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "### Функция для чтения датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fpath):\n",
    "    data = []\n",
    "    with open(fpath, 'r') as f:\n",
    "        for line in f:\n",
    "            ex = {}\n",
    "            for subline in line[1:-2].replace('\"', '').split(', '):\n",
    "                key, value = subline.split(': ')\n",
    "                try:\n",
    "                    ex[key] = int(value)\n",
    "                except Exception:\n",
    "                    ex[key] = value\n",
    "            data.append(ex)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коллекции уже представлен обработанный датасет. Произведена лемматизация, удалены стоп-слова и знаки препинания. Поэтому попробуем использовать уже подготовленные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data('imdb/lemmatized_wo_stopwords/train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сразу посмотрим на то, как связаны label и score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0., 5100.],\n",
       "       [   0., 2284.],\n",
       "       [   0., 2420.],\n",
       "       [   0., 2696.],\n",
       "       [   0.,    0.],\n",
       "       [   0.,    0.],\n",
       "       [2496.,    0.],\n",
       "       [3009.,    0.],\n",
       "       [2263.,    0.],\n",
       "       [4732.,    0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_twos = np.zeros((10, 2))\n",
    "for s in train_data:\n",
    "    i = s['score'] - 1\n",
    "    j = s['label'] - 1\n",
    "    ones_twos[i, j] += 1\n",
    "ones_twos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что score 5 и 6 отсутствует, высоким оценкам (7-10) соответствует label 1, а низким (1-4) - 2.\n",
    "Поэтому задача определения score автоматически решает задачу определения label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на данные --- количество, среднюю и минимальную длину текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of samples: 25000\n",
      "Mean length: 117.6394\n",
      "Std: 88.88958098472509\n",
      "Minimum: 3\n",
      "10%: 45\n",
      "25%: 62\n",
      "50%: 87\n",
      "75%: 143\n",
      "90%: 233\n",
      "Maximum: 1416\n"
     ]
    }
   ],
   "source": [
    "count_data = len(train_data)\n",
    "data_length = sorted([len(sample['text'].split()) for sample in train_data])\n",
    "print('Count of samples: {}\\nMean length: {}\\nStd: {}\\nMinimum: {}\\n10%: {}\\n25%: {}\\n50%: {}\\n'\n",
    "      '75%: {}\\n90%: {}\\nMaximum: {}'.format(count_data, np.mean(data_length), np.std(data_length), \n",
    "                                             data_length[0], data_length[count_data//10], \n",
    "                                             data_length[count_data//4], data_length[count_data//2], \n",
    "                                             data_length[count_data * 3//4], data_length[count_data * 9//10], \n",
    "                                             data_length[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что разброс в данных большой. Вспоминая специфику данных -- отзывы к фильмам -- понимаем, что большинство пользователей пишут коротко, и лишь некоторые растекаются мысью по древу.\n",
    "Во всяком случае, это не твиттер с 30-40 словами, поэтому можно ожидать приемлемых результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для перевода данных в формат BigARTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data_for_bigartm(data, bigartm_data_path):\n",
    "    with open(bigartm_data_path, 'w') as f:\n",
    "        for i, sample in enumerate(data):\n",
    "            f.write('review_{} {} |@score {}\\n'.format(i, sample['text'], sample['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data_for_bigartm(data, bigartm_data_path):\n",
    "    with open(bigartm_data_path, 'w') as f:\n",
    "        for i, sample in enumerate(data):\n",
    "            f.write('review_{} {}\\n'.format(i, sample['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = train_test_split(train_data, test_size=0.2)\n",
    "prepare_train_data_for_bigartm(train_set, 'train.txt')\n",
    "prepare_test_data_for_bigartm(val_set, 'val.txt')\n",
    "train_scores = [x['score'] for x in train_set]\n",
    "val_scores = [x['score'] for x in val_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построим модель BigARTM\n",
    "\n",
    "### Сначала определим vectorizer и сохраним словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.2\n"
     ]
    }
   ],
   "source": [
    "import artm\n",
    "import pyLDAvis\n",
    "\n",
    "artm.ARTM(num_topics=1)\n",
    "print(artm.version())\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bv_train = artm.BatchVectorizer(data_path='train.txt', data_format='vowpal_wabbit', \n",
    "                                batch_size=10000, target_folder='train_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_val = artm.BatchVectorizer(data_path='val.txt', data_format='vowpal_wabbit', \n",
    "                              batch_size=10000, target_folder='val_batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = artm.Dictionary()\n",
    "dictionary.gather(data_path='train_batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализируем модель и добавим метрики качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = artm.ARTM(num_topics=100, dictionary=dictionary, class_ids={'@default_class': 1.0, '@score': 5.0})\n",
    "\n",
    "model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=15))\n",
    "model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@score'))\n",
    "model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В качестве первой попытки обучим модель без регуляризаторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.0, perplexity = 469871.375\n",
      "Iteration 1: sparsity = 0.0, perplexity = 17075.998046875\n",
      "Iteration 2: sparsity = 0.0, perplexity = 10442.12109375\n",
      "Iteration 3: sparsity = 0.0, perplexity = 6890.9521484375\n",
      "Iteration 4: sparsity = 0.0, perplexity = 5574.27783203125\n",
      "Iteration 5: sparsity = 0.0, perplexity = 5052.9931640625\n",
      "Iteration 6: sparsity = 0.0, perplexity = 4818.98974609375\n",
      "Iteration 7: sparsity = 0.0, perplexity = 4706.3388671875\n",
      "Iteration 8: sparsity = 0.0, perplexity = 4654.90380859375\n",
      "Iteration 9: sparsity = 0.0, perplexity = 4637.9873046875\n",
      "Iteration 10: sparsity = 0.0024999999441206455, perplexity = 4641.61962890625\n",
      "Iteration 11: sparsity = 0.007499999832361937, perplexity = 4658.40673828125\n",
      "Iteration 12: sparsity = 0.01875000074505806, perplexity = 4682.6748046875\n",
      "Iteration 13: sparsity = 0.026249999180436134, perplexity = 4710.31591796875\n",
      "Iteration 14: sparsity = 0.04500000178813934, perplexity = 4739.27783203125\n",
      "Iteration 15: sparsity = 0.05249999836087227, perplexity = 4768.1796875\n",
      "Iteration 16: sparsity = 0.07500000298023224, perplexity = 4795.986328125\n",
      "Iteration 17: sparsity = 0.08874999731779099, perplexity = 4822.60302734375\n",
      "Iteration 18: sparsity = 0.10999999940395355, perplexity = 4848.00830078125\n",
      "Iteration 19: sparsity = 0.12999999523162842, perplexity = 4871.7158203125\n",
      "Iteration 20: sparsity = 0.14499999582767487, perplexity = 4893.90234375\n",
      "Iteration 21: sparsity = 0.16750000417232513, perplexity = 4914.810546875\n",
      "Iteration 22: sparsity = 0.19499999284744263, perplexity = 4934.5185546875\n",
      "Iteration 23: sparsity = 0.2199999988079071, perplexity = 4953.44140625\n",
      "Iteration 24: sparsity = 0.2462500035762787, perplexity = 4971.31396484375\n",
      "Iteration 25: sparsity = 0.2750000059604645, perplexity = 4988.01611328125\n",
      "Iteration 26: sparsity = 0.29499998688697815, perplexity = 5004.2177734375\n",
      "Iteration 27: sparsity = 0.3162499964237213, perplexity = 5019.9287109375\n",
      "Iteration 28: sparsity = 0.32875001430511475, perplexity = 5034.97216796875\n",
      "Iteration 29: sparsity = 0.3462499976158142, perplexity = 5049.4248046875\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    model.fit_offline(bv_train, num_collection_passes=1)\n",
    "    print('Iteration {}: sparsity = {}, perplexity = {}'.format(\\\n",
    "        i, model.score_tracker['sparsity'].value[-1], model.score_tracker['perplexity'].value[-1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      1.00      4065\n",
      "           2       1.00      1.00      1.00      1878\n",
      "           3       1.00      0.99      1.00      1927\n",
      "           4       1.00      1.00      1.00      2135\n",
      "           7       1.00      1.00      1.00      1990\n",
      "           8       1.00      1.00      1.00      2418\n",
      "           9       1.00      1.00      1.00      1832\n",
      "          10       1.00      1.00      1.00      3755\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.63      0.49      1035\n",
      "           2       0.11      0.11      0.11       406\n",
      "           3       0.11      0.11      0.11       493\n",
      "           4       0.20      0.21      0.20       561\n",
      "           7       0.24      0.17      0.19       506\n",
      "           8       0.22      0.17      0.19       591\n",
      "           9       0.13      0.08      0.10       431\n",
      "          10       0.40      0.36      0.38       977\n",
      "\n",
      "    accuracy                           0.29      5000\n",
      "   macro avg       0.23      0.23      0.22      5000\n",
      "weighted avg       0.27      0.29      0.27      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_pd = model.transform(bv_train)\n",
    "X_train = np.array([X_train_pd[i].values for i in range(len(train_scores))])\n",
    "y_train = np.array(train_scores)\n",
    "X_val_pd = model.transform(bv_val)\n",
    "X_val = np.array([X_val_pd[i].values for i in range(len(val_scores))])\n",
    "y_val = np.array(val_scores)\n",
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "results = {}\n",
    "results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "print(results['train'])\n",
    "print(results['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что с 10 итерации перплексия начинает расти -- эффект переобучения. И, ожидаемо, качество на валидации оказалось очень плохим. Значит, надо использовать регуляризаторы или сократить число итераций. Чтобы не копировать постоянно код, напишем функцию, создающую и обучающую модель. Но сначала -- 10 итераций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.0, perplexity = 471030.71875\n",
      "Iteration 1: sparsity = 0.0, perplexity = 17012.111328125\n",
      "Iteration 2: sparsity = 0.0, perplexity = 10382.5693359375\n",
      "Iteration 3: sparsity = 0.0, perplexity = 6861.544921875\n",
      "Iteration 4: sparsity = 0.0, perplexity = 5559.42578125\n",
      "Iteration 5: sparsity = 0.0, perplexity = 5047.93896484375\n",
      "Iteration 6: sparsity = 0.0, perplexity = 4821.7041015625\n",
      "Iteration 7: sparsity = 0.0, perplexity = 4712.08203125\n",
      "Iteration 8: sparsity = 0.0, perplexity = 4658.92138671875\n",
      "Iteration 9: sparsity = 0.0, perplexity = 4638.44775390625\n"
     ]
    }
   ],
   "source": [
    "model = artm.ARTM(num_topics=100, dictionary=dictionary, class_ids={'@default_class': 1.0, '@score': 5.0})\n",
    "\n",
    "model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=15))\n",
    "model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@score'))\n",
    "model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary))\n",
    "\n",
    "for i in range(10):\n",
    "    model.fit_offline(bv_train, num_collection_passes=1)\n",
    "    print('Iteration {}: sparsity = {}, perplexity = {}'.format(\\\n",
    "        i, model.score_tracker['sparsity'].value[-1], model.score_tracker['perplexity'].value[-1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.66      0.50      1002\n",
      "           2       0.15      0.12      0.13       475\n",
      "           3       0.15      0.16      0.15       489\n",
      "           4       0.19      0.16      0.17       559\n",
      "           7       0.21      0.16      0.18       499\n",
      "           8       0.24      0.18      0.21       624\n",
      "           9       0.16      0.09      0.11       453\n",
      "          10       0.41      0.37      0.39       899\n",
      "\n",
      "    accuracy                           0.29      5000\n",
      "   macro avg       0.24      0.24      0.23      5000\n",
      "weighted avg       0.27      0.29      0.27      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train_pd = model.transform(bv_train)\n",
    "X_train = np.array([X_train_pd[i].values for i in range(len(train_scores))])\n",
    "y_train = np.array(train_scores)\n",
    "X_val_pd = model.transform(bv_val)\n",
    "X_val = np.array([X_val_pd[i].values for i in range(len(val_scores))])\n",
    "y_val = np.array(val_scores)\n",
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "results = {}\n",
    "results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "print(results['train'])\n",
    "print(results['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если больше эстиматоров в решающем лесу?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.71      0.54      1002\n",
      "           2       0.16      0.09      0.12       475\n",
      "           3       0.16      0.13      0.15       489\n",
      "           4       0.23      0.20      0.21       559\n",
      "           7       0.21      0.16      0.18       499\n",
      "           8       0.26      0.25      0.25       624\n",
      "           9       0.19      0.10      0.13       453\n",
      "          10       0.41      0.46      0.43       899\n",
      "\n",
      "    accuracy                           0.32      5000\n",
      "   macro avg       0.26      0.26      0.25      5000\n",
      "weighted avg       0.29      0.32      0.30      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=20)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "results = {}\n",
    "results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "print(results['train'])\n",
    "print(results['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже лучше. А если не 20, а 50?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.75      0.56      1002\n",
      "           2       0.14      0.07      0.10       475\n",
      "           3       0.18      0.12      0.15       489\n",
      "           4       0.23      0.20      0.22       559\n",
      "           7       0.20      0.14      0.17       499\n",
      "           8       0.27      0.23      0.25       624\n",
      "           9       0.20      0.09      0.12       453\n",
      "          10       0.42      0.54      0.47       899\n",
      "\n",
      "    accuracy                           0.34      5000\n",
      "   macro avg       0.26      0.27      0.25      5000\n",
      "weighted avg       0.29      0.34      0.30      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "results = {}\n",
    "results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "print(results['train'])\n",
    "print(results['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy 0.34. Пора пробовать регуляризаторы. Испытаем три: декоррелятор на входных данных, декоррелятор на классах, сглаживатель-разреживатель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция для создания и обучения модели с регуляризаторами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_fit_model_with_regularizers(bv_train, num_topics, epochs, \n",
    "                                           tau_def, tau_score, tau_smooth, score_idx, verbose=1):\n",
    "    model = artm.ARTM(num_topics=num_topics, dictionary=dictionary, \n",
    "                      class_ids={'@default_class': 1.0, '@score': score_idx})\n",
    "    model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=15))\n",
    "    model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@score'))\n",
    "    model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary))\n",
    "    \n",
    "    if tau_def > 0:\n",
    "        model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_default', \n",
    "                                                               tau=tau_def,\n",
    "                                                               class_ids=['@default_class']))\n",
    "    if tau_score > 0:\n",
    "        model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_score', \n",
    "                                                               tau=tau_score,\n",
    "                                                               class_ids=['@score']))\n",
    "    if tau_smooth > 0:\n",
    "        model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='smoothing', \n",
    "                                                               tau=tau_smooth,\n",
    "                                                               class_ids=['@score']))\n",
    "    old_sparsity = -1\n",
    "    old_perplexity = -1\n",
    "    stop = 5\n",
    "    iter_done = epochs\n",
    "    for i in range(epochs):\n",
    "        model.fit_offline(bv_train, num_collection_passes=1)\n",
    "        sparsity = model.score_tracker['sparsity'].value[-1]\n",
    "        perplexity = model.score_tracker['perplexity'].value[-1]\n",
    "        if verbose == 2:\n",
    "            print('Iteration {}: sparsity = {}, perplexity = {}'.format(i, sparsity, perplexity))\n",
    "        if sparsity < old_sparsity or old_perplexity != -1 and perplexity > old_perplexity:\n",
    "            stop -= 1\n",
    "            if stop == 0:\n",
    "                iter_done = i + 1\n",
    "                break\n",
    "        else:\n",
    "            old_sparsity = sparsity\n",
    "            old_perplexity = perplexity\n",
    "    if verbose == 1:\n",
    "        print('Iteration {}: sparsity = {}, perplexity = {}'.format(iter_done, sparsity, perplexity))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция, которая для заданных параметров обучает модель и оценивает её на валидационных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hyperparameters(bv_train, bv_val, scores_train, scores_val, classifier, \n",
    "                             num_topics, epochs, tau_def, tau_score, tau_smooth, score_idx, verbose=1):\n",
    "    model = create_and_fit_model_with_regularizers(bv_train, num_topics, epochs, \n",
    "                                                   tau_def, tau_score, tau_smooth, score_idx, verbose)\n",
    "    X_train_pd = model.transform(bv_train)\n",
    "    X_train = np.array([X_train_pd[i].values for i in range(len(scores_train))])\n",
    "    y_train = np.array(scores_train)\n",
    "    X_val_pd = model.transform(bv_val)\n",
    "    X_val = np.array([X_val_pd[i].values for i in range(len(scores_val))])\n",
    "    y_val = np.array(scores_val)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    y_val_pred = classifier.predict(X_val)\n",
    "    results = {}\n",
    "    results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "    results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "    return results['train'], results['val'], model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем первый регуляризатор с $\\tau=10000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15: sparsity = 0.036249998956918716, perplexity = 4862.2568359375\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 10000, 0, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.63      0.48      1002\n",
      "           2       0.15      0.14      0.15       475\n",
      "           3       0.13      0.13      0.13       489\n",
      "           4       0.17      0.16      0.16       559\n",
      "           7       0.18      0.12      0.15       499\n",
      "           8       0.20      0.16      0.18       624\n",
      "           9       0.18      0.10      0.13       453\n",
      "          10       0.41      0.40      0.40       899\n",
      "\n",
      "    accuracy                           0.28      5000\n",
      "   macro avg       0.23      0.23      0.22      5000\n",
      "weighted avg       0.26      0.28      0.26      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не улучшает. Попробуем $\\tau=100000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15: sparsity = 0.026249999180436134, perplexity = 4502.76025390625\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 100000, 0, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      0.99      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.37      0.67      0.48      1002\n",
      "           2       0.16      0.11      0.13       475\n",
      "           3       0.13      0.12      0.12       489\n",
      "           4       0.19      0.16      0.18       559\n",
      "           7       0.15      0.09      0.11       499\n",
      "           8       0.24      0.18      0.21       624\n",
      "           9       0.20      0.10      0.13       453\n",
      "          10       0.40      0.41      0.41       899\n",
      "\n",
      "    accuracy                           0.29      5000\n",
      "   macro avg       0.23      0.23      0.22      5000\n",
      "weighted avg       0.26      0.29      0.26      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По крайней мере, перплексия уменьшается. Попробуем взять $\\tau=1000000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24: sparsity = 0.22750000655651093, perplexity = 4600.70947265625\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 1000000, 0, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.99      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.35      0.73      0.48      1002\n",
      "           2       0.07      0.02      0.03       475\n",
      "           3       0.13      0.06      0.08       489\n",
      "           4       0.15      0.07      0.10       559\n",
      "           7       0.16      0.14      0.15       499\n",
      "           8       0.19      0.15      0.17       624\n",
      "           9       0.11      0.05      0.07       453\n",
      "          10       0.34      0.44      0.38       899\n",
      "\n",
      "    accuracy                           0.28      5000\n",
      "   macro avg       0.19      0.21      0.18      5000\n",
      "weighted avg       0.22      0.28      0.23      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хватит этой боли. Первый регуляризатор ничего не улучшает, потому летит в топку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15: sparsity = 0.2737500071525574, perplexity = 4669.71826171875\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 0, 10, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.65      0.49      1002\n",
      "           2       0.15      0.13      0.14       475\n",
      "           3       0.14      0.14      0.14       489\n",
      "           4       0.18      0.16      0.17       559\n",
      "           7       0.17      0.14      0.15       499\n",
      "           8       0.22      0.18      0.20       624\n",
      "           9       0.17      0.10      0.12       453\n",
      "          10       0.38      0.33      0.36       899\n",
      "\n",
      "    accuracy                           0.28      5000\n",
      "   macro avg       0.23      0.23      0.22      5000\n",
      "weighted avg       0.26      0.28      0.26      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21: sparsity = 0.9750000238418579, perplexity = 6804.9208984375\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 0, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      0.99      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.38      0.76      0.50      1002\n",
      "           2       0.16      0.04      0.06       475\n",
      "           3       0.20      0.11      0.14       489\n",
      "           4       0.22      0.20      0.21       559\n",
      "           7       0.25      0.18      0.21       499\n",
      "           8       0.24      0.14      0.18       624\n",
      "           9       0.22      0.11      0.15       453\n",
      "          10       0.39      0.49      0.44       899\n",
      "\n",
      "    accuracy                           0.32      5000\n",
      "   macro avg       0.26      0.25      0.24      5000\n",
      "weighted avg       0.28      0.32      0.28      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перплексия выросла, но качество классификации улучшилось, причём заметно. Попробуем закрепить, взяв больше деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 21: sparsity = 0.9750000238418579, perplexity = 6804.9208984375\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=30)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 0, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.77      0.52      1002\n",
      "           2       0.20      0.04      0.06       475\n",
      "           3       0.20      0.10      0.13       489\n",
      "           4       0.24      0.22      0.23       559\n",
      "           7       0.26      0.15      0.19       499\n",
      "           8       0.27      0.14      0.19       624\n",
      "           9       0.27      0.11      0.16       453\n",
      "          10       0.40      0.62      0.48       899\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.30      0.35      0.29      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.35. Хоть что-то. Попробуем присоединить первый регуляризатор ко второму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6724.1650390625\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=30)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 10000, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.78      0.53      1002\n",
      "           2       0.23      0.04      0.07       475\n",
      "           3       0.20      0.11      0.14       489\n",
      "           4       0.23      0.22      0.23       559\n",
      "           7       0.23      0.16      0.19       499\n",
      "           8       0.28      0.16      0.21       624\n",
      "           9       0.27      0.12      0.17       453\n",
      "          10       0.42      0.59      0.49       899\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.35      0.30      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy такое же, а f1 мера чуть выросла. Пытаемся дальше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12: sparsity = 0.9700000286102295, perplexity = 6616.259765625\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=30)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 100000, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.69      0.53      1002\n",
      "           2       0.12      0.07      0.08       475\n",
      "           3       0.17      0.14      0.16       489\n",
      "           4       0.26      0.18      0.21       559\n",
      "           7       0.22      0.18      0.20       499\n",
      "           8       0.20      0.12      0.16       624\n",
      "           9       0.17      0.12      0.14       453\n",
      "          10       0.38      0.51      0.44       899\n",
      "\n",
      "    accuracy                           0.32      5000\n",
      "   macro avg       0.24      0.25      0.24      5000\n",
      "weighted avg       0.27      0.32      0.28      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кажется, не очень полезная затея. Пробуем взять 200 тем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 30: sparsity = 1.0, perplexity = 8588.4248046875\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=30)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    200, 30, 10000, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.78      0.54      1002\n",
      "           2       0.15      0.05      0.08       475\n",
      "           3       0.16      0.11      0.13       489\n",
      "           4       0.19      0.17      0.18       559\n",
      "           7       0.21      0.11      0.14       499\n",
      "           8       0.23      0.18      0.20       624\n",
      "           9       0.21      0.09      0.13       453\n",
      "          10       0.42      0.53      0.47       899\n",
      "\n",
      "    accuracy                           0.33      5000\n",
      "   macro avg       0.25      0.25      0.23      5000\n",
      "weighted avg       0.28      0.33      0.28      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12: sparsity = 0.9212499856948853, perplexity = 3739.251708984375\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=30)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    200, 20, 100000, 10, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.68      0.50      1002\n",
      "           2       0.13      0.07      0.09       475\n",
      "           3       0.18      0.09      0.12       489\n",
      "           4       0.19      0.16      0.17       559\n",
      "           7       0.17      0.18      0.18       499\n",
      "           8       0.22      0.17      0.19       624\n",
      "           9       0.15      0.09      0.11       453\n",
      "          10       0.38      0.46      0.42       899\n",
      "\n",
      "    accuracy                           0.30      5000\n",
      "   macro avg       0.23      0.24      0.22      5000\n",
      "weighted avg       0.26      0.30      0.27      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекордно низкая перплексия, а классификация - совсем не на уровне. Больше деревьев!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12: sparsity = 0.9212499856948853, perplexity = 3739.251708984375\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    200, 20, 100000, 10, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.72      0.52      1002\n",
      "           2       0.15      0.07      0.10       475\n",
      "           3       0.16      0.08      0.11       489\n",
      "           4       0.22      0.15      0.18       559\n",
      "           7       0.18      0.18      0.18       499\n",
      "           8       0.25      0.19      0.21       624\n",
      "           9       0.13      0.07      0.09       453\n",
      "          10       0.41      0.52      0.46       899\n",
      "\n",
      "    accuracy                           0.32      5000\n",
      "   macro avg       0.24      0.25      0.23      5000\n",
      "weighted avg       0.27      0.32      0.28      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Попробуем третий регуляризатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15: sparsity = 0.054375000298023224, perplexity = 3864.0283203125\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    200, 20, 0, 0, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.73      0.51      1002\n",
      "           2       0.15      0.05      0.08       475\n",
      "           3       0.12      0.09      0.11       489\n",
      "           4       0.25      0.22      0.23       559\n",
      "           7       0.19      0.16      0.18       499\n",
      "           8       0.23      0.17      0.20       624\n",
      "           9       0.22      0.10      0.13       453\n",
      "          10       0.36      0.42      0.38       899\n",
      "\n",
      "    accuracy                           0.31      5000\n",
      "   macro avg       0.24      0.24      0.23      5000\n",
      "weighted avg       0.27      0.31      0.27      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не хуже и не лучше. Первый к нему, и второй заодно - в их лучшей конфигурации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20: sparsity = 1.0, perplexity = 8821.5732421875\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    200, 20, 10000, 100, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.79      0.54      1002\n",
      "           2       0.17      0.05      0.08       475\n",
      "           3       0.18      0.10      0.13       489\n",
      "           4       0.25      0.21      0.23       559\n",
      "           7       0.24      0.11      0.15       499\n",
      "           8       0.25      0.20      0.22       624\n",
      "           9       0.29      0.09      0.14       453\n",
      "          10       0.41      0.60      0.48       899\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6724.1650390625\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "tr, v, _ = evaluate_hyperparameters(bv_train, bv_val, train_scores, val_scores, classifier, \n",
    "                                    100, 30, 10000, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4098\n",
      "           2       1.00      1.00      1.00      1809\n",
      "           3       1.00      1.00      1.00      1931\n",
      "           4       1.00      1.00      1.00      2137\n",
      "           7       1.00      1.00      1.00      1997\n",
      "           8       1.00      1.00      1.00      2385\n",
      "           9       1.00      1.00      1.00      1810\n",
      "          10       1.00      1.00      1.00      3833\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.80      0.54      1002\n",
      "           2       0.20      0.03      0.06       475\n",
      "           3       0.17      0.08      0.11       489\n",
      "           4       0.22      0.22      0.22       559\n",
      "           7       0.24      0.14      0.18       499\n",
      "           8       0.25      0.12      0.16       624\n",
      "           9       0.28      0.11      0.16       453\n",
      "          10       0.42      0.65      0.51       899\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.35      0.29      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy 0.35. Рекордное значение; видимо, на этих данных такими средствами большего не достичь. Выделим модель из кода и посмотрим на результаты ещё раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекордная модель после долгих мучений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 тем\n",
    "\n",
    "Выполнено 20 итераций\n",
    "\n",
    "Коэффициенты регуляризаторов 10000, 100 и 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469871.375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12513.50390625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9772.8193359375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8346.1923828125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7529.2841796875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7127.4921875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6908.1142578125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6784.4072265625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6709.66552734375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6664.8095703125\n",
      "Iteration 10: sparsity = 0.9700000286102295, perplexity = 6641.49169921875\n",
      "Iteration 11: sparsity = 0.9700000286102295, perplexity = 6634.55859375\n",
      "Iteration 12: sparsity = 0.9700000286102295, perplexity = 6639.9619140625\n",
      "Iteration 13: sparsity = 0.9700000286102295, perplexity = 6654.83447265625\n",
      "Iteration 14: sparsity = 0.9700000286102295, perplexity = 6677.1064453125\n",
      "Iteration 15: sparsity = 0.9700000286102295, perplexity = 6704.44580078125\n",
      "Iteration 16: sparsity = 0.9700000286102295, perplexity = 6735.58642578125\n",
      "Iteration 17: sparsity = 0.9700000286102295, perplexity = 6768.7216796875\n",
      "Iteration 18: sparsity = 0.9700000286102295, perplexity = 6803.0166015625\n",
      "Iteration 19: sparsity = 0.9712499976158142, perplexity = 6837.55224609375\n"
     ]
    }
   ],
   "source": [
    "model = artm.ARTM(num_topics=100, dictionary=dictionary, \n",
    "                  class_ids={'@default_class': 1.0, '@score': 5})\n",
    "model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=100))\n",
    "model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@score'))\n",
    "model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary))\n",
    "\n",
    "model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_default', \n",
    "                                                       tau=10000, class_ids=['@default_class']))\n",
    "model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_score', \n",
    "                                                       tau=100, class_ids=['@score']))\n",
    "\n",
    "for i in range(20):\n",
    "    model.fit_offline(bv_train, num_collection_passes=1)\n",
    "    sparsity = model.score_tracker['sparsity'].value[-1]\n",
    "    perplexity = model.score_tracker['perplexity'].value[-1]\n",
    "    print('Iteration {}: sparsity = {}, perplexity = {}'.format(i, sparsity, perplexity))\n",
    "    \n",
    "X_train_pd = model.transform(bv_train)\n",
    "X_train = np.array([X_train_pd[i].values for i in range(len(train_scores))])\n",
    "y_train = np.array(train_scores)\n",
    "X_val_pd = model.transform(bv_val)\n",
    "X_val = np.array([X_val_pd[i].values for i in range(len(val_scores))])\n",
    "y_val = np.array(val_scores)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "results = {}\n",
    "results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "results['val'] = classification_report(y_val, y_val_pred, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4065\n",
      "           2       1.00      1.00      1.00      1878\n",
      "           3       1.00      1.00      1.00      1927\n",
      "           4       1.00      1.00      1.00      2135\n",
      "           7       1.00      1.00      1.00      1990\n",
      "           8       1.00      1.00      1.00      2418\n",
      "           9       1.00      1.00      1.00      1832\n",
      "          10       1.00      1.00      1.00      3755\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.74      0.55      1035\n",
      "           2       0.19      0.10      0.13       406\n",
      "           3       0.18      0.11      0.14       493\n",
      "           4       0.28      0.27      0.27       561\n",
      "           7       0.27      0.18      0.22       506\n",
      "           8       0.24      0.16      0.19       591\n",
      "           9       0.20      0.08      0.12       431\n",
      "          10       0.44      0.57      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results['train'])\n",
    "print(results['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на матрицу предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[768.  65.  58.  61.   0.   0.  11.   6.   5.  61.]\n",
      " [236.  40.  44.  35.   0.   0.  12.   8.   2.  29.]\n",
      " [233.  41.  53.  95.   0.   0.  21.  11.   1.  38.]\n",
      " [190.  32.  59. 152.   0.   0.  38.  26.   9.  55.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [ 80.  11.  25.  69.   0.   0.  92.  68.  18. 143.]\n",
      " [ 71.   9.  14.  66.   0.   0.  87.  94.  47. 203.]\n",
      " [ 59.   2.  11.  26.   0.   0.  37.  65.  36. 195.]\n",
      " [122.   7.  24.  47.   0.   0.  45. 116.  58. 558.]]\n"
     ]
    }
   ],
   "source": [
    "val_matrix = np.zeros((10, 10))\n",
    "for tru, prd in zip(y_val, y_val_pred):\n",
    "    val_matrix[tru-1,prd-1] += 1\n",
    "print(val_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2162.,  333.],\n",
       "       [ 643., 1862.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[val_matrix[:4, :4].sum(), val_matrix[:4, 6:].sum()], [val_matrix[6:, :4].sum(), val_matrix[6:, 6:].sum()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Из таблицы видно, что всё не так плохо, как казалось: в абсолютном большинстве случаев положительные оценки классифицированы как положительные, а отрицательные как отрицательные. А отличить единицу от двойки -- с этим не каждый преподаватель справится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Но всё-таки сравним с обычным методом tf-idf без всякого тематического моделирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [x['text'] for x in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = vectorizer.transform([x['text'] for x in val_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=50)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=50)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = classifier.predict(X_train)\n",
    "y_val_pred = classifier.predict(X_val)\n",
    "results = {}\n",
    "results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "results['val'] = classification_report(y_val, y_val_pred, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00      4065\n",
      "           2       1.00      1.00      1.00      1878\n",
      "           3       1.00      1.00      1.00      1927\n",
      "           4       1.00      1.00      1.00      2135\n",
      "           7       1.00      1.00      1.00      1990\n",
      "           8       1.00      1.00      1.00      2418\n",
      "           9       1.00      1.00      1.00      1832\n",
      "          10       1.00      1.00      1.00      3755\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      1.00      1.00     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.85      0.53      1035\n",
      "           2       0.17      0.01      0.03       406\n",
      "           3       0.24      0.04      0.06       493\n",
      "           4       0.31      0.09      0.13       561\n",
      "           7       0.24      0.07      0.10       506\n",
      "           8       0.28      0.14      0.18       591\n",
      "           9       0.26      0.02      0.04       431\n",
      "          10       0.37      0.76      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.25      0.20      5000\n",
      "weighted avg       0.30      0.36      0.27      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results['train'])\n",
    "print(results['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[879.   6.   9.  12.   0.   0.   5.   7.   1. 116.]\n",
      " [317.   6.   3.  15.   0.   0.   5.   6.   0.  54.]\n",
      " [346.   8.  18.  18.   0.   0.   7.  13.   2.  81.]\n",
      " [286.   7.  22.  48.   0.   0.  21.  29.   7. 141.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [123.   5.  12.  22.   0.   0.  33.  61.   6. 244.]\n",
      " [108.   1.   5.  19.   0.   0.  25.  81.   7. 345.]\n",
      " [ 71.   2.   3.   6.   0.   0.  19.  36.  10. 284.]\n",
      " [132.   0.   4.  15.   0.   0.  21.  55.   6. 744.]]\n"
     ]
    }
   ],
   "source": [
    "val_matrix = np.zeros((10, 10))\n",
    "for tru, prd in zip(y_val, y_val_pred):\n",
    "    val_matrix[tru-1,prd-1] += 1\n",
    "print(val_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2000.,  495.],\n",
       "       [ 528., 1977.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[val_matrix[:4, :4].sum(), val_matrix[:4, 6:].sum()], [val_matrix[6:, :4].sum(), val_matrix[6:, 6:].sum()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy не отличается, а вот f1-score у предсказаний BigARTM лучше. Переходим к основным экспериментам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно добавить в коллекцию виртуальные документы, в каждом -- n лучших токенов из одной темы. И посмотреть, улучшится ли качество решения задачи классификации. Естественно, у виртуальных документов нет и не может быть никакой метки классификации. Поэтому будем присваивать им фиктивную метку 0 и добавлять их только при обучении тематической модели, а не при решении задачи классификации с помощью случайного решающего леса.\n",
    "\n",
    "Для начала посмотрим на темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anna', 'evening', 'ann', 'simon', 'matthau', 'oscar', 'room', 'walter', 'neil', 'two', 'felix', 'burns', 'play', 'griffith', 'couple', 'busy', 'divorce', 'three', 'lemmon', 'daughter', 'deanna', 'francis', 'scene', 'old', 'young', 'lewis', 'claire', 'harris', 'cassidy', 'spice', 'george', 'die', 'work', 'durbin', 'khouri', 'boys', 'brady', 'madison', 'year', 'together', 'wife', 'forbidden', 'become', 'friend', 'stuart', 'woman', 'willy', 'vanessa', 'al', 'toni', 'lila', 'willie', 'buddy', 'benjamin', 'alicia', 'redgrave', 'mother', 'apartment', 'man', 'wedding', 'jack', 'count', 'later', 'odd', 'collette', 'dani', 'shooting', 'live', 'home', 'sunshine', 'soon', 'create', 'dw', 'another', 'patrick', 'danes', 'robot', 'screenplay', 'star', 'streep', 'meet', 'kirkland', 'first', 'course', 'bring', 'milland', 'last', 'court', 'meryl', 'stage', 'nielsen', 'jordan', 'christensen', 'marry', 'feature', 'lie', 'mistake', 'touch', 'around', 'rich']\n",
      "['bad', 'guy', 'boring', 'get', 'really', 'watch', 'worst', 'avoid', 'like', 'look', 'wow', 'want', 'walk', 'good', 'end', 'believe', 'back', 'thing', 'act', 'try', 'corny', 'cool', 'cant', 'go', 'sit', 'start', 'script', 'ok', 'people', 'even', 'spoiler', 'cost', 'one', 'ever', 'save', 'everything', 'big', 'almost', 'bore', 'kind', 'acting', 'awful', 'scene', 'fake', 'finish', 'could', 'next', 'actor', 'hard', 'sound', 'care', 'head', 'plot', 'actually', 'oh', 'right', 'take', 'laughable', 'voice', 'yes', 'possible', 'dub', 'first', 'call', 'idea', 'hear', 'plague', 'rent', 'thank', 'fall', 'weird', 'long', 'bottom', 'news', 'category', 'see', 'nothing', 'wrong', 'describe', 'word', 'okay', 'dialogue', 'time', 'completely', 'taste', 'stop', 'stinker', 'die', 'guess', 'hey', 'day', 'say', 'half', 'reason', 'beat', 'pretty', 'buy', 'away', 'madsen', 'wait']\n",
      "['original', 'sequel', 'monster', 'camp', 'rescue', 'mountain', 'ii', 'monkey', 'lion', 'return', 'first', 'make', 'go', 'scene', 'sea', 'helicopter', 'new', 'rocket', 'cg', 'back', 'part', 'rambo', 'take', 'use', 'stallone', 'cliffhanger', 'around', 'timon', 'hero', 'shoot', 'little', 'action', 'motion', 'die', 'disney', 'ariel', 'thunderbirds', 'sequence', 'away', 'without', 'mermaid', 'sound', 'launch', 'bullet', 'scarecrow', 'hills', 'tunnel', 'pumbaa', 'franchise', 'voice', 'fun', 'exciting', 'fire', 'forget', 'escape', 'place', 'melody', 'find', 'one', 'run', 'call', 'whole', 'rocky', 'set', 'group', 'never', 'want', 'brain', 'mention', 'attack', 'ursula', 'much', 'near', 'yes', 'come', 'actually', 'add', 'arrow', 'right', 'penelope', 'second', 'trap', 'manage', 'muscle', 'course', 'entertaining', 'truck', 'two', 'could', 'ridiculous', 'another', 'uncle', 'even', 'happen', 'basically', 'simba', 'kidnap', 'matrix', 'camper', 'eric']\n",
      "['agent', 'secret', 'sean', 'guard', 'chase', 'hunter', 'service', 'bourne', 'security', 'kim', 'drive', 'eva', 'irish', 'assassin', 'cia', 'matt', 'mall', 'president', 'thug', 'man', 'damon', 'frankie', 'wendigo', 'gypo', 'basinger', 'boom', 'deer', 'goldie', 'ultimatum', 'take', 'longoria', 'hawn', 'ireland', 'desperate', 'mclaglen', 'housewife', 'ford', 'john', 'identity', 'thriller', 'pete', 'friend', 'miles', 'informer', 'run', 'car', 'douglas', 'shopping', 'home', 'assassination', 'della', 'two', 'betrayal', 'drew', 'kill', 'turn', 'sutherland', 'pound', 'victor', 'george', 'work', 'james', 'katie', 'away', 'supremacy', 'shoot', 'girlfriend', 'control', 'sunny', 'name', 'meyer', 'play', 'best', 'skater', 'night', 'first', 'four', 'whole', 'agency', 'go', 'ira', 'would', 'save', 'parking', 'executive', 'protocol', 'connery', 'place', 'betray', 'track', 'involve', 'member', 'fawcett', 'tell', 'possibly', 'order', 'republican', 'toolbox', 'leave', 'bounty']\n",
      "['indeed', 'beauty', 'artist', 'nature', 'work', 'artistic', 'art', 'epic', 'masterpiece', 'narration', 'beautifully', 'well', 'rarely', 'haunting', 'creator', 'appreciate', 'empty', 'tremendous', 'presentation', 'without', 'poetry', 'skull', 'chinese', 'create', 'piece', 'wang', 'photograph', 'second', 'achieve', 'rhythm', 'write', 'integrity', 'truly', 'inspire', 'vision', 'picture', 'first', 'light', 'goldsworthy', 'lang', 'sublime', 'craft', 'pearl', 'poem', 'criticism', 'direct', 'grass', 'form', 'vonnegut', 'talent', 'beautiful', 'receive', 'allow', 'bring', 'gift', 'true', 'masterfully', 'american', 'brilliantly', 'muni', 'triumph', 'glorious', 'production', 'painting', 'rainer', 'sound', 'humanity', 'comment', 'kriemhild', 'preserve', 'act', 'view', 'translate', 'world', 'lung', 'become', 'oscar', 'breathtaking', 'example', 'wife', 'pure', 'cinematography', 'collection', 'superb', 'natural', 'rank', 'perfectly', 'must', 'cinematic', 'screenplay', 'method', 'uncle', 'etc', 'result', 'special', 'expression', 'creative', 'script', 'within', 'study']\n",
      "['van', 'ray', 'howard', 'price', 'lugosi', 'elvira', 'ha', 'vincent', 'universal', 'karloff', 'mildred', 'play', 'philip', 'davis', 'dr', 'bela', 'damme', 'shaw', 'rukh', 'colman', 'carey', 'leslie', 'boris', 'phillip', 'role', 'find', 'man', 'w', 'also', 'ronald', 'soldier', 'waitress', 'later', 'star', 'bette', 'wife', 'john', 'performance', 'daniels', 'become', 'bondage', 'obrien', 'tell', 'town', 'mistress', 'dark', 'medical', 'two', 'othello', 'dee', 'cyborg', 'aunt', 'peterson', 'bake', 'woman', 'gus', 'year', 'help', 'steal', 'kristofferson', 'cassandra', 'include', 'use', 'career', 'mad', 'start', 'frances', 'sherry', 'finally', 'double', 'julie', 'fall', 'cockney', 'uncle', 'burn', 'actress', 'invisible', 'new', 'back', 'snafu', 'kind', 'already', 'even', 'sant', 'sheppard', 'seth', 'due', 'lose', 'daughter', 'robert', 'kris', 'fail', 'old', 'take', 'former', 'another', 'olen', 'whose', 'among', 'work']\n",
      "['henry', 'fool', 'segment', 'che', 'grim', 'part', 'parker', 'fay', 'mummy', 'hartley', 'olivier', 'jamie', 'soderbergh', 'foxx', 'jeremy', 'revolutionary', 'louise', 'two', 'posey', 'del', 'cypher', 'hal', 'brooks', 'director', 'give', 'guevara', 'natali', 'castro', 'tell', 'toro', 'style', 'revolution', 'andrew', 'corporate', 'identity', 'bait', 'liu', 'much', 'espionage', 'morgan', 'guerrilla', 'fall', 'hero', 'cuba', 'set', 'screen', 'bolivia', 'oberon', 'second', 'take', 'turn', 'kate', 'morse', 'feel', 'vincenzo', 'company', 'louis', 'ichi', 'one', 'vignette', 'struggle', 'without', 'feature', 'fiancé', 'new', 'europe', 'work', 'gruner', 'fuqua', 'trancers', 'another', 'sense', 'poor', 'become', 'project', 'audience', 'narrative', 'benicio', 'lady', 'logan', 'goldblum', 'final', 'simon', 'ashley', 'steven', 'man', 'romantic', 'choose', 'try', 'northam', 'shoot', 'image', 'mere', 'enough', 'cuban', 'irons', 'similar', 'whistler', 'assume', 'spy']\n",
      "['camera', 'shot', 'angle', 'technique', 'witch', 'work', 'seem', 'use', 'closeup', 'close', 'blair', 'chaos', 'beast', 'glenn', 'coffee', 'shaky', 'digital', 'detail', 'explosion', 'end', 'audience', 'andre', 'clothing', 'scene', 'train', 'zoom', 'front', 'two', 'open', 'inside', 'feel', 'yet', 'cruella', 'handheld', 'grainy', 'trick', 'field', 'shoot', 'around', 'edgy', 'lighting', 'full', 'make', 'cal', 'movement', 'like', 'across', 'follow', 'enough', 'finally', 'glowing', 'behaviour', 'random', 'draw', 'everything', 'paperhouse', 'someone', 'rather', 'moment', 'hand', 'thing', 'leave', 'director', 'main', 'appear', 'entirety', 'behind', 'kind', 'butterfly', 'sense', 'run', 'fantasy', 'burke', 'project', 'back', 'style', 'flash', 'probably', 'look', 'score', 'assume', 'effect', 'focus', 'set', 'cure', 'seat', 'mumble', 'even', 'whilst', 'give', 'reality', 'head', 'way', 'hold', 'ebert', 'jump', 'box', 'catch', 'perhaps', 'puppy']\n",
      "['animation', 'ring', 'lord', 'atmosphere', 'creepy', 'eerie', 'bakshi', 'horror', 'modesty', 'victor', 'story', 'animated', 'gothic', 'peter', 'castle', 'voice', 'ralph', 'leonard', 'spooky', 'renaissance', 'cushing', 'tale', 'dark', 'steele', 'segment', 'lee', 'tarantino', 'look', 'vargas', 'create', 'poe', 'jackson', 'far', 'fan', 'genre', 'anthology', 'feature', 'blood', 'blaise', 'terror', 'elliott', 'christopher', 'macabre', 'beautiful', 'wax', 'pertwee', 'visual', 'four', 'edgar', 'style', 'character', 'futuristic', 'john', 'raising', 'sweet', 'fear', 'quentin', 'use', 'mysterious', 'incredibly', 'score', 'ingrid', 'writer', 'highly', 'name', 'lordi', 'tolkien', 'city', 'barbara', 'lotr', 'vampire', 'pitt', 'comic', 'also', 'night', 'set', 'version', 'star', 'crypt', 'english', 'atmospheric', 'second', 'faye', 'element', 'quite', 'haunt', 'chloe', 'provide', 'darkly', 'bring', 'live', 'old', 'gandalf', 'cloak', 'creation', 'museum', 'miss', 'denholm', 'appear', 'noir']\n",
      "['love', 'dvd', 'song', 'soundtrack', 'buy', 'band', 'sing', 'release', 'horse', 'voice', 'hear', 'available', 'singer', 'enjoy', 'record', 'purchase', 'fan', 'u', 'find', 'streisand', 'star', 'like', 'get', 'metal', 'never', 'come', 'look', 'fall', 'album', 'special', 'collection', 'hit', 'yes', 'also', 'well', 'watch', 'lose', 'cd', 'around', 'year', 'site', 'make', 'sue', 'finally', 'sale', 'tour', 'full', 'still', 'people', 'know', 'many', 'thumb', 'sound', 'late', 'barbra', 'remember', 'sweet', 'miss', 'anyone', 'everyone', 'could', 'much', 'back', 'call', 'night', 'name', 'heavy', 'friend', 'want', 'listen', 'catchy', 'fun', 'especially', 'although', 'singing', 'recording', 'old', 'understand', 'set', 'include', 'perform', 'stuff', 'part', 'ebay', 'probably', 'need', 'anyways', 'use', 'favorite', 'opening', 'huge', 'edition', 'web', 'big', 'contain', 'break', 'originally', 'click', 'classic', 'tell']\n",
      "['good', 'film', 'story', 'also', 'one', 'well', 'pretty', 'really', 'much', 'make', 'nice', 'get', 'though', 'like', 'look', 'give', 'still', 'little', 'scene', 'job', 'see', 'overall', 'work', 'play', 'expect', 'lot', 'fun', 'come', 'entertaining', 'surprise', 'plot', 'seem', 'many', 'cast', 'although', 'great', 'two', 'interesting', 'time', 'however', 'actor', 'actually', 'find', 'way', 'probably', 'could', 'character', 'somewhat', 'better', 'rest', 'fact', 'part', 'take', 'together', 'may', 'always', 'go', 'bit', 'never', 'worth', 'thing', 'first', 'star', 'enough', 'enjoy', 'enjoyable', 'think', 'director', 'know', 'rather', 'man', 'year', 'help', 'put', 'keep', 'woman', 'fairly', 'fan', 'lead', 'might', 'flaw', 'decent', 'solid', 'quite', 'usual', 'feel', 'say', 'actress', 'try', 'problem', 'twist', 'watch', 'course', 'hard', 'last', 'entertainment', 'line', 'acting', 'become', 'role']\n",
      "['kill', 'death', 'blood', 'scream', 'slasher', 'victim', 'night', 'die', 'horror', 'body', 'scene', 'torture', 'gore', 'killing', 'bloody', 'cut', 'end', 'killer', 'one', 'sick', 'get', 'head', 'gory', 'start', 'halloween', 'dead', 'stab', 'woman', 'knife', 'psycho', 'revenge', 'flick', 'eye', 'know', 'nasty', 'murder', 'stalk', 'around', 'creepy', 'also', 'gruesome', 'girl', 'try', 'slaughter', 'scare', 'friday', 'burn', 'survive', 'stomach', 'escape', 'prom', 'later', 'chair', 'alive', 'boyfriend', 'take', 'people', 'pig', 'fake', 'gut', 'twist', 'disturbing', 'several', 'friend', 'everyone', 'turn', 'group', 'shoot', 'mask', 'come', 'splatter', 'course', 'begin', 'throat', 'room', 'flesh', 'skin', 'scary', 'cannibal', 'use', 'reason', 'slash', 'genre', 'set', 'sadistic', 'butcher', 'marty', 'floor', 'young', 'finally', 'terror', 'face', 'last', 'wound', 'pick', 'witness', 'happen', 'decide', 'bed', 'fear']\n",
      "['anime', 'wing', 'sin', 'blend', 'alice', 'monk', 'circle', 'gundam', 'oz', 'mobile', 'representation', 'foolish', 'character', 'english', 'ranma', 'lead', 'end', 'meet', 'may', 'art', 'suit', 'voice', 'believe', 'retelling', 'design', 'young', 'london', 'live', 'beautiful', 'manga', 'gentlemen', 'many', 'runaway', 'virgin', 'ling', 'krige', 'style', 'arjun', 'become', 'one', 'plot', 'side', 'five', 'animation', 'two', 'must', 'fall', 'dialogue', 'check', 'hate', 'suleiman', 'heero', 'afterlife', 'know', 'right', 'actress', 'every', 'follow', 'turn', 'deep', 'mysterious', 'city', 'amick', 'score', 'detmers', 'choi', 'thai', 'force', 'detail', 'social', 'course', 'monastery', 'teenage', 'sacrifice', 'stephen', 'local', 'tsing', 'beyond', 'appear', 'subtitle', 'act', 'moment', 'quality', 'krause', 'easily', 'use', 'face', 'find', 'dreams', 'script', 'incredible', 'dub', 'take', 'set', 'elder', 'manu', 'anansa', 'base', 'work', 'kind']\n",
      "['house', 'ghost', 'door', 'wife', 'island', 'haunt', 'hotel', 'go', 'room', 'tree', 'woman', 'man', 'home', 'cabin', 'start', 'get', 'run', 'away', 'phone', 'stay', 'forest', 'decide', 'window', 'lisa', 'move', 'creepy', 'meet', 'open', 'find', 'friend', 'two', 'walk', 'couple', 'daughter', 'back', 'around', 'call', 'arrive', 'old', 'lock', 'leave', 'soon', 'help', 'little', 'take', 'sit', 'head', 'stick', 'happen', 'outside', 'next', 'scarecrow', 'small', 'neighbor', 'dead', 'station', 'hill', 'along', 'way', 'look', 'jump', 'owner', 'come', 'scene', 'wood', 'die', 'parker', 'later', 'one', 'appear', 'apparently', 'evil', 'use', 'supernatural', 'miss', 'build', 'night', 'mysterious', 'dark', 'begin', 'cell', 'mansion', 'hang', 'suddenly', 'steal', 'new', 'road', 'escape', 'afraid', 'finally', 'matt', 'george', 'live', 'discover', 'middle', 'attack', 'catch', 'return', 'weird', 'scare']\n",
      "['father', 'son', 'brother', 'god', 'church', 'christian', 'andy', 'jesus', 'religious', 'religion', 'faith', 'priest', 'christ', 'catholic', 'wife', 'believe', 'bible', 'hank', 'pokemon', 'man', 'work', 'christianity', 'old', 'play', 'younger', 'become', 'also', 'heaven', 'mother', 'know', 'tell', 'director', 'die', 'claim', 'name', 'belief', 'come', 'young', 'use', 'try', 'whose', 'need', 'myra', 'death', 'devil', 'robbery', 'welch', 'interview', 'without', 'demon', 'may', 'rapture', 'paxton', 'sam', 'passion', 'fact', 'turn', 'call', 'huston', 'daughter', 'bring', 'ask', 'attempt', 'story', 'another', 'preacher', 'john', 'title', 'mira', 'wrong', 'ash', 'dead', 'actor', 'argument', 'photographer', 'figure', 'west', 'help', 'present', 'sorvino', 'bill', 'marry', 'world', 'way', 'day', 'thing', 'people', 'commit', 'situation', 'travel', 'behind', 'return', 'believer', 'visit', 'charles', 'discover', 'find', 'decide', 'gina', 'older']\n",
      "['villain', 'evil', 'creature', 'batman', 'hero', 'animated', 'mask', 'cruise', 'sword', 'fantasy', 'attack', 'new', 'defeat', 'knight', 'freeze', 'dark', 'joker', 'conan', 'pat', 'phantasm', 'return', 'ancient', 'aaron', 'look', 'treasure', 'design', 'minion', 'b', 'use', 'reid', 'tara', 'beyond', 'slater', 'batwoman', 'lommel', 'comic', 'eye', 'transplant', 'feature', 'fire', 'formula', 'bruce', 'adventure', 'even', 'create', 'mystery', 'around', 'demon', 'robin', 'hammerhead', 'quite', 'fan', 'penguin', 'reason', 'alone', 'enough', 'animation', 'costume', 'land', 'hybrid', 'set', 'frank', 'usual', 'robertson', 'become', 'style', 'stop', 'sorcery', 'ice', 'punch', 'since', 'include', 'could', 'civilization', 'wayne', 'dialog', 'actor', 'face', 'beat', 'mad', 'kevin', 'wrong', 'another', 'barbarian', 'monster', 'apparently', 'order', 'mention', 'frazetta', 'truly', 'run', 'manage', 'wb', 'secret', 'whole', 'still', 'escape', 'weapon', 'combs', 'stuff']\n",
      "['page', 'fox', 'ms', 'ford', 'lincoln', 'lawyer', 'trial', 'fonda', 'bettie', 'young', 'photo', 'notorious', 'accuse', 'court', 'john', 'games', 'attorney', 'law', 'american', 'warming', 'watcher', 'abraham', 'mol', 'justice', 'global', 'mr', 'career', 'mamet', 'lady', 'fetish', 'bleak', 'harron', 'defense', 'history', 'mary', 'gretchen', 'portray', 'henry', 'issue', 'informative', 'crouse', 'biopic', 'capture', 'year', 'biography', 'support', 'tell', 'america', 'ada', 'trauma', 'courtroom', 'work', 'mantegna', 'treatment', 'production', 'include', 'case', 'would', 'hollywood', 'dickens', 'become', 'icon', 'offer', 'lindsay', 'appeal', 'pinup', 'sense', 'new', 'play', 'abe', 'interest', 'era', 'early', 'tribulation', 'create', 'take', 'honest', 'recreation', 'shoot', 'present', 'reenactment', 'miss', 'cromwell', 'gore', 'result', 'talented', 'serling', 'fact', 'problem', 'fifty', 'screen', 'tyson', 'fine', 'judge', 'director', 'false', 'bondage', 'famous', 'direct', 'performance']\n",
      "['one', 'character', 'like', 'even', 'predictable', 'plot', 'much', 'could', 'get', 'time', 'really', 'suppose', 'way', 'nothing', 'give', 'look', 'story', 'seem', 'make', 'line', 'annoying', 'actor', 'try', 'go', 'script', 'act', 'end', 'come', 'thing', 'scene', 'good', 'turn', 'boring', 'two', 'play', 'happen', 'point', 'watch', 'care', 'around', 'least', 'need', 'little', 'actually', 'would', 'problem', 'know', 'dialogue', 'take', 'well', 'work', 'something', 'feel', 'another', 'pointless', 'start', 'sure', 'acting', 'first', 'maybe', 'main', 'better', 'every', 'cast', 'idea', 'guess', 'never', 'worse', 'dull', 'fact', 'instead', 'leave', 'movie', 'might', 'think', 'write', 'lame', 'reason', 'except', 'fail', 'pathetic', 'wrong', 'long', 'enough', 'oh', 'hard', 'appear', 'cliché', 'pretty', 'almost', 'put', 'obvious', 'many', 'interesting', 'kind', 'half', 'clichés', 'drag', 'case', 'still']\n",
      "['violence', 'violent', 'native', 'exploitation', 'urban', 'cowboy', 'gratuitous', 'cliche', 'brutal', 'texas', 'omen', 'seventy', 'eighty', 'bud', 'register', 'american', 'extreme', 'language', 'lazy', 'orange', 'artsy', 'rear', 'travolta', 'man', 'redneck', 'cliched', 'title', 'askey', 'screen', 'real', 'absence', 'woman', 'brutality', 'river', 'jed', 'ultimately', 'issue', 'distinct', 'yet', 'zhang', 'director', 'massacre', 'iv', 'turn', 'use', 'without', 'wrong', 'fact', 'ironside', 'wrist', 'clockwork', 'slick', 'storyline', 'confrontation', 'train', 'nasty', 'deserve', 'soundtrack', 'take', 'shock', 'character', 'resort', 'fall', 'create', 'rate', 'winger', 'play', 'franco', 'need', 'far', 'list', 'include', 'style', 'snowy', 'hunting', 'credit', 'offer', 'opening', 'face', 'regularly', 'astonish', 'back', 'sequence', 'wife', 'dialog', 'write', 'beyond', 'least', 'across', 'debra', 'excuse', 'element', 'point', 'social', 'certainly', 'bear', 'pointless', 'fate', 'gilley', 'typical']\n",
      "['best', 'one', 'great', 'ever', 'time', 'favorite', 'amazing', 'wonderful', 'heart', 'always', 'perfect', 'true', 'every', 'watch', 'cry', 'story', 'win', 'greatest', 'make', 'give', 'actor', 'fantastic', 'award', 'soul', 'get', 'must', 'never', 'year', 'truly', 'say', 'see', 'emotion', 'deserve', 'come', 'many', 'tear', 'could', 'know', 'everyone', 'beautiful', 'picture', 'incredible', 'superb', 'wish', 'recommend', 'tell', 'much', 'thank', 'eye', 'forever', 'doubt', 'chance', 'first', 'really', 'absolutely', 'masterpiece', 'top', 'performance', 'simply', 'feel', 'sad', 'real', 'day', 'move', 'everything', 'nominate', 'live', 'simple', 'three', 'opinion', 'miss', 'talent', 'end', 'job', 'without', 'two', 'character', 'still', 'actress', 'forget', 'friend', 'line', 'deep', 'touch', 'back', 'believe', 'remember', 'alltime', 'learn', 'magnificent', 'hear', 'experience', 'talented', 'together', 'like', 'highly', 'enough', 'history', 'think', 'especially']\n",
      "['match', 'al', 'vs', 'johnny', 'pacino', 'hardy', 'bride', 'cusack', 'laurel', 'hall', 'tag', 'one', 'john', 'madonna', 'last', 'wwe', 'title', 'win', 'sellers', 'ladder', 'kennedy', 'team', 'big', 'championship', 'arkin', 'main', 'go', 'night', 'come', 'wrestling', 'seller', 'wrestler', 'hit', 'take', 'political', 'end', 'put', 'godfather', 'would', 'duff', 'back', 'stan', 'get', 'fan', 'fu', 'name', 'manchu', 'hand', 'retain', 'event', 'many', 'dangerously', 'two', 'cena', 'year', 'city', 'wrestlemania', 'michaels', 'ring', 'short', 'defeat', 'spot', 'point', 'line', 'danny', 'benoit', 'brand', 'champion', 'undertaker', 'roach', 'booker', 'right', 'tomei', 'man', 'ollie', 'face', 'crowd', 'wwf', 'wrestle', 'reporter', 'h', 'satire', 'clyde', 'marisa', 'pull', 'edge', 'give', 'top', 'mcmahon', 'kane', 'ppv', 'corleone', 'run', 'regal', 'mayor', 'mvp', 'hal', 'fire', 'next', 'result']\n",
      "['war', 'soldier', 'bill', 'army', 'military', 'world', 'us', 'brando', 'ted', 'history', 'vietnam', 'germany', 'american', 'wwii', 'nazi', 'propaganda', 'man', 'ii', 'battle', 'civil', 'macarthur', 'german', 'kubrick', 'general', 'nation', 'union', 'korean', 'two', 'hollywood', 'year', 'later', 'training', 'force', 'troops', 'hotel', 'marlon', 'uniform', 'first', 'also', 'soviet', 'simmons', 'take', 'many', 'tell', 'second', 'bogus', 'point', 'iraq', 'return', 'unit', 'use', 'would', 'wendy', 'president', 'jean', 'vet', 'america', 'attack', 'squad', 'nuclear', 'powell', 'london', 'make', 'home', 'begin', 'capture', 'sergeant', 'follow', 'include', 'machine', 'tower', 'present', 'combat', 'nathan', 'last', 'west', 'cause', 'go', 'become', 'well', 'die', 'journey', 'radio', 'day', 'veteran', 'order', 'cover', 'ww2', 'lead', 'enemy', 'march', 'need', 'civilian', 'wartime', 'must', 'bomb', 'fire', 'lose', 'death', 'firefighter']\n",
      "['minute', 'video', 'five', 'poorly', 'weak', 'cut', 'ten', 'directing', 'unnecessary', '_', 'edit', 'script', 'line', 'last', 'release', 'store', 'twenty', 'much', 'mile', 'without', 'complete', 'give', 'ant', 'dialogue', 'write', 'finish', 'entire', 'fifteen', 'drivel', 'worse', 'disjointed', 'slow', 'enough', 'mess', 'feel', 'plot', 'scene', 'point', 'act', 'sex', 'lester', 'sense', 'come', 'ninety', 'material', 'could', 'seconds', 'turn', 'say', 'let', 'final', 'within', 'less', 'shoot', 'better', 'wait', 'manage', 'involve', 'bug', 'three', 'start', 'run', 'back', 'consider', 'addict', 'offer', 'even', 'walk', 'forty', 'worth', 'dud', 'single', 'rate', 'basically', 'little', 'least', 'midget', 'editing', 'story', 'misleading', 'thirty', 'fan', 'whole', 'random', 'bunch', 'dismal', 'see', 'straight', 'flow', 'wish', 'make', 'production', 'effort', 'print', 'irritating', 'section', 'follow', 'theatrical', 'long', 'listen']\n",
      "['western', 'christmas', 'scott', 'stewart', 'santa', 'john', 'anthony', 'wave', 'c', 'james', 'holmes', 'holiday', 'hamilton', 'mann', 'indians', 'scrooge', 'surfing', 'play', 'gun', 'rifle', 'eve', 'hopkins', 'claus', 'george', 'surf', 'drake', 'surfer', 'dutch', 'winchester', 'nights', 'wonderland', 'boogie', 'ride', 'charles', 'star', 'man', 'bring', 'indian', 'kilmer', 'present', 'dodge', 'carol', 'cast', 'hand', 'duryea', 'classic', 'affleck', 'owner', 'val', 'tony', 'sherlock', 'jimmy', 'tale', 'outlaw', 'ranch', 'big', 'shooting', 'revenge', 'steal', 'josh', 'henry', 'cavalry', 'canyon', 'rock', 'mcintire', 'sport', 'dan', 'earp', 'dickens', 'often', 'west', 'hero', 'mcnally', 'far', 'mitchell', 'director', 'together', 'wyatt', 'along', 'portrayal', 'curtis', 'famous', 'spirit', 'actor', 'hudson', 'face', 'lola', 'giants', 'peralta', 'north', 'screen', 'american', 'become', 'lin', 'genre', 'part', 'course', 'city', 'open', 'marley']\n",
      "['college', 'class', 'shallow', 'stereotype', 'racism', 'act', 'every', 'issue', 'stilted', 'caricature', 'portray', 'ugly', 'woman', 'tempt', 'adolescent', 'ignorant', 'university', 'single', 'lawrence', 'hopelessly', 'pia', 'slightest', 'believe', 'fact', 'value', 'character', 'write', 'dialogue', 'accent', 'man', 'campus', 'dialog', 'piece', 'poor', 'wretched', 'social', 'assume', 'give', 'less', 'speech', 'entire', 'many', 'kibbutz', 'mentally', 'contempt', 'learning', 'person', 'way', 'male', 'quality', 'fellini', 'begin', 'student', 'find', 'worse', 'far', 'pornography', 'loren', 'theme', 'attempt', 'southern', 'misguided', 'superficial', 'perhaps', 'view', 'exaggerated', 'unnatural', 'ridiculous', 'english', 'truly', 'sophia', 'line', 'must', 'general', 'sex', 'soft', 'rather', 'receive', 'american', 'ridden', 'admittedly', 'janeane', 'hollywood', 'retarded', 'script', 'root', 'capitalist', 'stereotypical', 'higher', 'etc', 'career', 'force', 'completely', 'turkey', 'look', 'ought', 'hackneyed', 'small', 'zadora', 'date']\n",
      "['studio', 'stage', 'star', 'jones', 'eddie', 'hollywood', 'murphy', 'davis', 'william', 'warner', 'mgm', 'davies', 'play', 'routine', 'career', 'bette', 'picture', 'judy', 'robert', 'silent', 'production', 'chaplin', 'cameo', 'screen', 'talent', 'role', 'marion', 'broadway', 'brothers', 'chapter', 'contract', 'performance', 'james', 'billy', 'zorro', 'also', 'bros', 'producer', 'garland', 'lukas', 'paramount', 'george', 'serial', 'later', 'big', 'give', 'actress', 'showcase', 'haines', 'paul', 'peggy', 'indiana', 'casablanca', 'voice', 'performer', 'republic', 'become', 'era', 'act', 'john', 'delirious', 'director', 'great', 'could', 'player', 'macdonald', 'produce', 'year', 'mrs', 'bogart', 'win', 'gloria', 'oscar', 'duvall', 'part', 'rachel', 'must', 'appear', 'izzard', 'cagney', 'history', 'many', 'turner', 'tcm', 'eddy', 'cast', 'costume', 'best', 'early', 'radio', 'vehicle', 'top', 'jimmy', 'colbert', 'well', 'reed', 'joan', 'wonderful', 'midler', 'europe']\n",
      "['footage', 'green', 'rob', 'extra', 'keaton', 'roy', 'commentary', 'disc', 'stock', 'audio', 'roth', 'duke', 'buster', 'bubble', 'braveheart', 'alley', 'transfer', 'soylent', 'heston', 'daisy', 'dukes', 'beware', 'claim', 'liam', 'speak', 'noam', 'feature', 'scene', 'use', 'hazzard', 'cunningham', 'featurette', 'robinson', 'durante', 'also', 'boss', 'ashraf', 'thorn', 'final', 'set', 'post', 'several', 'moment', 'mary', 'neeson', 'marquis', 'jessica', 'menu', 'general', 'attempt', 'actual', 'robert', 'screen', 'many', 'charlton', 'easily', 'let', 'tel', 'gallery', 'dvd', 'run', 'aviv', 'food', 'hogg', 'rider', 'bo', 'gibson', 'new', 'though', 'piece', 'nasa', 'come', 'burt', 'put', 'natural', 'english', 'evil', 'william', 'must', 'hardly', 'release', 'two', 'picture', 'palestinian', 'lange', 'fine', 'sibrel', 'star', 'g', 'false', 'conflict', 'latter', 'israeli', 'historical', 'edward', 'stunt', 'view', 'fake', 'man', 'wallace']\n",
      "['rating', 'people', 'r', 'snake', 'dude', 'seriously', 'club', 'rate', 'poster', 'bite', 'socalled', 'bat', 'look', 'imdb', 'retard', 'dear', 'cheese', 'user', 'popcorn', 'moron', 'lowest', 'random', 'wall', 'lot', 'yell', 'vote', 'like', 'disorder', 'higher', 'ritchie', 'trashy', 'bunch', 'filth', 'instead', 'bag', 'ridiculous', 'whole', 'x', 'take', 'piece', 'star', 'dish', 'cult', 'rod', 'pg13', 'actually', 'yet', 'another', 'biker', 'liotta', 'revolver', 'yes', 'worst', 'everyone', 'somewhere', 'furthermore', 'give', 'bottom', 'prophecy', 'creativity', 'mark', 'whatever', 'beyond', 'total', 'holy', 'throw', 'cradle', 'plane', 'clearly', 'walk', 'junk', 'unfortunate', 'believe', 'nothing', 'expect', 'eat', 'must', 'sound', 'away', 'walmart', 'heap', 'shut', 'pull', 'dynamite', 'map', 'list', 'bash', 'feed', 'serious', 'napoleon', 'comment', 'coke', 'idea', 'overall', 'enough', 'produce', 'full', 'suppose', 'score', 'creative']\n",
      "['number', 'musical', 'dance', 'de', 'dancing', 'singing', 'kelly', 'song', 'silent', 'sinatra', 'gene', 'dancer', 'rain', 'sequence', 'sing', 'stage', 'american', 'broadway', 'frank', 'demon', 'palma', 'ballet', 'piano', 'mgm', 'niro', 'perform', 'dress', 'voice', 'set', 'choreography', 'paris', 'iii', 'tune', 'play', 'hollywood', 'jerry', 'grayson', 'sailor', 'fall', 'best', 'gershwin', 'iturbi', 'lady', 'leave', 'score', 'production', 'two', 'caron', 'work', 'screen', 'oscar', 'picture', 'fantasy', 'plot', 'leslie', 'minnelli', 'include', 'hitchcock', 'ogre', 'style', 'george', 'aweigh', 'scene', 'feature', 'star', 'also', 'town', 'audition', 'highlight', 'young', 'office', 'famous', 'singin', 'levant', 'singer', 'entertainment', 'call', 'career', 'example', 'art', 'often', 'clarence', 'shot', 'step', 'mouse', 'girl', 'charming', 'among', 'costume', 'use', 'top', 'first', 'certainly', 'choreograph', 'take', 'kathryn', 'place', 'perhaps', 'anchor', 'classic']\n",
      "['trailer', 'superman', 'shame', 'boll', 'lane', 'seed', 'motive', 'amateur', 'uwe', 'package', 'lois', 'outline', 'poor', 'miserable', 'reeves', 'hero', 'worst', 'phoenix', 'imaginable', 'electric', 'jigsaw', 'put', 'row', 'another', 'mole', 'frog', 'dead', 'shoot', 'silver', 'clark', 'kent', 'must', 'corey', 'excrement', 'fly', 'run', 'case', 'act', 'work', 'alive', 'little', 'reporter', 'least', 'point', 'even', 'never', 'wear', 'set', 'every', 'stand', 'create', 'head', 'talentless', 'office', 'crawl', 'short', 'around', 'sequence', 'face', 'still', 'writer', 'week', 'luthor', 'capitalize', 'director', 'choice', 'try', 'advert', 'close', 'line', 'gymnast', 'quality', 'producer', 'pass', 'filmmaker', 'square', 'plot', 'much', 'legend', 'blind', 'whatsoever', 'foot', 'thomas', 'ever', 'entire', 'suspect', 'single', 'take', 'dozen', 'attempt', 'staff', 'shot', 'come', 'roll', 'full', 'look', 'collora', 'hee', 'donner', 'supposedly']\n",
      "['game', 'bond', 'nightmare', 'freddy', 'street', 'graphic', 'candy', 'mission', 'play', 'dream', 'arnold', 'level', 'enemy', 'james', 'elm', 'beat', 'demon', 'series', 'final', 'krueger', 'dead', 'mario', 'jonathan', 'control', 'weapon', 'get', 'use', 'dictator', 'also', 'first', 'system', 'alice', 'franchise', 'dev', 'spiderman', 'storyline', 'take', 'best', 'fun', 'super', 'installment', 'last', 'cleese', 'lisa', 'feature', 'voice', 'john', 'fan', 'gameplay', 'around', 'two', 'englund', 'head', 'cameo', 'one', 'classic', 'many', 'nintendo', 'search', 'go', 'robert', 'hand', 'standard', 'noah', 'cooper', 'rpg', 'friend', 'fave', 'contain', 'essence', 'player', 'gun', 'visual', 'casino', 'drive', 'stick', 'fantasy', 'doe', 'zane', 'load', 'hit', 'add', 'allow', 'warrior', 'moment', 'hare', 'pierce', 'come', 'form', 'new', 'bonus', 'fire', 'evil', 'sean', 'short', 'dreyfuss', 'attack', 'appear', 'eventually', 'parador']\n",
      "['old', 'year', 'copy', 'japanese', 'us', 'american', 'tape', 'japan', 'vhs', 'mine', 'asian', 'cage', 'culture', 'foreign', 'new', 'release', 'blank', 'website', 'screening', 'still', 'english', 'ago', 'samurai', 'check', 'clip', 'antonioni', 'internet', 'york', 'public', 'see', 'cher', 'many', 'pop', 'sandra', 'accent', 'cultural', 'nicholas', 'traditional', 'theater', 'may', 'borrow', 'bye', 'geisha', 'chinese', 'period', 'library', 'tiger', 'image', 'use', 'maybe', 'comment', 'almost', 'current', 'moonstruck', 'forget', 'youth', 'desert', 'etc', 'long', 'search', 'pass', 'say', 'home', 'since', 'burn', 'nicolas', 'could', 'iron', 'hear', 'look', 'today', 'state', 'yes', 'product', 'print', 'sell', 'superstar', 'authentic', 'costume', 'audience', 'realize', 'blow', 'fantasy', 'call', 'zabriskie', 'language', 'late', 'buy', 'lot', 'rather', 'local', 'daria', 'work', 'popular', 'different', 'hidden', 'must', 'recognize', 'ticket', 'time']\n",
      "['town', 'police', 'cop', 'criminal', 'gang', 'boss', 'steve', 'man', 'gangster', 'crime', 'martin', 'nick', 'sheriff', 'take', 'go', 'gun', 'gold', 'kill', 'arrest', 'wife', 'get', 'andrews', 'partner', 'find', 'blob', 'run', 'mob', 'shoot', 'laura', 'jeff', 'tony', 'city', 'local', 'mafia', 'chief', 'back', 'play', 'jimmy', 'try', 'turn', 'leave', 'dixon', 'trail', 'noir', 'policeman', 'right', 'shootout', 'help', 'hero', 'jail', 'law', 'another', 'also', 'street', 'tierney', 'new', 'robert', 'want', 'come', 'friend', 'end', 'suspect', 'dana', 'old', 'cattle', 'scarface', 'hand', 'become', 'business', 'officer', 'face', 'montana', 'gannon', 'mcqueen', 'eugene', 'sidewalk', 'threaten', 'deal', 'involve', 'leo', 'steal', 'dawson', 'corrupt', 'one', 'mark', 'preminger', 'set', 'dead', 'dave', 'tough', 'chase', 'judge', 'dominick', 'know', 'start', 'later', 'night', 'mayor', 'york', 'bring']\n",
      "['french', 'paris', 'france', 'city', 'le', 'revolution', 'director', 'mathieu', 'wagner', 'carla', 'young', 'les', 'english', 'story', 'deaf', 'natalie', 'de', 'rohmer', 'byron', 'find', 'lover', 'je', 'hearing', 'denis', 'two', 'work', 'du', 'short', 'new', 'place', 'pierre', 'subtitle', 'nora', 'theme', 'depardieu', 'opera', 'cedric', 'parsifal', 'des', 'lip', 'tell', 'whose', 'poet', 'piece', 'form', 'romantic', 'portman', 'rather', 'jacques', 'meet', 'relationship', 'take', 'syberberg', 'leave', 'speak', 'first', 'york', 'payne', 'paul', 'act', 'already', 'perhaps', 'aime', 'inner', 'period', 'couple', 'without', 'viewer', 'use', 'moment', 'also', 'devos', 'fact', 'woman', 'last', 'day', 'tourist', 'another', 'mime', 'maria', 'whole', 'narrative', 'un', 'father', 'focus', 'title', 'cédric', 'audiard', 'wife', 'coen', 'passion', 'various', 'live', 'include', 'feat', 'play', 'mean', 'chocolat', 'cassel', 'quartier']\n",
      "['team', 'player', 'sport', 'lynch', 'worker', 'baseball', 'league', 'roger', 'china', 'shorts', 'larry', 'factory', 'cup', 'mouse', 'soccer', 'may', 'tea', 'straight', 'edie', 'coach', 'morris', 'stooge', 'iran', 'curly', 'alvin', 'win', 'little', 'two', 'football', 'stooges', 'begin', 'three', 'angels', 'woman', 'short', 'daughter', 'iranian', 'stadium', 'darren', 'wilder', 'play', 'panahi', 'man', 'moe', 'live', 'world', 'eccentric', 'craig', 'ho', 'rookie', 'twin', 'tell', 'major', 'drive', 'quaid', 'ask', 'industrial', 'offside', 'around', 'professional', 'brother', 'shemp', 'david', 'visit', 'edith', 'dream', 'farnsworth', 'find', 'include', 'even', 'go', 'paula', 'behind', 'picture', 'still', 'new', 'wonder', 'butch', 'know', 'big', 'leave', 'rule', 'lawn', 'along', 'bead', 'language', 'help', 'decide', 'try', 'gung', 'hear', 'pitcher', 'right', 'full', 'manager', 'reid', 'also', 'journey', 'story', 'home']\n",
      "['south', 'washington', 'officer', 'men', 'africa', 'racist', 'african', 'honor', 'denzel', 'navy', 'woods', 'homeless', 'man', 'biko', 'harris', 'deniro', 'cuba', 'sunday', 'boyer', 'become', 'betty', 'sunshine', 'steve', 'creasy', 'racial', 'fire', 'connery', 'parsons', 'freedom', 'brashear', 'gooding', 'half', 'first', 'shinae', 'gram', 'city', 'earl', 'carl', 'armstrong', 'dietrich', 'apartheid', 'mexico', 'diver', 'pita', 'never', 'chief', 'speak', 'tell', 'marlene', 'begin', 'bobby', 'give', 'diving', 'kidnap', 'take', 'help', 'police', 'robert', 'american', 'secret', 'university', 'face', 'wife', 'chen', 'cause', 'struggle', 'park', 'lorre', 'lead', 'beery', 'kidnapping', 'death', 'bacall', 'world', 'donald', 'racism', 'second', 'two', 'shoot', 'attenborough', 'law', 'escape', 'kline', 'fanning', 'randy', 'truth', 'hand', 'another', 'greene', 'name', 'brodie', 'force', 'country', 'young', 'seek', 'performance', 'paxinou', 'cry', 'spanish', 'try']\n",
      "['get', 'one', 'go', 'see', 'take', 'thing', 'way', 'say', 'come', 'scene', 'want', 'look', 'hot', 'lot', 'like', 'make', 'two', 'know', 'first', 'much', 'back', 'around', 'turn', 'time', 'another', 'really', 'try', 'guess', 'need', 'away', 'maybe', 'old', 'sex', 'keep', 'man', 'even', 'might', 'wrong', 'would', 'whole', 'point', 'run', 'three', 'fall', 'woman', 'little', 'right', 'actually', 'find', 'couple', 'place', 'shoot', 'enough', 'plot', 'head', 'talk', 'give', 'let', 'something', 'reason', 'call', 'think', 'ok', 'happen', 'long', 'play', 'watch', 'sure', 'never', 'chick', 'well', 'guy', 'doctor', 'yes', 'part', 'stuff', 'decide', 'female', 'course', 'body', 'flick', 'work', 'half', 'name', 'start', 'day', 'big', 'pretty', 'explain', 'anyway', 'every', 'break', 'end', 'stick', 'also', 'must', 'girl', 'oh', 'year', 'home']\n",
      "['moon', 'marriage', 'scenery', 'ice', 'indian', 'relation', 'cube', 'nt', 'beautiful', 'thru', 'hyde', 'amrita', 'enhance', 'full', 'shy', 'hepburn', 'gackt', 'shahid', 'sid', 'serbian', 'audrey', 'sho', 'group', 'india', 'bogdanovich', 'voice', 'hardship', 'arrange', 'whip', 'kamal', 'ethnic', 'librarian', 'bollywood', 'romantic', 'diego', 'couple', 'kei', 'rao', 'age', 'stratten', 'follow', 'culture', 'hiphop', 'immigrant', 'tradition', 'vivah', 'apt', 'yugoslavia', 'value', 'look', 'blossom', 'yes', 'mannerism', 'sloth', 'deal', 'pakeezah', 'stein', 'become', 'fall', 'word', 'mammoth', 'omega', 'mention', 'two', 'name', 'among', 'new', 'carefully', 'beauty', 'glimpse', 'largo', 'dorothy', 'success', 'tiger', 'ritter', 'many', 'zero', 'paul', 'phase', 'language', 'leary', 'along', 'content', 'every', 'meena', 'pick', 'first', 'social', 'engagement', 'society', 'develop', 'director', 'without', 'star', 'ask', 'meeting', 'traditional', 'sacrifice', 'kumari', 'comment']\n",
      "['earth', 'space', 'question', 'answer', 'scifi', 'planet', 'crew', 'pilot', 'race', 'reality', 'technology', 'chuck', 'contestant', 'new', 'world', 'outer', 'stargate', 'season', 'create', 'use', 'future', 'human', 'become', 'versus', 'sg1', 'fact', 'without', 'spaceship', 'another', 'three', 'colony', 'many', 'include', 'push', 'call', 'different', 'unanswered', 'species', 'team', 'galactica', 'shatner', 'mission', 'mention', 'come', 'order', 'destroy', 'name', 'collector', 'galaxy', 'ask', 'one', 'save', 'less', 'mars', 'also', 'begin', 'provide', 'focus', 'c', 'leave', 'follow', 'universe', 'death', 'exploration', 'two', 'futuristic', 'later', 'interesting', 'kind', 'oneill', 'reason', 'set', 'member', 'send', 'last', 'course', 'end', 'raise', 'amount', 'grow', 'find', 'host', 'survive', 'around', 'time', 'complete', 'first', 'cause', 'predator', 'travel', 'possible', 'der', 'battlestar', 'escape', 'people', 'select', 'polar', 'power', 'huge', 'daisy']\n",
      "['version', 'jane', 'tarzan', 'emma', 'ape', 'rochester', 'jungle', 'novel', 'scene', 'eyre', 'burton', 'timothy', 'elephant', 'dalton', 'paltrow', 'mr', 'adaptation', 'bbc', 'austen', 'chess', 'carter', 'clarke', 'production', 'charlotte', 'man', 'look', 'handsome', 'miniseries', 'gwyneth', 'mark', 'jesse', 'play', 'knightley', 'bronte', 'original', 'miss', 'zelah', 'english', 'costume', 'crocodile', 'helena', 'helen', 'tribe', 'find', 'portray', 'holt', 'james', 'graveyard', 'strong', 'come', 'much', 'animal', 'maureen', 'although', 'kate', 'capture', 'classic', 'mate', 'speak', 'thompson', 'passion', 'woman', 'osullivan', 'include', 'restore', 'dialogue', 'mrs', 'bonham', 'perfect', 'fall', 'far', 'right', 'never', 'young', 'pride', 'beckinsale', 'rather', 'weissmuller', 'england', 'one', 'george', 'northam', 'passionate', 'leave', 'back', 'long', 'watson', 'true', 'imagine', 'screen', 'heroine', 'nature', 'year', 'bring', 'hunter', 'jeremy', 'bates', 'tim', 'almost', 'lion']\n",
      "['luke', 'star', 'trilogy', 'wars', 'elvis', 'carpenter', 'rebel', 'lucas', 'wilson', 'kurt', 'empire', 'saga', 'back', 'vader', 'uma', 'strike', 'return', 'new', 'emperor', 'battle', 'jedi', 'thurman', 'death', 'final', 'jenny', 'solo', 'leigh', 'darth', 'voyager', 'jabba', 'han', 'superhero', 'first', 'john', 'halloween', 'go', 'year', 'leia', 'captain', 'ewoks', 'original', 'finally', 'since', 'borg', 'skywalker', 'still', 'yoda', 'less', 'complete', 'presley', 'laurie', 'anakin', 'dark', 'norwegian', 'george', 'weller', 'side', 'ggirl', 'take', 'friend', 'meyers', 'home', 'become', 'face', 'two', 'matt', 'fall', 'fan', 'super', 'young', 'destroy', 'franchise', 'turn', 'shuttle', 'plan', 'escape', 'alliance', 'climax', 'owen', 'installment', 'hamill', 'faris', 'sequence', 'must', 'bring', 'ship', 'challenger', 'stay', 'sands', 'curtis', 'janeway', 'include', 'end', 'probably', 'special', 'nearly', 'give', 'part', 'could', 'follow']\n",
      "['cinema', 'critic', 'director', 'audience', 'express', 'protagonist', 'affect', 'imagery', 'cinematic', 'influence', 'experience', 'pretentious', 'miike', 'von', 'hitchcock', 'work', 'film', 'profound', 'disturbing', 'character', 'feel', 'beautiful', 'personal', 'create', 'independent', 'become', 'element', 'style', 'masterpiece', 'train', 'spiritual', 'something', 'glover', 'actress', 'portrait', 'yet', 'without', 'cassavetes', 'thoroughly', 'viewer', 'ultimately', 'world', 'image', 'inner', 'kind', 'level', 'trier', 'monologue', 'truly', 'fill', 'improvise', 'woman', 'chinese', 'essential', 'perhaps', 'play', 'danish', 'myrtle', 'stage', 'yokai', 'cinematography', 'mind', 'past', 'opening', 'praise', 'allow', 'rowlands', 'night', 'understand', 'europa', 'simplicity', 'almost', 'narrative', 'remind', 'poetic', 'one', 'interpret', 'dark', 'view', 'filmmaker', 'important', 'heartfelt', 'identify', 'around', 'society', 'feeling', 'bravo', 'many', 'captivating', 'never', 'piece', 'complex', 'break', 'fascinating', 'filmmaking', 'involve', 'uncomfortable', 'takashi', 'way', 'gena']\n",
      "['high', 'school', 'please', 'message', 'student', 'badly', 'sexual', 'teacher', 'challenge', 'subject', 'teach', 'class', 'learn', 'watch', 'topic', 'disgust', 'issue', 'zero', 'attend', 'need', 'consider', 'point', 'never', 'hard', 'value', 'toilet', 'gere', 'comment', 'try', 'day', 'discussion', 'disbelief', 'content', 'agree', 'director', 'level', 'producer', 'sick', 'believe', 'force', 'hollywood', 'flock', 'ever', 'every', 'get', 'might', 'unreal', 'could', 'people', 'controversial', 'excuse', 'sex', 'make', 'moral', 'educational', 'board', 'attempt', 'even', 'person', 'still', 'far', 'public', 'without', 'behind', 'preach', 'statement', 'sometimes', 'allow', 'meaningless', 'pick', 'hit', 'debate', 'act', 'disturbing', 'problem', 'around', 'hold', 'principal', 'understand', 'matter', 'share', 'present', 'suspension', 'social', 'fashion', 'write', 'mind', 'handle', 'major', 'sad', 'save', 'bully', 'existence', 'week', 'view', 'real', 'hire', 'compare', 'nothing', 'flush']\n",
      "['event', 'documentary', 'america', 'political', 'people', 'government', 'history', 'american', 'americans', 'media', 'grand', 'society', 'states', 'politics', 'united', 'truth', 'historical', 'lie', 'world', 'kirk', 'present', 'live', 'document', 'mel', 'us', 'interview', 'day', 'spock', 'account', 'make', 'first', 'brooks', 'fact', 'view', 'president', 'many', 'filmmaker', 'year', 'order', 'spanish', 'power', 'use', 'chavez', 'information', 'coup', 'news', 'free', 'century', 'issue', 'past', 'take', 'group', 'important', 'propaganda', 'state', 'picture', 'aids', 'national', 'member', 'become', 'actual', 'research', 'know', 'street', 'educate', 'happen', 'call', 'record', 'canyon', 'personal', 'new', 'mccoy', 'shoot', 'allow', 'way', 'see', 'lesson', 'also', 'side', 'cheech', 'small', 'point', 'organize', 'help', 'problem', 'course', 'bias', 'million', 'rights', 'believe', 'bush', 'darius', 'community', 'woman', 'common', 'poverty', 'oil', 'hold', 'change', 'chong']\n",
      "['action', 'fight', 'car', 'art', 'fly', 'vampire', 'scene', 'sequence', 'martial', 'ship', 'kong', 'gun', 'battle', 'dragon', 'use', 'chase', 'fast', 'hong', 'fu', 'kung', 'plot', 'blah', 'matrix', 'fighting', 'skill', 'jet', 'effect', 'li', 'hero', 'kick', 'drive', 'stunt', 'flight', 'flick', 'titanic', 'many', 'run', 'take', 'genre', 'chinese', 'fighter', 'power', 'cool', 'move', 'explosion', 'ninja', 'sword', 'bruce', 'hk', 'warrior', 'weapon', 'woo', 'let', 'enter', 'master', 'fist', 'choreography', 'first', 'blade', 'throw', 'way', 'speed', 'machine', 'kungfu', 'force', 'race', 'standard', 'star', 'dub', 'style', 'sammo', 'lot', 'air', 'wire', 'least', 'break', 'chiba', 'one', 'dubbing', 'million', 'stand', 'special', 'blow', 'stop', 'choreograph', 'sound', 'furious', 'punch', 'fire', 'mask', 'shaolin', 'silly', 'could', 'look', 'become', 'ability', 'tsui', 'turn', 'get', 'face']\n",
      "['trash', 'garbage', 'utterly', 'stone', 'utter', 'piece', 'basic', 'worst', 'even', 'f', 'ashamed', 'steven', 'seagal', 'inept', 'instinct', 'plot', 'redeeming', 'worthless', 'appalling', 'completely', 'believe', 'insulting', 'dreadful', 'dire', 'absolute', 'unwatchable', 'beyond', 'pile', 'dreck', 'complete', 'sharon', 'abysmal', 'tripe', 'erotic', 'intent', 'mess', 'blatant', 'filmmaking', 'apparently', 'travesty', 'awful', 'truly', 'whatsoever', 'involve', 'voiceover', 'describe', 'offensive', 'may', 'moronic', 'ripoff', 'vile', 'word', 'feature', 'laughable', 'nonsense', 'pathetic', 'nothing', 'barely', 'embarrassingly', 'fail', 'ever', 'brain', 'one', 'sex', 'appear', 'obvious', 'produce', 'suffer', 'cost', 'never', 'disgrace', 'embarrassment', 'fury', 'let', 'hard', 'script', 'tasteless', 'belief', 'cinematic', 'shame', 'actually', 'allow', 'avoid', 'thin', 'boring', 'embarrassing', 'force', 'except', 'painful', 'point', 'rubbish', 'clear', 'since', 'become', 'instead', 'bargain', 'disgusting', 'constant', 'wannabe', 'abomination']\n",
      "['human', 'emotional', 'story', 'unique', 'character', 'find', 'deeply', 'film', 'explore', 'experience', 'viewer', 'eye', 'emotion', 'world', 'struggle', 'move', 'situation', 'emotionally', 'depiction', 'impact', 'compelling', 'one', 'different', 'capture', 'present', 'element', 'within', 'may', 'much', 'subtle', 'individual', 'become', 'take', 'journey', 'also', 'people', 'environment', 'way', 'use', 'understanding', 'relationship', 'draw', 'sense', 'theme', 'personal', 'give', 'feel', 'without', 'leave', 'portray', 'paulie', 'issue', 'focus', 'level', 'narrative', 'setting', 'ordinary', 'often', 'tension', 'humanity', 'yet', 'create', 'add', 'set', 'need', 'work', 'make', 'moral', 'perspective', 'intensity', 'depict', 'simple', 'many', 'moment', 'viewing', 'craven', 'like', 'lose', 'fully', 'scene', 'style', 'man', 'complexity', 'force', 'visually', 'storytelling', 'well', 'two', 'loss', 'watch', 'stunning', 'stanley', 'deep', 'reveal', 'act', 'meaning', 'friendship', 'society', 'rather', 'highly']\n",
      "['music', 'rock', 'arthur', 'spirit', 'moore', 'canadian', 'john', 'musician', 'grey', 'roll', 'concert', 'canada', 'blues', 'sound', 'score', 'song', 'classical', 'annie', 'jazz', 'lennon', 'lyric', 'carmen', 'beatles', 'dudley', 'gielgud', 'susan', 'owl', 'caruso', 'perform', 'drum', 'oscar', 'lanza', 'marry', 'die', 'listen', 'become', 'sing', 'kusturica', 'singer', 'witherspoon', 'produce', 'millionaire', 'bach', 'voice', 'reese', 'liza', 'also', 'write', 'bear', 'opera', 'singing', 'order', 'direct', 'fact', 'sam', 'brosnan', 'drunk', 'appear', 'attenborough', 'alex', 'sir', 'archie', 'take', 'lead', 'hear', 'minnelli', 'course', 'linda', 'last', 'adams', 'marc', 'treat', 'style', 'indian', 'hobson', 'late', 'butler', 'year', 'together', 'vocal', 'historical', 'background', 'play', 'truly', 'follow', 'especially', 'wilderness', 'beatle', 'famous', 'screen', 'intrusive', 'dorothy', 'press', 'talented', 'choose', 'hollywood', 'note', 'remain', 'direction', 'history']\n",
      "['alan', 'fred', 'charlie', 'adam', 'castle', 'sandler', 'rogers', 'johnson', 'lady', 'cagney', 'ginger', 'astaire', 'ustinov', 'ramones', 'cook', 'maggie', 'ruby', 'phantom', 'evelyn', 'cheadle', 'wife', 'riff', 'randolph', 'reign', 'berkeley', 'new', 'scott', 'let', 'hammer', 'play', 'joan', 'red', 'smith', 'raines', 'carol', 'busby', 'woman', 'lead', 'secretary', 'keeler', 'elisha', 'election', 'york', 'george', 'harriet', 'parade', 'cliff', 'fineman', 'put', 'missile', 'course', 'baron', 'tone', 'young', 'lombard', 'loss', 'run', 'bob', 'fun', 'ella', 'fontaine', 'curtis', 'wodehouse', 'newhart', 'usual', 'become', 'feature', 'face', 'robert', 'macmurray', 'footlight', 'togar', 'head', 'lose', 'appear', 'star', 'performance', 'plot', 'take', 'blondell', 'woronov', 'soles', 'name', 'since', 'set', 'hilliard', 'carole', 'grable', 'damsel', 'burns', 'score', 'follow', 'female', 'among', 'ivanna', 'marry', 'romance', 'aztec', 'man', 'three']\n",
      "['classic', 'age', 'jack', 'richard', 'comic', 'big', 'world', 'character', 'screen', 'one', 'new', 'charm', 'play', 'like', 'little', 'color', 'star', 'city', 'era', 'look', 'fun', 'see', 'dick', 'many', 'make', 'go', 'time', 'russell', 'great', 'set', 'today', 'tracy', 'almost', 'cast', 'use', 'york', 'work', 'take', 'relief', 'top', 'bob', 'way', 'early', 'enjoyable', 'tale', 'hollywood', 'year', 'still', 'jean', 'part', 'another', 'fan', 'better', 'picture', 'miss', 'red', 'strip', 'live', 'different', 'fresh', 'give', 'witch', 'finest', 'add', 'find', 'support', 'remake', 'familiar', 'wild', 'course', 'beautiful', 'create', 'warren', 'charming', 'delightful', 'animated', 'far', 'actor', 'worth', 'put', 'romantic', 'win', 'novak', 'name', 'late', 'best', 'would', 'production', 'release', 'style', 'direct', 'watch', 'steal', 'spell', 'turn', 'james', 'jolie', 'remind', 'since', 'old']\n",
      "['television', 'dad', 'program', 'tony', 'mom', 'abuse', 'teenage', 'joan', 'bush', 'fisher', 'foster', 'amanda', 'antwone', 'illness', 'adopt', 'friend', 'melissa', 'bsg', 'teenager', 'must', 'problem', 'become', 'boyfriend', 'sabrina', 'new', 'tell', 'help', 'childhood', 'caprica', 'mental', 'home', 'learn', 'grow', 'hart', 'session', 'jenna', 'therapy', 'hope', 'suffer', 'young', 'britney', 'past', 'need', 'year', 'live', 'use', 'call', 'private', 'junior', 'jon', 'nephew', 'release', 'eric', 'different', 'abusive', 'wesley', 'stoltz', 'power', 'begin', 'psychiatrist', 'washington', 'harvey', 'take', 'move', 'skeptical', 'three', 'battlestar', 'often', 'painful', 'include', 'derek', 'know', 'may', 'jameson', 'endure', 'idea', 'mother', 'virtual', 'institution', 'hand', 'marry', 'name', 'deal', 'holly', 'eagerly', 'grandparent', 'hold', 'want', 'consider', 'progress', 'ambition', 'already', 'mean', 'dr', 'enough', 'pressure', 'ramtha', 'star', 'continue', 'greed']\n",
      "['skip', 'bank', 'professor', 'barbara', 'stanwyck', 'stan', 'widmark', 'code', 'lily', 'mickey', 'domino', 'ollie', 'peters', 'new', 'face', 'fuller', 'woman', 'tank', 'man', 'gray', 'power', 'use', 'ritter', 'york', 'candy', 'baby', 'street', 'hackman', 'pickup', 'jean', 'thelma', 'city', 'richard', 'mars', 'red', 'diamond', 'work', 'gun', 'toxic', 'doyle', 'south', 'end', 'sam', 'deathstalker', 'moe', 'top', 'head', 'europeans', 'tough', 'brent', 'steel', 'purse', 'born', 'precode', 'communist', 'joey', 'fall', 'pickpocket', 'noir', 'police', 'killers', 'two', 'whose', 'get', 'scam', 'horn', 'charlie', 'allow', 'appear', 'powers', 'steal', 'goat', 'hand', 'zatoichi', 'gas', 'microfilm', 'start', 'escape', 'john', 'chaplin', 'coffy', 'sell', 'pimp', 'information', 'right', 'style', 'despite', 'mccoy', 'subway', 'victim', 'date', 'ending', 'germ', 'sea', 'stop', 'lou', 'mastermind', 'give', 'also', 'sap']\n",
      "['detective', 'wind', 'master', 'jackie', 'brown', 'foot', 'chan', 'melodrama', 'oscar', 'powell', 'douglas', 'hudson', 'christy', 'stunt', 'kyle', 'robert', 'stack', 'vance', 'play', 'sirk', 'man', 'write', 'malone', 'performance', 'dorothy', 'police', 'bacall', 'daniel', 'lowe', 'daylewis', 'william', 'case', 'award', 'cerebral', 'win', 'woman', 'left', 'lauren', 'name', 'philo', 'mitch', 'support', 'pavarotti', 'leave', 'brenda', 'marry', 'actor', 'work', 'part', 'room', 'fall', 'disabled', 'best', 'career', 'always', 'later', 'call', 'actress', 'include', 'worthy', 'puri', 'curtiz', 'melvyn', 'rock', 'character', 'hadley', 'coe', 'archer', 'brother', 'marcel', 'another', 'om', 'also', 'hand', 'fricker', 'rich', 'destiny', 'usual', 'rathbone', 'irish', 'palsy', 'oil', 'lucy', 'director', 'seven', 'hugh', 'gig', 'kennel', 'palette', 'hypnotize', 'become', 'color', 'give', 'come', 'drunken', 'help', 'hollywood', 'imitation', 'mary', 'direct']\n",
      "['mike', 'patient', 'dan', 'dentist', 'wife', 'man', 'noble', 'brosnan', 'julian', 'kolchak', 'daniel', 'carell', 'bobby', 'cox', 'deliverance', 'campbell', 'alex', 'four', 'macy', 'take', 'jon', 'feinstone', 'hitman', 'pierce', 'businessman', 'dr', 'voight', 'member', 'turn', 'kinnear', 'h', 'beatty', 'trip', 'choir', 'greg', 'chicago', 'meet', 'danny', 'local', 'william', 'river', 'bernsen', 'business', 'hillbilly', 'reynolds', 'sanders', 'become', 'candidate', 'find', 'manager', 'job', 'leave', 'corbin', 'lose', 'day', 'work', 'run', 'burt', 'night', 'go', 'norris', 'often', 'mcgavin', 'dental', 'steve', 'friend', 'moto', 'ned', 'change', 'fear', 'wright', 'decide', 'fall', 'panic', 'deal', 'morty', 'come', 'crisis', 'carl', 'also', 'realize', 'follow', 'sweden', 'task', 'forrest', 'lupino', 'noon', 'darren', 'swedish', 'dark', 'yuzna', 'people', 'davis', 'return', 'new', 'boorman', 'john', 'ronny', 'figure', 'everyman']\n",
      "['cinderella', 'fairy', 'pitch', 'diamond', 'scooby', 'tale', 'red', 'barrymore', 'preston', 'drew', 'ben', 'fever', 'satan', 'devil', 'sox', 'vampire', 'boston', 'yeti', 'shaggy', 'fallon', 'scoobydoo', 'prince', 'bridget', 'stepmother', 'busey', 'shelley', 'lionel', 'lady', 'winters', 'reynolds', 'burt', 'new', 'ball', 'lindsey', 'include', 'mouse', 'glass', 'holmes', 'go', 'bat', 'frye', 'milverton', 'atwill', 'fan', 'bronson', 'name', 'course', 'dwight', 'get', 'wray', 'become', 'doll', 'around', 'back', 'woman', 'ace', 'since', 'dress', 'take', 'run', 'slipper', 'look', 'dr', 'work', 'godmother', 'use', 'without', 'stepsister', 'old', 'clue', 'step', 'debbie', 'victim', 'queen', 'home', 'chikatilo', 'burakov', 'village', 'quigley', 'helen', 'dahlia', 'chicken', 'charles', 'find', 'day', 'title', 'fay', 'lindsay', 'matter', 'steal', 'baseball', 'hand', 'turn', 'lucifer', 'two', 'local', 'jimmy', 'watson', 'one', 'andrew']\n",
      "['movie', 'see', 'one', 'make', 'go', 'really', 'like', 'good', 'watch', 'also', 'people', 'say', 'find', 'well', 'end', 'enjoy', 'look', 'get', 'part', 'lot', 'recommend', 'could', 'try', 'want', 'reason', 'thing', 'plot', 'scene', 'expect', 'tell', 'theater', 'must', 'way', 'story', 'first', 'rent', 'still', 'time', 'understand', 'actually', 'character', 'nothing', 'actor', 'wait', 'many', 'pretty', 'kind', 'come', 'sure', 'take', 'right', 'ever', 'fan', 'another', 'every', 'act', 'enough', 'opinion', 'better', 'main', 'definitely', 'cool', 'top', 'entire', 'rate', 'feel', 'fun', 'acting', 'completely', 'keep', 'big', 'course', 'use', 'interesting', 'friend', 'guess', 'storyline', 'hollywood', 'type', 'hard', 'never', 'play', 'admit', 'fact', 'great', 'buy', 'beginning', 'may', 'hear', 'name', 'far', 'person', 'throughout', 'care', 'absolutely', 'wrong', 'even', 'much', 'easily', 'figure']\n",
      "['story', 'man', 'young', 'relationship', 'thriller', 'two', 'twist', 'character', 'make', 'fine', 'play', 'one', 'give', 'work', 'get', 'highly', 'way', 'woman', 'tell', 'lead', 'become', 'plot', 'suspense', 'also', 'interesting', 'performance', 'end', 'recommend', 'set', 'begin', 'keep', 'move', 'turn', 'paul', 'new', 'tale', 'well', 'find', 'friendship', 'director', 'complex', 'act', 'little', 'wife', 'along', 'friend', 'discover', 'somewhat', 'unfold', 'despite', 'around', 'would', 'take', 'meet', 'seem', 'especially', 'verhoeven', 'home', 'develop', 'writer', 'deal', 'surprise', 'rather', 'different', 'psychological', 'manage', 'throughout', 'although', 'hit', 'go', 'beautiful', 'write', 'unusual', 'fascinating', 'help', 'tight', 'without', 'may', 'worth', 'need', 'intriguing', 'even', 'part', 'lady', 'sense', 'former', 'level', 'create', 'come', 'location', 'cast', 'another', 'might', 'follow', 'clever', 'fourth', 'nicole', 'real', 'stylish', 'surprising']\n",
      "['book', 'read', 'story', 'base', 'novel', 'write', 'adaptation', 'way', 'could', 'say', 'first', 'writer', 'author', 'character', 'many', 'tell', 'different', 'true', 'original', 'much', 'change', 'point', 'make', 'leave', 'never', 'nancy', 'part', 'feel', 'word', 'adapt', 'line', 'time', 'instead', 'without', 'start', 'would', 'take', 'play', 'follow', 'something', 'work', 'screen', 'screenplay', 'miss', 'also', 'name', 'actually', 'cruel', 'roberts', 'really', 'screenwriter', 'lose', 'whole', 'another', 'fail', 'disappointment', 'little', 'understand', 'one', 'reader', 'important', 'totally', 'use', 'source', 'thing', 'explain', 'imagine', 'get', 'seem', 'try', 'put', 'description', 'actor', 'drew', 'opinion', 'completely', 'however', 'detail', 'wrong', 'turn', 'whether', 'stephen', 'reading', 'happen', 'fan', 'casting', 'adaption', 'audience', 'prequel', 'know', 'reviewer', 'nothing', 'give', 'end', 'half', 'hollywood', 'mean', 'maybe', 'director', 'believe']\n",
      "['peter', 'science', 'fiction', 'spy', 'speed', 'jake', 'wells', 'greek', 'world', 'shadow', 'falk', 'ben', 'portion', 'pulp', 'highest', 'production', 'paul', 'sassy', 'print', 'century', 'historically', 'reiser', 'year', 'faithful', 'future', 'nelson', 'bind', 'martian', 'rendition', 'j', 'design', 'period', 'thing', 'odyssey', 'english', 'scottish', 'trip', 'rather', 'set', 'raymond', 'voice', 'blackadder', 'script', 'massey', 'england', 'often', 'photography', 'update', 'chance', 'fine', 'might', 'futuristic', 'write', 'wonderful', 'line', 'otoole', 'setting', 'actor', 'martians', 'june', 'richardson', 'description', 'exposure', 'color', 'tailor', 'fact', 'format', 'score', 'wonder', 'hg', 'far', 'screen', 'produce', 'highlight', 'imax', 'text', 'follow', 'laurie', 'include', 'try', 'hugh', 'diminish', 'vital', 'element', 'everytown', 'fascinating', 'homeward', 'second', 'bliss', 'require', 'outing', 'effort', 'vision', 'subtitle', 'appear', 'pet', 'note', 'capra', 'ralph', 'costume']\n",
      "['time', 'money', 'waste', 'hour', 'someone', 'get', 'else', 'go', 'spend', 'pay', 'something', 'could', 'make', 'plot', 'totally', 'anything', 'rent', 'unbelievable', 'need', 'say', 'give', 'sorry', 'everything', 'hope', 'half', 'come', 'back', 'free', 'shot', 'everyone', 'maybe', 'want', 'either', 'feel', 'write', 'long', 'credit', 'line', 'worth', 'let', 'rental', 'actor', 'take', 'little', 'well', 'buy', 'save', 'dollar', 'keep', 'end', 'run', 'much', 'nothing', 'thing', 'absolutely', 'wait', 'one', 'put', 'warn', 'complete', 'people', 'every', 'dialog', 'entire', 'look', 'like', 'another', 'million', 'case', 'check', 'director', 'worst', 'use', 'die', 'figure', 'total', 'would', 'rave', 'see', 'wonder', 'tell', 'except', 'better', 'try', 'usually', 'actually', 'sure', 'sit', 'never', 'talent', 'theatre', 'porn', 'least', 'ask', 'screen', 'im', 'bet', 'cash', 'find', 'lead']\n",
      "['would', 'think', 'like', 'watch', 'even', 'know', 'better', 'see', 'make', 'first', 'one', 'want', 'review', 'thing', 'act', 'anyone', 'write', 'never', 'original', 'actually', 'though', 'comment', 'idea', 'probably', 'least', 'something', 'way', 'anything', 'good', 'since', 'happen', 'feel', 'give', 'fact', 'might', 'year', 'say', 'actor', 'maybe', 'could', 'imdb', 'fan', 'far', 'go', 'really', 'bother', 'still', 'people', 'whole', 'leave', 'let', 'second', 'part', 'plot', 'guess', 'remake', 'agree', 'away', 'scene', 'start', 'line', 'back', 'take', 'worse', 'wish', 'movie', 'stay', 'big', 'honestly', 'compare', 'personally', 'hope', 'half', 'come', 'try', 'list', 'time', 'point', 'disappointed', 'sure', 'look', 'well', 'two', 'last', 'either', 'call', 'expect', 'suppose', 'end', 'lot', 'name', 'little', 'much', 'right', 'kind', 'person', 'new', 'notice', 'different', 'old']\n",
      "['show', 'series', 'tv', 'episode', 'season', 'watch', 'first', 'get', 'one', 'character', 'go', 'like', 'new', 'see', 'air', 'really', 'say', 'also', 'still', 'well', 'fan', 'star', 'back', 'start', 'every', 'good', 'last', 'year', 'remember', 'great', 'never', 'next', 'always', 'would', 'give', 'people', 'second', 'trek', 'sitcom', 'could', 'time', 'lot', 'come', 'thing', 'hope', 'many', 'run', 'day', 'network', 'find', 'two', 'long', 'television', 'look', 'cast', 'use', 'week', 'something', 'much', 'enjoy', 'another', 'way', 'bring', 'take', 'original', 'end', 'writer', 'cancel', 'want', 'channel', 'line', 'old', 'part', 'rest', 'abc', 'full', 'set', 'leave', 'lose', 'live', 'put', 'late', 'keep', 'make', 'commercial', 'kind', 'wait', 'try', 'viewer', 'right', 'three', 'popular', 'different', 'since', 'even', 'actually', 'rerun', 'think', 'work', 'writing']\n",
      "['prison', 'harry', 'allen', 'russian', 'dirty', 'woody', 'sudden', 'san', 'eastwood', 'maria', 'prisoner', 'clint', 'francisco', 'brendan', 'scoop', 'impact', 'potter', 'sondra', 'inmate', 'fassbinder', 'locke', 'kells', 'coburn', 'johansson', 'georges', 'mitchum', 'ian', 'jennifer', 'warden', 'name', 'scene', 'hugh', 'callahan', 'day', 'last', 'new', 'line', 'take', 'scarlett', 'melinda', 'london', 'point', 'heston', 'jackman', 'arizona', 'work', 'escape', 'magician', 'tell', 'mcshane', 'revenge', 'punk', 'woman', 'secret', 'peruvian', 'sister', 'use', 'waterman', 'direct', 'karl', 'force', 'american', 'appear', 'involve', 'sid', 'ahead', 'city', 'victim', 'return', 'pow', 'town', 'latin', 'peter', 'herschel', 'spencer', 'include', 'journalist', 'audience', 'order', 'less', 'three', 'different', 'magnum', 'york', 'schygulla', 'convict', 'execute', 'become', 'give', 'narrator', 'annie', 'law', 'attitude', 'light', 'hershey', 'along', 'however', 'mysterious', 'mick', 'lead']\n",
      "['michael', 'hospital', 'jackson', 'buddy', 'dinosaur', 'john', 'wayne', 'dennis', 'flynn', 'man', 'walsh', 'hopper', 'rick', 'cage', 'smooth', 'melt', 'melting', 'rex', 'rabbit', 'red', 'gentleman', 'nurse', 'incredible', 'clark', 'turn', 'errol', 'hulk', 'boyle', 'also', 'back', 'corbett', 'west', 'doctor', 'mr', 'lead', 'mj', 'big', 'suzanne', 'shrink', 'look', 'garfield', 'decide', 'darkman', 'get', 'jim', 'day', 'j', 'dahl', 'come', 'feature', 'wife', 'marine', 'gates', 'omar', 'alamo', 'gun', 'criminal', 'hero', 'mexican', 'name', 'famous', 'leave', 'lara', 'pesci', 'smallville', 'raoul', 'history', 'world', 'go', 'club', 'escape', 'keep', 'moonwalker', 'plan', 'ian', 'right', 'texas', 'battle', 'schmid', 'trex', 'help', 'quite', 'stop', 'fear', 'local', 'run', 'chase', 'raptor', 'chandler', 'start', 'ang', 'honey', 'molest', 'jt', 'director', 'rubber', 'lyle', 'lex', 'thing', 'gorilla']\n",
      "['drug', 'queen', 'prince', 'victoria', 'danny', 'rose', 'albert', 'young', 'emily', 'hippie', 'barry', 'spielberg', 'purple', 'walker', 'blunt', 'goldberg', 'doctor', 'whoopi', 'glover', 'royal', 'mother', 'friend', 'history', 'costume', 'later', 'ranger', 'melbourne', 'take', 'become', 'foster', 'year', 'force', 'woman', 'celie', 'historical', 'include', 'joel', 'also', 'rodney', 'keep', 'jodie', 'make', 'period', 'miranda', 'rupert', 'hand', 'homicide', 'two', 'wife', 'africanamerican', 'cast', 'throne', 'gosha', 'dreamy', 'support', 'course', 'color', 'piper', 'lord', 'power', 'john', 'dangerfield', 'loretta', 'schindler', 'oscar', 'state', 'design', 'play', 'princess', 'miya', 'since', 'business', 'court', 'oprah', 'new', 'strong', 'england', 'regard', 'conroy', 'trent', 'pit', 'figure', 'turn', 'act', 'various', 'cocaine', 'richardson', 'crazed', 'struggle', 'dog', 'especially', 'mark', 'come', 'whose', 'early', 'fashion', 'affair', 'may', 'romance', 'hitokiri']\n",
      "['alien', 'scientist', 'angel', 'experiment', 'machine', 'jessica', 'invisible', 'special', 'lab', 'lost', 'hollow', 'simpson', 'effect', 'look', 'research', 'mutant', 'chorus', 'bacon', 'naschy', 'fx', 'bike', 'mad', 'line', 'work', 'bang', 'audition', 'test', 'scientific', 'even', 'thing', 'kevin', 'apparently', 'use', 'jar', 'paul', 'assistant', 'man', 'dafoe', 'doctor', 'turn', 'bikini', 'alba', 'invasion', 'destroy', 'laboratory', 'flashy', 'cassie', 'one', 'find', 'leather', 'model', 'arm', 'every', 'serum', 'make', 'el', 'funding', 'right', 'power', 'remake', 'head', 'appear', 'plotline', 'together', 'nuclear', 'add', 'send', 'basement', 'discover', 'dark', 'disaster', 'signal', 'kingdom', 'something', 'complete', 'assume', 'ask', 'direct', 'shiny', 'never', 'decide', 'disappear', 'manage', 'chocolate', 'sort', 'max', 'attenborough', 'area', 'george', 'road', 'device', 'open', 'set', 'dozen', 'invisibility', 'cody', 'decent', 'continuity', 'bennett', 'suppose']\n",
      "['murder', 'mystery', 'crime', 'columbo', 'welles', 'murderer', 'australian', 'evidence', 'citizen', 'noir', 'case', 'dean', 'investigate', 'investigation', 'solve', 'suspect', 'guilty', 'australia', 'orson', 'kane', 'gordon', 'streep', 'lady', 'detective', 'chamberlain', 'meryl', 'serial', 'robert', 'shanghai', 'clue', 'rita', 'director', 'public', 'turn', 'michael', 'accent', 'hayworth', 'lindy', 'accuse', 'ohara', 'innocent', 'baby', 'mabel', 'victim', 'later', 'revolt', 'press', 'know', 'direct', 'charge', 'find', 'head', 'wife', 'prove', 'aussie', 'even', 'star', 'reveal', 'also', 'commit', 'investigator', 'death', 'blackmail', 'legacy', 'sam', 'fact', 'early', 'hollywood', 'zodiac', 'course', 'year', 'cut', 'actual', 'usual', 'take', 'believe', 'dingo', 'elsa', 'begin', 'cain', 'sloane', 'call', 'media', 'partner', 'jagger', 'famous', 'lt', 'discover', 'femme', 'fatale', 'hand', 'power', 'shoot', 'central', 'trial', 'chris', 'play', 'come', 'trouble', 'charlie']\n",
      "['performance', 'drama', 'realistic', 'give', 'great', 'gem', 'outstanding', 'deliver', 'well', 'shop', 'bruce', 'strong', 'one', 'make', 'work', 'much', 'play', 'like', 'job', 'get', 'especially', 'little', 'story', 'actor', 'cast', 'support', 'scene', 'hollywood', 'jennifer', 'also', 'move', 'star', 'carrey', 'underrated', 'character', 'way', 'take', 'film', 'around', 'charming', 'fact', 'use', 'kudos', 'right', 'powerful', 'power', 'could', 'even', 'corner', 'lead', 'lot', 'actress', 'find', 'better', 'easy', 'turn', 'come', 'perfect', 'superb', 'realism', 'may', 'hard', 'usual', 'enjoy', 'real', 'act', 'truly', 'different', 'genuine', 'role', 'worth', 'script', 'woman', 'every', 'add', 'romance', 'really', 'look', 'shine', 'stellar', 'always', 'share', 'keep', 'two', 'believe', 'appreciate', 'without', 'top', 'big', 'frank', 'able', 'end', 'impressed', 'face', 'small', 'however', 'apart', 'yet', 'bit', 'minor']\n",
      "['country', 'gay', 'people', 'hitler', 'bomb', 'terrorist', 'beach', 'ben', 'usa', 'world', 'man', 'person', 'learn', 'attack', 'tell', 'accept', 'sniper', 'truth', 'understand', 'medical', 'die', 'american', 'fact', 'piece', 'different', 'suicide', 'september', 'also', 'leader', 'present', 'kind', 'would', 'segment', 'portray', 'evil', 'right', 'take', 'clay', 'example', 'real', 'happen', 'one', 'even', 'could', 'penn', 'side', 'inaccuracy', 'israel', 'know', 'many', 'director', 'gender', 'anyone', 'another', 'help', 'everyone', 'way', 'rights', 'straight', 'try', 'surgery', 'two', 'able', 'unhappy', 'point', 'woman', 'organ', 'thing', 'believe', 'eleven', 'uk', 'give', 'russia', 'corky', 'muslim', 'share', 'power', 'rise', 'commit', 'lover', 'ignore', 'abortion', 'us', 'historical', 'wtc', 'suffering', 'cause', 'palm', 'short', 'laden', 'totally', 'culture', 'loach', 'bin', 'homosexual', 'carlyle', 'hero', 'find', 'mraovich', 'subject']\n",
      "['cartoon', 'joke', 'cat', 'like', 'jerry', 'store', 'teen', 'remember', 'eat', 'kinda', 'teens', 'rent', 'fun', 'cult', 'get', 'hat', 'still', 'skit', 'old', 'pie', 'host', 'tap', 'stuff', 'kick', 'hip', 'watch', 'power', 'n', 'people', 'etc', 'lol', 'night', 'rap', 'try', 'swear', 'e', 'back', 'talk', 'american', 'dont', 'spinal', 'see', 'grade', 'short', 'rangers', 'springer', 'slug', 'tube', 'better', 'hop', 'classic', 'enjoy', 'mtv', 'listen', 'late', 'box', 'crazy', 'thing', 'say', 'myers', 'soup', 'nostalgia', 'catch', 'nerd', 'amazon', 'day', 'name', 'roll', 'come', 'actually', 'big', 'jay', 'chase', 'even', 'super', 'weird', 'hate', 'ever', 'maybe', 'lot', 'must', 'alot', 'yesterday', 'year', 'chevy', 'franco', 'since', 'swearing', 'group', 'mike', 'find', 'seuss', 'live', 'know', 'forget', 'video', 'theater', 'mid', 'groove', 'much']\n",
      "['zombie', 'dead', 'italian', 'fulci', 'attack', 'giallo', 'fido', 'billy', 'corpse', 'gore', 'romero', 'virus', 'zombi', 'live', 'flick', 'undead', 'tail', 'turn', 'gas', 'flesh', 'bruno', 'dawn', 'moss', 'martino', 'lance', 'head', 'george', 'timmy', 'bloodbath', 'eating', 'infect', 'genre', 'infected', 'case', 'hand', 'mattei', 'still', 'living', 'fest', 'director', 'eat', 'face', 'run', 'soon', 'shaun', 'lucio', 'take', 'todd', 'fence', 'scene', 'italy', 'shoot', 'body', 'original', 'lassie', 'woman', 'hide', 'look', 'sergio', 'zombies', 'insurance', 'mostel', 'scientist', 'fan', 'title', 'makeup', 'sheets', 'collar', 'come', 'set', 'bird', 'try', 'argento', 'also', 'chemical', 'follow', 'chris', 'involve', 'may', 'another', 'reporter', 'every', 'military', 'exactly', 'horde', 'release', 'name', 'wellington', 'fall', 'steal', 'spot', 'people', 'wife', 'put', 'death', 'group', 'actually', 'make', 'fun', 'fanfan']\n",
      "['winner', 'anderson', 'taylor', 'jeff', 'magazine', 'celebrity', 'lloyd', 'spike', 'doll', 'storm', 'alienate', 'gary', 'linda', 'gillian', 'pegg', 'star', 'get', 'sidney', 'angle', 'bridges', 'ann', 'triple', 'simon', 'dunst', 'back', 'big', 'rock', 'try', 'lose', 'face', 'goldblum', 'witchcraft', 'table', 'exorcist', 'come', 'occult', 'start', 'ring', 'raggedy', 'take', 'kirsten', 'begin', 'say', 'bubba', 'pin', 'rope', 'finch', 'soon', 'go', 'blair', 'taker', 'hasselhoff', 'breed', 'kick', 'towel', 'turn', 'heat', 'witch', 'megan', 'evil', 'walk', 'dourif', 'top', 'dyer', 'might', 'career', 'catch', 'gabe', 'angels', 'new', 'dreyfuss', 'george', 'irwin', 'pregnant', 'brock', 'chef', 'byrne', 'title', 'mortensen', 'sun', 'beat', 'ireland', 'angelopoulos', 'around', 'though', 'abby', 'along', 'sarandon', 'upside', 'hooper', 'whose', 'robert', 'todd', 'score', 'girlfriend', 'support', 'rest', 'bring', 'camel', 'journalist']\n",
      "['desire', 'german', 'giant', 'quote', 'theory', 'gadget', 'inspector', 'guide', 'intellectual', 'zizek', 'device', 'discuss', 'logic', 'volume', 'ally', 'speak', 'point', 'analysis', 'psychology', 'english', 'nazis', 'scorsese', 'use', 'sholay', 'example', 'bread', 'say', 'physics', 'power', 'language', 'penny', 'correctly', 'id', 'vampire', 'brain', 'claw', 'take', 'bon', 'word', 'ego', 'original', 'however', 'philosophy', 'quantum', 'germans', 'pervert', 'comment', 'agenda', 'kind', 'head', 'one', 'possible', 'create', 'obviously', 'thought', 'jovi', 'varma', 'talk', 'dame', 'ajay', 'voice', 'field', 'aag', 'justify', 'world', 'may', 'rather', 'psycho', 'hitchcock', 'exist', 'least', 'come', 'already', 'three', 'several', 'feminine', 'cloud', 'cinema', 'open', 'work', 'question', 'people', 'place', 'accord', 'therefore', 'let', 'body', 'part', 'fantasy', 'mention', 'truth', 'pokémon', 'reaction', 'idea', 'fiennes', 'mind', 'devgan', 'even', 'thing', 'context']\n",
      "['black', 'white', 'jim', 'cold', 'academy', 'mary', 'perry', 'blake', 'man', 'need', 'crystal', 'kansas', 'milo', 'color', 'portray', 'award', 'stiller', 'envy', 'vegas', 'capote', 'shepard', 'nothing', 'culture', 'othello', 'brooks', 'dick', 'race', 'less', 'brain', 'rourke', 'wilson', 'become', 'truman', 'clutter', 'las', 'hair', 'lose', 'two', 'atlantis', 'robert', 'fishburne', 'whites', 'play', 'little', 'sense', 'moment', 'mr', 'jones', 'go', 'place', 'control', 'noise', 'richard', 'dr', 'must', 'voice', 'actual', 'colour', 'use', 'coward', 'sell', 'seem', 'scott', 'could', 'well', 'blood', 'write', 'iago', 'face', 'people', 'set', 'change', 'industry', 'begin', 'plot', 'thin', 'desdemona', 'point', 'writer', 'take', 'steal', 'barry', 'character', 'beginning', 'problem', 'hickock', 'script', 'world', '_', 'quincy', 'appear', 'find', 'make', 'hall', 'visible', 'new', 'able', 'perhaps', 'piece', 'spray']\n",
      "['year', 'brilliant', 'ago', 'see', 'remember', 'play', 'story', 'time', 'first', 'back', 'two', 'summer', 'west', 'great', 'day', 'later', 'past', 'scene', 'well', 'memory', 'young', 'still', 'old', 'future', 'also', 'one', 'get', 'character', 'recommend', 'highly', 'james', 'donald', 'send', 'last', 'hunt', 'man', 'part', 'never', 'since', 'watch', 'history', 'cole', 'cast', 'actor', 'lead', 'tell', 'twelve', 'surprise', 'must', 'sutherland', 'brad', 'nine', 'long', 'many', 'plot', 'little', 'picture', 'find', 'wonderful', 'miss', 'cinematography', 'new', 'work', 'sure', 'flashback', 'decade', 'forget', 'pitt', 'willis', 'give', 'release', 'fine', 'make', 'screenplay', 'look', 'today', 'much', 'full', 'leave', 'combine', 'dramatic', 'gilliam', 'together', 'bruce', 'almost', 'several', 'date', 'terrific', 'screen', 'end', 'even', 'know', 'come', 'begin', 'try', 'terry', 'perfectly', 'oscar', 'live', 'help']\n",
      "['eric', 'river', 'bo', 'ned', 'norman', 'derek', 'jewish', 'fbi', 'homer', 'lena', 'john', 'kelly', 'woman', 'blind', 'bloom', 'florida', 'worm', 'donna', 'run', 'hanzo', 'play', 'fishing', 'paul', 'artemisia', 'maclean', 'orlando', 'esther', 'dillinger', 'heath', 'chip', 'rosenstrasse', 'redford', 'story', 'two', 'colonel', 'ledger', 'puerto', 'montana', 'give', 'miles', 'higher', 'also', 'fly', 'rocket', 'october', 'hoover', 'place', 'brad', 'rico', 'take', 'culture', 'include', 'pasolini', 'help', 'vera', 'hand', 'seberg', 'fact', 'beautiful', 'whose', 'move', 'hannah', 'pitt', 'flamenco', 'fez', 'purvis', 'name', 'become', 'razor', 'quinn', 'girls', 'ruth', 'gyllenhaal', 'justice', 'saura', 'grace', 'holocaust', 'bey', 'kelso', 'red', 'force', 'berlin', 'legend', 'look', 'capture', 'present', 'career', 'laura', 'kerr', 'robert', 'carry', 'work', 'ashton', 'von', 'strike', 'later', 'member', 'history', 'day', 'watts']\n",
      "['role', 'great', 'play', 'excellent', 'actor', 'well', 'cast', 'enjoy', 'scene', 'one', 'definitely', 'also', 'act', 'really', 'character', 'see', 'especially', 'film', 'like', 'make', 'get', 'job', 'quite', 'look', 'performance', 'recommend', 'john', 'much', 'watch', 'beautiful', 'little', 'support', 'many', 'small', 'fan', 'direct', 'think', 'highly', 'certainly', 'terrific', 'give', 'lovely', 'better', 'lot', 'always', 'big', 'know', 'director', 'actress', 'time', 'although', 'lead', 'acting', 'keep', 'direction', 'work', 'fantastic', 'favourite', 'go', 'include', 'extremely', 'perfectly', 'fine', 'feature', 'fun', 'good', 'script', 'never', 'pace', 'different', 'entertaining', 'line', 'worth', 'turn', 'memorable', 'end', 'enjoyable', 'intense', 'perfect', 'throughout', 'eye', 'plenty', 'first', 'even', 'everyone', 'man', 'plus', 'genre', 'thing', 'career', 'old', 'part', 'likable', 'probably', 'way', 'long', 'story', 'truly', 'shoot', 'come']\n",
      "['lee', 'party', 'sleep', 'talk', 'night', 'go', 'bar', 'jason', 'pack', 'morning', 'play', 'meet', 'saturday', 'bed', 'vacation', 'julia', 'afternoon', 'take', 'day', 'dinner', 'start', 'driver', 'wake', 'come', 'back', 'star', 'new', 'two', 'date', 'next', 'late', 'sunday', 'wild', 'clothes', 'never', 'paul', 'ex', 'put', 'could', 'time', 'job', 'combination', 'get', 'lady', 'friend', 'thing', 'wife', 'try', 'still', 'gable', 'course', 'part', 'every', 'ellen', 'summer', 'much', 'later', 'london', 'suffer', 'taxi', 'live', 'scene', 'young', 'friday', 'happen', 'together', 'sex', 'jared', 'bachelor', 'along', 'grandmother', 'end', 'find', 'work', 'woman', 'nostalgic', 'kind', 'karen', 'hughes', 'fall', 'spike', 'break', 'apartment', 'first', 'leave', 'crawford', 'need', 'although', 'character', 'mexican', 'rich', 'well', 'become', 'mexico', 'remember', 'far', 'despite', 'worth', 'little', 'many']\n",
      "['funny', 'comedy', 'laugh', 'humor', 'joke', 'hilarious', 'gag', 'funniest', 'humour', 'comedic', 'parody', 'unfunny', 'fun', 'comedian', 'loud', 'spoof', 'laughter', 'moment', 'romantic', 'amusing', 'line', 'friend', 'serious', 'funnier', 'slapstick', 'satire', 'witty', 'audience', 'see', 'comic', 'like', 'make', 'ben', 'every', 'crude', 'silly', 'hard', 'clever', 'get', 'ever', 'smile', 'chris', 'probably', 'virgin', 'humorous', 'date', 'situation', 'chuckle', 'stand', 'sketch', 'farce', 'many', 'entertain', 'absolutely', 'lampoon', 'seriously', 'timing', 'come', 'material', 'steve', 'fall', 'sarah', 'one', 'watch', 'writer', 'keep', 'national', 'attempt', 'cameo', 'want', 'standup', 'park', 'first', 'dry', 'flat', 'wit', 'never', 'hysterical', 'time', 'premise', 'write', 'talent', 'entire', 'stiller', 'sight', 'straight', 'imagine', 'work', 'since', 'goofy', 'suppose', 'right', 'go', 'fart', 'play', 'enough', 'lighthearted', 'genius', 'hilariously', 'vulgar']\n",
      "['film', 'see', 'make', 'one', 'go', 'say', 'people', 'would', 'like', 'get', 'scene', 'could', 'want', 'never', 'director', 'give', 'try', 'find', 'watch', 'even', 'use', 'know', 'anyone', 'look', 'reason', 'end', 'take', 'thing', 'something', 'two', 'man', 'away', 'day', 'completely', 'must', 'way', 'believe', 'anything', 'ever', 'tell', 'nothing', 'wonder', 'really', 'leave', 'work', 'well', 'many', 'filmmaker', 'maker', 'hear', 'walk', 'sense', 'sit', 'release', 'long', 'star', 'lot', 'actor', 'word', 'happen', 'every', 'title', 'real', 'might', 'kind', 'name', 'put', 'enough', 'act', 'talk', 'point', 'instead', 'credit', 'without', 'turn', 'probably', 'run', 'theater', 'mind', 'friend', 'whole', 'come', 'screen', 'wait', 'sure', 'last', 'year', 'audience', 'recommend', 'attempt', 'rest', 'absolutely', 'time', 'else', 'matter', 'level', 'actually', 'let', 'decide', 'story']\n",
      "['girl', 'boy', 'sister', 'water', 'grant', 'marie', 'young', 'pet', 'three', 'teenage', 'louis', 'charles', 'crush', 'two', 'din', 'marry', 'go', 'kiss', 'cary', 'back', 'birthday', 'older', 'friend', 'gunga', 'woman', 'old', 'mary', 'home', 'crosby', 'meet', 'cemetery', 'ask', 'learn', 'take', 'become', 'little', 'want', 'dream', 'floriane', 'lead', 'first', 'sex', 'swimming', 'fall', 'pursue', 'cutter', 'move', 'anne', 'use', 'new', 'title', 'fairbanks', 'soon', 'later', 'whose', 'rich', 'sixteen', 'find', 'sematary', 'ground', 'sam', 'victor', 'way', 'body', 'gal', 'stephen', 'heather', 'around', 'date', 'break', 'george', 'arquette', 'kramer', 'still', 'hair', 'help', 'gage', 'return', 'decide', 'lily', 'bing', 'year', 'graham', 'dead', 'haim', 'sibling', 'leave', 'gregory', 'male', 'local', 'bernie', 'appear', 'douglas', 'rachel', 'bury', 'teacher', 'frog', 'follow', 'come', 'man']\n",
      "['one', 'time', 'film', 'may', 'way', 'much', 'take', 'see', 'viewer', 'people', 'many', 'seem', 'however', 'find', 'perhaps', 'modern', 'today', 'point', 'make', 'view', 'come', 'first', 'well', 'true', 'still', 'visual', 'place', 'theme', 'like', 'image', 'also', 'short', 'mind', 'yet', 'say', 'certain', 'attention', 'even', 'audience', 'though', 'lot', 'leave', 'appreciate', 'scene', 'easy', 'fact', 'use', 'quite', 'another', 'aspect', 'often', 'experience', 'go', 'world', 'part', 'long', 'work', 'thing', 'simple', 'since', 'character', 'similar', 'tend', 'particular', 'style', 'never', 'entertainment', 'little', 'might', 'rather', 'sometimes', 'opinion', 'remain', 'hard', 'understand', 'element', 'hollywood', 'comment', 'difficult', 'sure', 'face', 'screen', 'almost', 'different', 'feel', 'although', 'course', 'year', 'treat', 'right', 'story', 'easily', 'simply', 'back', 'case', 'follow', 'dark', 'day', 'provide', 'compare']\n",
      "['disney', 'adventure', 'magic', 'thief', 'magical', 'bug', 'princess', 'savage', 'song', 'bollywood', 'doc', 'khan', 'akshay', 'dolph', 'kapoor', 'amitabh', 'lundgren', 'indian', 'atlantis', 'kumar', 'walt', 'anil', 'genie', 'three', 'story', 'okay', 'hindi', 'govinda', 'salman', 'bachchan', 'shah', 'ahmad', 'ram', 'john', 'abu', 'transform', 'comic', 'fall', 'tashan', 'sabu', 'india', 'fantasy', 'jaffar', 'elmer', 'ali', 'abhay', 'romance', 'fly', 'wolverine', 'couple', 'raj', 'mccarthy', 'hero', 'bagdad', 'conrad', 'climax', 'element', 'kareena', 'soha', 'half', 'deol', 'work', 'part', 'special', 'korda', 'animated', 'yet', 'eye', 'second', 'paresh', 'veidt', 'time', 'toy', 'serious', 'abraham', 'still', 'belle', 'gopal', 'turn', 'bring', 'give', 'priyanka', 'wish', 'top', 'akshaye', 'till', 'usual', 'effect', 'evil', 'find', 'tell', 'xmen', 'six', 'marry', 'wait', 'send', 'try', 'carpet', 'escape', 'chopra']\n",
      "['film', 'unfortunately', 'plot', 'one', 'seem', 'script', 'lack', 'rather', 'interesting', 'cast', 'director', 'effort', 'particularly', 'enough', 'quite', 'lead', 'dull', 'much', 'interest', 'work', 'set', 'character', 'however', 'make', 'major', 'scene', 'little', 'though', 'exception', 'also', 'star', 'two', 'mediocre', 'although', 'nothing', 'performance', 'appear', 'yet', 'time', 'production', 'actor', 'decent', 'become', 'talent', 'either', 'chemistry', 'support', 'fairly', 'pacing', 'result', 'engaging', 'least', 'picture', 'fail', 'attempt', 'release', 'often', 'less', 'material', 'moment', 'involve', 'genre', 'direction', 'better', 'talented', 'point', 'feature', 'come', 'reason', 'despite', 'many', 'premise', 'female', 'neither', 'problem', 'actress', 'idea', 'element', 'ultimately', 'thin', 'limited', 'certainly', 'might', 'even', 'long', 'direct', 'year', 'score', 'earlier', 'producer', 'half', 'simply', 'sequence', 'find', 'end', 'contrived', 'main', 'almost', 'role', 'part']\n",
      "['tooth', 'get', 'much', 'promising', 'come', 'look', 'little', 'like', 'darkness', 'go', 'pretty', 'way', 'end', 'try', 'story', 'could', 'man', 'one', 'lead', 'though', 'make', 'baldwin', 'cohen', 'muddle', 'long', 'time', 'however', 'another', 'well', 'around', 'bergman', 'better', 'feel', 'even', 'enough', 'kind', 'seem', 'rather', 'nothing', 'place', 'fairy', 'moment', 'call', 'sense', 'couple', 'turn', 'sort', 'idea', 'almost', 'fan', 'least', 'weird', 'take', 'say', 'asylum', 'day', 'bland', 'fall', 'thing', 'start', 'average', 'lack', 'let', 'eye', 'meet', 'become', 'without', 'forgettable', 'supernatural', 'also', 'find', 'stuff', 'run', 'director', 'background', 'unsatisfying', 'poor', 'character', 'gabriel', 'first', 'lot', 'old', 'sound', 'cyborg', 'face', 'give', 'use', 'azumi', 'problem', 'quite', 'keep', 'soon', 'might', 'dialogue', 'hallucination', 'george', 'real', 'include', 'people', 'four']\n",
      "['stupid', 'make', 'say', 'horrible', 'mean', 'ever', 'awful', 'could', 'crap', 'see', 'really', 'scary', 'thing', 'oh', 'suck', 'hell', 'hate', 'dumb', 'worst', 'nothing', 'even', 'get', 'plain', 'lame', 'people', 'stop', 'one', 'come', 'na', 'piece', 'anything', 'ask', 'give', 'know', 'something', 'acting', 'worse', 'put', 'whole', 'call', 'watch', 'start', 'word', 'tell', 'crappy', 'actually', 'least', 'wrong', 'anymore', 'think', 'gon', 'god', 'damn', 'pathetic', 'go', 'plot', 'never', 'friend', 'let', 'sense', 'absolutely', 'sit', 'hey', 'asleep', 'wan', 'anyway', 'remember', 'eye', 'idiot', 'ridiculous', 'special', 'stupidity', 'hear', 'sorry', 'stink', 'take', 'suppose', 'want', 'way', 'actor', 'write', 'right', 'ass', 'believe', 'movie', 'whoever', 'another', 'theater', 'probably', 'yeah', 'possibly', 'mind', 'sure', 'either', 'old', 'good', 'seriously', 'insult', 'fall', 'yes']\n",
      "['woman', 'nudity', 'rape', 'sex', 'computer', 'scene', 'werewolf', 'cgi', 'naked', 'lesbian', 'female', 'nude', 'look', 'man', 'yeah', 'rat', 'wolf', 'jerk', 'pretty', 'porn', 'actually', 'male', 'ta', 'descent', 'find', 'eye', 'beast', 'hardcore', 'dawson', 'graphic', 'sexually', 'like', 'bitch', 'rapist', 'mate', 'shot', 'roommate', 'victim', 'sexual', 'dahmer', 'plenty', 'topless', 'one', 'rosario', 'breast', 'take', 'three', 'erotic', 'throw', 'really', 'real', 'sexy', 'lead', 'two', 'assault', 'around', 'revenge', 'sleaze', 'sexuality', 'tit', 'plastic', 'cult', 'screen', 'effect', 'back', 'first', 'attach', 'softcore', 'gratuitous', 'frontal', 'porno', 'cool', 'almost', 'strip', 'stuff', 'horny', 'hand', 'face', 'opening', 'boring', 'well', 'box', 'anyway', 'involve', 'everything', 'however', 'danning', 'transformation', 'body', 'mouth', 'shower', 'want', 'something', 'attack', 'especially', 'tame', 'fact', 'cracker', 'disturbing', 'hair']\n",
      "['williams', 'crash', 'ball', 'plane', 'lake', 'robin', 'boat', 'hood', 'kevin', 'bridge', 'lucy', 'mst3k', 'puppet', 'baker', 'los', 'channel', 'angeles', 'wreck', 'spacey', 'look', 'justin', 'timberlake', 'fun', 'beowulf', 'run', 'slam', 'stalker', 'rome', 'psychic', 'could', 'submarine', 'nemesis', 'sock', 'lucille', 'hobgoblin', 'arnie', 'instead', 'mitchell', 'joe', 'grendel', 'help', 'disaster', 'final', 'justice', 'wear', 'warning', 'airport', 'monster', 'much', 'play', 'since', 'palermo', 'fact', 'another', 'slim', 'mistake', 'shoot', 'least', 'mostly', 'like', 'malta', 'bigfoot', 'must', 'crater', 'desi', 'land', 'rubber', 'might', 'yet', 'seal', 'name', 'passenger', 'set', 'sink', 'schwarzenegger', 'cast', 'complete', 'edison', 'big', 'better', 'cardboard', 'right', 'york', 'hijack', 'escape', 'carry', 'day', 'j', 'even', 'put', 'chase', 'survive', 'none', 'point', 'main', 'clearly', 'helmet', 'guess', 'appear', 'rather']\n",
      "['mr', 'sky', 'jr', 'oliver', 'golden', 'robert', 'captain', 'caine', 'man', 'reed', 'sally', 'william', 'downey', 'sir', 'alec', 'play', 'keith', 'field', 'miyazaki', 'angie', 'sidney', 'pirate', 'nancy', 'london', 'fagin', 'globe', 'mini', 'guinness', 'dickens', 'world', 'suit', 'bill', 'look', 'elevator', 'never', 'stella', 'clooney', 'invention', 'mystery', 'cast', 'picture', 'perfect', 'h', 'superhero', 'star', 'macy', 'stanley', 'away', 'sykes', 'turn', 'work', 'long', 'charismatic', 'kline', 'guiness', 'spirited', 'pym', 'brett', 'laputa', 'yet', 'men', 'shepherd', 'new', 'lad', 'miniseries', 'charming', 'find', 'young', 'ealing', 'rest', 'castle', 'fabric', 'performance', 'need', 'one', 'cloth', 'maradona', 'industry', 'set', 'oneal', 'motion', 'score', 'thaw', 'ron', 'kevin', 'lamarr', 'dub', 'miss', 'finest', 'magnus', 'bafta', 'dodger', 'talent', 'artful', 'egan', 'moment', 'york', 'run', 'second', 'english']\n",
      "['david', 'king', 'festival', 'international', 'freeman', 'indie', 'morgan', 'valentine', 'holly', 'item', 'richards', 'temple', 'sell', 'gamera', 'hatred', 'shirley', 'toronto', 'godzilla', 'biblical', 'short', 'stephen', 'vega', 'subject', 'testament', 'scarlet', 'tokyo', 'patrick', 'peck', 'denise', 'paz', 'premiere', 'target', 'help', 'bathsheba', 'world', 'audience', 'less', 'american', 'gregory', 'feature', 'hayward', 'insomniac', 'turtle', 'dominic', 'filmmaker', 'new', 'hollywood', 'tell', 'display', 'mainstream', 'powerful', 'store', 'supermarket', 'take', 'screening', 'death', 'pictures', 'play', 'susan', 'infidelity', 'offer', 'flame', 'giant', 'brothel', 'student', 'successful', 'eye', 'genre', 'raymond', 'big', 'research', 'monaghan', 'direct', 'paul', 'return', 'find', 'produce', 'grow', 'discover', 'gammera', 'talent', 'saul', 'nathan', 'send', 'hope', 'reference', 'follow', 'cambodia', 'adultery', 'save', 'qa', 'north', 'studios', 'kingdom', 'journey', 'face', 'call', 'topic', 'year', 'interview']\n",
      "['killer', 'joe', 'wood', 'serial', 'buck', 'ed', 'midnight', 'shark', 'creep', 'kate', 'whale', 'snow', 'underground', 'kurosawa', 'city', 'clown', 'ratso', 'frightening', 'cowboy', 'voight', 'jaws', 'also', 'snowman', 'begin', 'troma', 'hoffman', 'man', 'find', 'two', 'many', 'jon', 'way', 'help', 'try', 'hit', 'look', 'thailand', 'new', 'frost', 'texas', 'tourist', 'hand', 'claire', 'plan', 'york', 'implausible', 'misty', 'crocodile', 'become', 'come', 'rate', 'danes', 'around', 'alligator', 'scene', 'pullman', 'hustler', 'london', 'rizzo', 'meet', 'dream', 'couple', 'tell', 'include', 'later', 'orca', 'take', 'genetic', 'start', 'dustin', 'fail', 'x', 'far', 'cecil', 'director', 'tomato', 'catch', 'reason', 'lowbudget', 'back', 'kind', 'search', 'terrorize', 'harris', 'flashback', 'ripoff', 'town', 'beckinsale', 'trip', 'little', 'd', 'wenders', 'know', 'obvious', 'unborn', 'system', 'nolan', 'actually', 'local', 'late']\n",
      "['horror', 'effect', 'terrible', 'budget', 'acting', 'low', 'special', 'cheap', 'production', 'worst', 'look', 'editing', 'gore', 'flick', 'cheesy', 'ever', 'sound', 'scare', 'quality', 'script', 'worse', 'value', 'genre', 'rip', 'awful', 'bmovie', 'decent', 'fan', 'direction', 'poor', 'act', 'actually', 'lowbudget', 'cinematography', 'lighting', 'plot', 'atrocious', 'amateurish', 'set', 'filmmaker', 'produce', 'scene', 'title', 'one', 'example', 'even', 'lousy', 'obviously', 'remake', 'shoot', 'porn', 'amateur', 'least', 'laughable', 'direct', 'way', 'etc', 'suspense', 'like', 'avoid', 'level', 'truly', 'atmosphere', 'independent', 'feature', 'expect', 'makeup', 'cover', 'use', 'continuity', 'far', 'creepy', 'storyline', 'lot', 'camcorder', 'cost', 'absolutely', 'lower', 'deserve', 'name', 'writing', 'standard', 'actor', 'project', 'dialogue', 'redeeming', 'straight', 'try', 'new', 'unless', 'director', 'mix', 'b', 'check', 'add', 'rate', 'every', 'thing', 'much', 'work']\n",
      "['husband', 'shakespeare', 'christopher', 'branagh', 'play', 'anne', 'hamlet', 'elizabeth', 'walken', 'patrick', 'garbo', 'kenneth', 'widow', 'apartment', 'alison', 'wife', 'version', 'old', 'sentinel', 'casper', 'visconti', 'work', 'loy', 'gino', 'blandings', 'beverly', 'giovanna', 'woman', 'clara', 'always', 'build', 'chris', 'myrna', 'ossessione', 'production', 'meredith', 'jill', 'new', 'eli', 'actress', 'romeo', 'building', 'lead', 'home', 'american', 'raines', 'role', 'cristina', 'carradine', 'affair', 'wallach', 'sarandon', 'swim', 'cortez', 'twice', 'together', 'igor', 'gardner', 'meet', 'begin', 'ian', 'actor', 'model', 'text', 'burgess', 'look', 'career', 'become', 'although', 'holm', 'leonora', 'juliet', 'marry', 'attempt', 'brooklyn', 'cast', 'include', 'dench', 'christie', 'daughter', 'neighbor', 'direction', 'creepy', 'postman', 'two', 'connecticut', 'must', 'find', 'return', 'cain', 'muriel', 'ferdie', 'well', 'decide', 'granddaughter', 'part', 'print', 'bring', 'lawyer', 'upon']\n",
      "['la', 'opera', 'grace', 'soap', 'tiny', 'dig', 'football', 'fish', 'east', 'brian', 'et', 'anton', 'band', 'fi', 'sci', 'picture', 'leon', 'dandy', 'diana', 'warhol', 'waters', 'live', 'become', 'art', 'pecker', 'firm', 'bowl', 'saving', 'beckham', 'like', 'godzilla', 'bend', 'woman', 'snl', 'newcombe', 'alexandre', 'want', 'eagle', 'tell', 'michelle', 'say', 'man', 'world', 'play', 'romanian', 'massacre', 'anger', 'enough', 'fan', 'take', 'rodriguez', 'meadows', 'way', 'ladies', 'daytime', 'reality', 'jonestown', 'front', 'go', 'people', 'milk', 'truth', 'courtney', 'future', 'two', 'bjm', 'audience', 'tim', 'sugar', 'genius', 'hooligan', 'romania', 'respect', 'face', 'follow', 'use', 'look', 'edward', 'talent', 'girlfight', 'marijuana', 'need', 'record', 'club', 'mention', 'member', 'sell', 'main', 'idea', 'difference', 'english', 'self', 'great', 'boxing', 'know', 'work', 'kick', 'exist', 'american', 'failure']\n",
      "['family', 'kid', 'child', 'mother', 'adult', 'parent', 'dog', 'daughter', 'animal', 'little', 'baby', 'young', 'home', 'cute', 'find', 'older', 'younger', 'charlie', 'old', 'age', 'grow', 'bear', 'friend', 'care', 'back', 'help', 'altman', 'heaven', 'childhood', 'go', 'leave', 'gandhi', 'toy', 'learn', 'try', 'loving', 'talk', 'want', 'father', 'become', 'raise', 'grinch', 'live', 'bring', 'dr', 'alone', 'small', 'sad', 'take', 'year', 'move', 'food', 'mom', 'come', 'save', 'poor', 'cindy', 'voice', 'divorce', 'wife', 'heartwarming', 'especially', 'carface', 'dark', 'die', 'steal', 'whole', 'youngest', 'worry', 'animated', 'kidnap', 'throw', 'aim', 'itchy', 'neighborhood', 'hackenstein', 'send', 'farm', 'special', 'soon', 'marry', 'feed', 'may', 'behavior', 'bright', 'orphan', 'woman', 'struggle', 'sweet', 'diane', 'dom', 'also', 'remember', 'share', 'kind', 'lesson', 'start', 'win', 'give', 'mama']\n",
      "['tom', 'awesome', 'smith', 'smart', 'genius', 'robot', 'ryan', 'tim', 'joseph', 'fabulous', 'law', 'hanks', 'paul', 'sullivan', 'boxing', 'robbins', 'play', 'rooney', 'thomas', 'bull', 'newman', 'sissy', 'meg', 'ride', 'online', 'romantic', 'iq', 'mormon', 'l', 'jude', 'einstein', 'boxer', 'road', 'young', 'work', 'downs', 'sweet', 'roller', 'intelligent', 'portray', 'lot', 'barman', 'coaster', 'john', 'small', 'tyler', 'course', 'divine', 'anytime', 'anyway', 'bud', 'connor', 'kick', 'career', 'like', 'theo', 'mendes', 'albert', 'daddy', 'lds', 'fry', 'redemption', 'away', 'uncle', 'crowe', 'prophet', 'degree', 'help', 'new', 'move', 'true', 'face', 'day', 'miss', 'erika', 'ed', 'look', 'perdition', 'almost', 'transformers', 'james', 'matthau', 'gangster', 'give', 'antwerp', 'catherine', 'quality', 'fall', 'taste', 'believe', 'hoechlin', 'mormons', 'everyone', 'time', 'mustache', 'forget', 'talent', 'line', 'jay', 'cause']\n",
      "['british', 'britain', 'hoffman', 'hawke', 'lumet', 'ethan', 'nun', 'buffalo', 'flavia', 'london', 'pickford', 'set', 'population', 'man', 'beetle', 'morality', 'mary', 'seymour', 'england', 'year', 'last', 'lead', 'local', 'end', 'notion', 'woman', 'new', 'finney', 'force', 'lansbury', 'english', 'world', 'muslims', 'though', 'convent', 'granger', 'mark', 'area', 'wife', 'uk', 'century', 'society', 'taylor', 'pimlico', 'muslim', 'include', 'become', 'angela', 'sidney', 'part', 'cobra', 'york', 'leave', 'side', 'hagar', 'tomlinson', 'edmund', 'feature', 'devil', 'hunt', 'question', 'island', 'philip', 'remain', 'sandy', 'home', 'weir', 'conclusion', 'come', 'aboriginal', 'follow', 'mr', 'outside', 'india', 'poppins', 'seek', 'resident', 'albert', 'delve', 'julie', 'tomei', 'nation', 'live', 'angry', 'create', 'condemn', 'city', 'slave', 'behavior', 'state', 'movement', 'central', 'independent', 'army', 'moral', 'representative', 'bring', 'away', 'turn', 'might']\n",
      "['life', 'real', 'feel', 'see', 'character', 'people', 'make', 'come', 'really', 'way', 'man', 'live', 'like', 'world', 'change', 'time', 'bring', 'many', 'story', 'one', 'end', 'beautiful', 'feeling', 'thing', 'know', 'even', 'want', 'go', 'friend', 'happy', 'realize', 'woman', 'never', 'could', 'touch', 'think', 'though', 'work', 'reality', 'day', 'back', 'take', 'seem', 'yet', 'everyone', 'say', 'would', 'hope', 'help', 'tell', 'person', 'believe', 'get', 'also', 'still', 'big', 'without', 'need', 'believable', 'first', 'lose', 'sometimes', 'place', 'keep', 'main', 'try', 'normal', 'another', 'part', 'understand', 'happen', 'con', 'around', 'always', 'two', 'problem', 'long', 'deal', 'dream', 'much', 'away', 'truly', 'together', 'ask', 'give', 'moment', 'everything', 'grow', 'little', 'care', 'new', 'meet', 'whole', 'sad', 'portray', 'face', 'chance', 'close', 'something', 'right']\n",
      "['end', 'bit', 'quite', 'find', 'moment', 'time', 'strange', 'first', 'go', 'something', 'see', 'like', 'thing', 'still', 'part', 'type', 'say', 'well', 'happen', 'way', 'start', 'get', 'take', 'early', 'watch', 'one', 'worth', 'sure', 'little', 'right', 'seem', 'make', 'lot', 'couple', 'know', 'especially', 'check', 'much', 'scene', 'great', 'try', 'really', 'place', 'enjoy', 'people', 'definitely', 'almost', 'however', 'always', 'leave', 'begin', 'turn', 'keep', 'last', 'later', 'day', 'least', 'rather', 'think', 'sort', 'would', 'actually', 'different', 'kind', 'tell', 'maybe', 'ending', 'become', 'even', 'two', 'understand', 'work', 'third', 'fact', 'slow', 'move', 'fan', 'around', 'whole', 'though', 'miss', 'decide', 'since', 'weird', 'dark', 'average', 'explain', 'somehow', 'could', 'big', 'already', 'light', 'old', 'case', 'exactly', 'notice', 'travel', 'along', 'night', 'another']\n",
      "['character', 'seem', 'make', 'like', 'much', 'story', 'could', 'really', 'scene', 'look', 'even', 'nothing', 'plot', 'lack', 'interesting', 'end', 'poor', 'actor', 'try', 'better', 'give', 'act', 'far', 'script', 'main', 'anything', 'almost', 'little', 'one', 'would', 'instead', 'development', 'fail', 'film', 'sense', 'problem', 'many', 'point', 'simply', 'director', 'part', 'feel', 'completely', 'dialogue', 'attempt', 'go', 'thing', 'none', 'potential', 'audience', 'however', 'sort', 'suppose', 'care', 'good', 'idea', 'first', 'style', 'find', 'leave', 'least', 'work', 'time', 'example', 'play', 'depth', 'mean', 'rather', 'either', 'boring', 'together', 'acting', 'say', 'less', 'without', 'direction', 'expect', 'way', 'lead', 'another', 'become', 'develop', 'pretty', 'well', 'disappointing', 'weak', 'happen', 'enough', 'fact', 'totally', 'real', 'never', 'quality', 'something', 'set', 'two', 'dull', 'whole', 'ridiculous', 'premise']\n"
     ]
    }
   ],
   "source": [
    "top_tokens = model.score_tracker['top-tokens'].last_tokens\n",
    "for topic_name in model.topic_names:\n",
    "    print(top_tokens[topic_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые из них выглядят интерпретируемыми -- например ['film', 'good', 'story', 'one', 'really', 'also', 'well', 'make', 'though', 'pretty', 'much', 'little', 'get', 'look', 'give'] и ['father', 'mother', 'son', 'brother', 'daughter', 'god', 'church', 'jesus', 'christian', 'andy', 'religious', 'religion', 'younger', 'priest', 'dad']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_augmented_train_data_for_bigartm(data, bigartm_data_path, top_tokens, n_words, n_docs):\n",
    "    with open(bigartm_data_path, 'w') as f:\n",
    "        for i, sample in enumerate(data):\n",
    "            f.write('review_{} {} |@score {}\\n'.format(i, sample['text'], sample['score']))\n",
    "        for i in range(n_docs):\n",
    "            for topic_name in model.topic_names:\n",
    "                text = 'virtual_{}_{} '.format(topic_name, i)\n",
    "                for j in range(n_words):\n",
    "                    text += top_tokens[topic_name][i] + ' '\n",
    "                text += '|@score 0\\n'\n",
    "                f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 документ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469860.03125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12512.8642578125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9772.29296875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8345.4931640625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7528.57568359375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7126.7275390625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6907.3564453125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6783.6923828125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6708.8642578125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6663.83935546875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.16      0.06      0.09       493\n",
      "           4       0.28      0.23      0.25       561\n",
      "           7       0.27      0.14      0.19       506\n",
      "           8       0.26      0.13      0.17       591\n",
      "           9       0.27      0.08      0.12       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36] [0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469848.6875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12512.21484375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9771.7373046875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8344.7744140625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7527.859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7125.93505859375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6906.5673828125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6782.943359375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6708.048828125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6662.900390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.18      0.07      0.10       406\n",
      "           3       0.19      0.06      0.09       493\n",
      "           4       0.26      0.23      0.25       561\n",
      "           7       0.27      0.14      0.19       506\n",
      "           8       0.23      0.11      0.15       591\n",
      "           9       0.17      0.06      0.09       431\n",
      "          10       0.41      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36] [0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469837.34375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12511.5615234375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9771.1708984375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8344.0498046875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7527.1162109375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7125.1240234375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6905.76416015625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6782.19970703125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6707.265625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6662.0322265625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.77      0.54      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.18      0.06      0.10       493\n",
      "           4       0.27      0.22      0.24       561\n",
      "           7       0.27      0.14      0.18       506\n",
      "           8       0.24      0.10      0.14       591\n",
      "           9       0.19      0.07      0.10       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.26      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36] [0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469826.0\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12510.9013671875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9770.5986328125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8343.3291015625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7526.3564453125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7124.2841796875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6904.9384765625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6781.43408203125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6706.4580078125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6661.23046875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.55      1035\n",
      "           2       0.17      0.08      0.11       406\n",
      "           3       0.18      0.08      0.11       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.31      0.16      0.21       506\n",
      "           8       0.28      0.14      0.18       591\n",
      "           9       0.24      0.10      0.14       431\n",
      "          10       0.41      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.29      0.27      0.26      5000\n",
      "weighted avg       0.32      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469814.625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12510.2333984375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9770.0126953125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8342.5966796875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7525.5947265625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7123.43359375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6904.09912109375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6780.630859375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6705.62255859375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6660.40478515625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.15      0.07      0.09       406\n",
      "           3       0.18      0.07      0.10       493\n",
      "           4       0.30      0.23      0.26       561\n",
      "           7       0.29      0.17      0.21       506\n",
      "           8       0.24      0.10      0.14       591\n",
      "           9       0.22      0.09      0.13       431\n",
      "          10       0.39      0.64      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469803.3125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12509.560546875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9769.40625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8341.8583984375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7524.8232421875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7122.5625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6903.24267578125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6779.79736328125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6704.744140625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6659.56201171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.13      0.04      0.06       406\n",
      "           3       0.20      0.08      0.11       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.27      0.14      0.18       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.19      0.06      0.10       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469791.96875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12508.880859375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9768.7861328125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8341.11328125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7524.048828125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7121.69873046875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6902.400390625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6778.96142578125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6703.85205078125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6658.7041015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.16      0.06      0.09       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.30      0.15      0.20       506\n",
      "           8       0.27      0.12      0.16       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.40      0.67      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469780.625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12508.193359375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9768.158203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8340.3662109375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7523.27685546875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7120.85546875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6901.56591796875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6778.130859375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6702.955078125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6657.79150390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.79      0.55      1035\n",
      "           2       0.17      0.06      0.08       406\n",
      "           3       0.21      0.08      0.11       493\n",
      "           4       0.28      0.22      0.25       561\n",
      "           7       0.25      0.13      0.17       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.18      0.06      0.09       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469769.28125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12507.5009765625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9767.5126953125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8339.611328125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7522.50537109375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7120.0361328125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6900.76220703125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6777.2978515625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6702.05517578125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6656.8671875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.80      0.56      1035\n",
      "           2       0.15      0.06      0.08       406\n",
      "           3       0.18      0.06      0.10       493\n",
      "           4       0.29      0.25      0.27       561\n",
      "           7       0.25      0.13      0.17       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.41      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469757.9375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12506.80078125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9766.8525390625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8338.845703125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7521.74072265625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7119.23046875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6899.95849609375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6776.4521484375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6701.12158203125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6655.95068359375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.80      0.56      1035\n",
      "           2       0.14      0.05      0.07       406\n",
      "           3       0.17      0.07      0.10       493\n",
      "           4       0.28      0.22      0.25       561\n",
      "           7       0.29      0.14      0.19       506\n",
      "           8       0.28      0.13      0.18       591\n",
      "           9       0.20      0.07      0.11       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469746.59375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12506.0966796875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9766.1826171875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8338.0576171875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7520.99658203125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7118.46142578125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6899.1572265625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6775.5625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6700.16357421875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6655.017578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.20      0.07      0.11       493\n",
      "           4       0.28      0.23      0.26       561\n",
      "           7       0.26      0.13      0.18       506\n",
      "           8       0.29      0.12      0.17       591\n",
      "           9       0.21      0.07      0.10       431\n",
      "          10       0.41      0.69      0.52       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469735.25\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12505.3837890625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9765.4951171875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8337.2451171875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7520.2490234375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7117.7373046875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6898.392578125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6774.6171875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6699.07421875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6653.93603515625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.12      0.05      0.07       406\n",
      "           3       0.18      0.07      0.10       493\n",
      "           4       0.27      0.22      0.24       561\n",
      "           7       0.28      0.14      0.19       506\n",
      "           8       0.29      0.13      0.18       591\n",
      "           9       0.22      0.08      0.11       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469723.9375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12504.6640625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9764.79296875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8336.4228515625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7519.46875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7117.02587890625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6897.74267578125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6773.75927734375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6697.91552734375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6652.76806640625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.55      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.16      0.06      0.09       493\n",
      "           4       0.28      0.24      0.26       561\n",
      "           7       0.31      0.16      0.21       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.41      0.66      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469712.59375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12503.9375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9764.0908203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8335.5947265625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7518.666015625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7116.2587890625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6897.06494140625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6773.09912109375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6697.1884765625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6651.94921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.18      0.07      0.10       493\n",
      "           4       0.27      0.24      0.25       561\n",
      "           7       0.26      0.13      0.18       506\n",
      "           8       0.26      0.11      0.15       591\n",
      "           9       0.22      0.07      0.11       431\n",
      "          10       0.41      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469701.25\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12503.20703125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9763.37109375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8334.7509765625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7517.845703125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7115.43310546875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6896.1787109375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6772.08984375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6696.08154296875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6650.85498046875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.14      0.04      0.07       406\n",
      "           3       0.17      0.07      0.10       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.28      0.14      0.18       506\n",
      "           8       0.24      0.11      0.15       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.41      0.66      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469689.9375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12502.470703125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9762.6376953125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8333.90234375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7517.0322265625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7114.59326171875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6895.16845703125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6770.82421875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6694.6171875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6649.26171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.56      1035\n",
      "           2       0.19      0.07      0.11       406\n",
      "           3       0.18      0.07      0.10       493\n",
      "           4       0.26      0.22      0.24       561\n",
      "           7       0.32      0.15      0.20       506\n",
      "           8       0.28      0.12      0.17       591\n",
      "           9       0.23      0.08      0.12       431\n",
      "          10       0.40      0.67      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469678.59375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12501.7294921875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9761.8896484375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8333.0390625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7516.2197265625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7113.7587890625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6894.1435546875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6769.50537109375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6693.25244140625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6647.8818359375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.16      0.06      0.08       406\n",
      "           3       0.20      0.08      0.11       493\n",
      "           4       0.28      0.23      0.25       561\n",
      "           7       0.27      0.13      0.18       506\n",
      "           8       0.23      0.10      0.14       591\n",
      "           9       0.23      0.08      0.12       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469667.28125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12500.9775390625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9761.1318359375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8332.162109375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7515.40771484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7112.92919921875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6893.0986328125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6768.13671875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6691.736328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6646.34033203125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.55      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.16      0.06      0.09       493\n",
      "           4       0.28      0.23      0.25       561\n",
      "           7       0.26      0.13      0.18       506\n",
      "           8       0.28      0.13      0.18       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469655.9375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12500.22265625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9760.3701171875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8331.279296875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7514.5771484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7112.123046875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6892.0732421875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6766.8310546875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6690.25048828125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6644.82421875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.79      0.55      1035\n",
      "           2       0.14      0.05      0.08       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.27      0.22      0.24       561\n",
      "           7       0.31      0.15      0.20       506\n",
      "           8       0.25      0.11      0.16       591\n",
      "           9       0.20      0.06      0.09       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469644.625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12499.4619140625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9759.6015625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8330.38671875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7513.744140625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7111.3486328125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6891.09228515625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6765.57373046875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6688.8134765625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6643.32666015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.56      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.18      0.07      0.10       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.28      0.14      0.19       506\n",
      "           8       0.26      0.10      0.15       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469633.3125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12498.697265625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9758.8154296875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8329.478515625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7512.88671875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7110.580078125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6890.2060546875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6764.4013671875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6687.5205078125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6641.8994140625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.20      0.08      0.11       493\n",
      "           4       0.27      0.24      0.26       561\n",
      "           7       0.28      0.15      0.20       506\n",
      "           8       0.24      0.11      0.15       591\n",
      "           9       0.20      0.07      0.10       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469621.96875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12497.92578125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9758.0224609375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8328.57421875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7512.0107421875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7109.7890625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6889.35400390625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6763.43359375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6686.34716796875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6640.642578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.14      0.05      0.08       406\n",
      "           3       0.16      0.06      0.09       493\n",
      "           4       0.28      0.25      0.26       561\n",
      "           7       0.25      0.12      0.17       506\n",
      "           8       0.25      0.11      0.15       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.40      0.67      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469610.65625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12497.1494140625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9757.21484375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8327.6611328125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7511.12939453125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7108.953125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6888.443359375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6762.365234375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6685.20654296875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6639.31201171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.56      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.17      0.06      0.09       493\n",
      "           4       0.29      0.26      0.27       561\n",
      "           7       0.28      0.14      0.19       506\n",
      "           8       0.24      0.10      0.15       591\n",
      "           9       0.23      0.07      0.11       431\n",
      "          10       0.40      0.68      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469599.34375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12496.3662109375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9756.3974609375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8326.7490234375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7510.2470703125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7108.1005859375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6887.5712890625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6761.28369140625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6683.919921875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6637.92041015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.18      0.07      0.10       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.26      0.14      0.18       506\n",
      "           8       0.26      0.13      0.17       591\n",
      "           9       0.26      0.09      0.13       431\n",
      "          10       0.42      0.68      0.52       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469588.0\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12495.5810546875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9755.5732421875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8325.8369140625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7509.37255859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7107.2490234375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6886.7451171875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6760.310546875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6682.83349609375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6636.78076171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.14      0.05      0.08       406\n",
      "           3       0.20      0.09      0.12       493\n",
      "           4       0.26      0.23      0.25       561\n",
      "           7       0.30      0.15      0.20       506\n",
      "           8       0.24      0.10      0.14       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.40      0.68      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469576.6875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12494.787109375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9754.73828125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8324.9111328125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7508.49658203125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7106.4111328125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6886.0673828125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6759.701171875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6682.31005859375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6636.3974609375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55      1035\n",
      "           2       0.19      0.07      0.11       406\n",
      "           3       0.19      0.07      0.11       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.27      0.13      0.17       506\n",
      "           8       0.25      0.11      0.16       591\n",
      "           9       0.20      0.08      0.11       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469565.375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12493.98828125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9753.888671875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8323.974609375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7507.6337890625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7105.56591796875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6885.3349609375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6759.03125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6681.48974609375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6635.47119140625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.17      0.06      0.09       493\n",
      "           4       0.26      0.25      0.25       561\n",
      "           7       0.25      0.12      0.16       506\n",
      "           8       0.26      0.11      0.16       591\n",
      "           9       0.18      0.07      0.10       431\n",
      "          10       0.39      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.26      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469554.0625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12493.185546875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9753.0322265625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8323.013671875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7506.765625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7104.70068359375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6884.53759765625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6758.46142578125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6680.6953125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6634.48876953125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.18      0.07      0.10       406\n",
      "           3       0.20      0.07      0.10       493\n",
      "           4       0.25      0.22      0.23       561\n",
      "           7       0.28      0.13      0.18       506\n",
      "           8       0.27      0.11      0.16       591\n",
      "           9       0.24      0.08      0.12       431\n",
      "          10       0.41      0.69      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469542.75\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12492.37890625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9752.15625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8322.033203125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7505.91650390625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7103.83837890625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6883.68798828125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6757.978515625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6680.18212890625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6633.59423828125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.17      0.06      0.09       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.28      0.13      0.18       591\n",
      "           9       0.20      0.06      0.10       431\n",
      "          10       0.41      0.64      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469531.4375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12491.5673828125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9751.271484375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8321.0205078125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7505.06005859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7102.98974609375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6882.8115234375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6757.32373046875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6680.056640625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6633.07861328125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.13      0.05      0.07       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.28      0.24      0.25       561\n",
      "           7       0.29      0.14      0.18       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.19      0.07      0.10       431\n",
      "          10       0.41      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469520.125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12490.748046875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9750.3720703125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8319.984375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7504.14501953125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7102.16259765625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6881.95654296875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6756.490234375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6679.4296875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6632.94482421875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.17      0.05      0.08       406\n",
      "           3       0.20      0.07      0.10       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.25      0.13      0.17       506\n",
      "           8       0.25      0.11      0.16       591\n",
      "           9       0.20      0.07      0.11       431\n",
      "          10       0.41      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469508.8125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12489.9267578125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9749.4619140625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8318.9287109375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7503.162109375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7101.2666015625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6881.16015625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6755.61181640625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6678.4873046875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6631.73193359375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.19      0.06      0.09       406\n",
      "           3       0.19      0.06      0.09       493\n",
      "           4       0.29      0.26      0.27       561\n",
      "           7       0.26      0.13      0.18       506\n",
      "           8       0.28      0.12      0.17       591\n",
      "           9       0.22      0.07      0.11       431\n",
      "          10       0.40      0.67      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469497.5\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12489.0986328125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9748.544921875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8317.8515625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7502.14990234375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7100.22998046875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6880.15234375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6754.69384765625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6677.57373046875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6630.6689453125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.16      0.05      0.08       406\n",
      "           3       0.22      0.08      0.12       493\n",
      "           4       0.27      0.24      0.25       561\n",
      "           7       0.27      0.13      0.18       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.21      0.07      0.10       431\n",
      "          10       0.40      0.67      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469486.1875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12488.26953125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9747.62109375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8316.755859375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7501.1181640625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7099.14599609375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6879.0546875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6753.54248046875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6676.279296875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6629.2158203125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.13      0.05      0.07       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.26      0.23      0.24       561\n",
      "           7       0.25      0.12      0.16       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469474.875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12487.4326171875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9746.689453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8315.6455078125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7500.05908203125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7098.015625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6877.9189453125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6752.4130859375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6675.0498046875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6627.95361328125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.14      0.05      0.08       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.30      0.14      0.19       506\n",
      "           8       0.25      0.11      0.15       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469463.59375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12486.591796875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9745.744140625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8314.5205078125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7498.98486328125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7096.8583984375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6876.70458984375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6751.24853515625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6674.013671875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6626.98291015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.17      0.06      0.09       493\n",
      "           4       0.23      0.23      0.23       561\n",
      "           7       0.28      0.13      0.18       506\n",
      "           8       0.25      0.11      0.16       591\n",
      "           9       0.22      0.07      0.11       431\n",
      "          10       0.41      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469452.28125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12485.74609375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9744.79296875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8313.38671875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7497.89501953125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7095.68359375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6875.4873046875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6750.0078125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6672.85791015625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6625.80126953125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.54      1035\n",
      "           2       0.18      0.07      0.10       406\n",
      "           3       0.18      0.06      0.10       493\n",
      "           4       0.26      0.22      0.24       561\n",
      "           7       0.30      0.15      0.20       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469440.96875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12484.8974609375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9743.83203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8312.2431640625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7496.79345703125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7094.52587890625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6874.30859375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6748.83740234375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6671.75\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6624.62646484375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.55      1035\n",
      "           2       0.19      0.06      0.10       406\n",
      "           3       0.20      0.07      0.11       493\n",
      "           4       0.25      0.24      0.25       561\n",
      "           7       0.27      0.13      0.17       506\n",
      "           8       0.26      0.11      0.16       591\n",
      "           9       0.20      0.07      0.11       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469429.65625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12484.04296875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9742.8623046875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8311.0859375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7495.67236328125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7093.333984375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6873.05712890625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6747.57421875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6670.5029296875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6623.4921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.18      0.06      0.09       493\n",
      "           4       0.28      0.23      0.25       561\n",
      "           7       0.30      0.15      0.20       506\n",
      "           8       0.26      0.12      0.17       591\n",
      "           9       0.21      0.07      0.10       431\n",
      "          10       0.40      0.67      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.37      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469418.375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12483.1845703125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9741.8876953125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8309.916015625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7494.546875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7092.1259765625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6871.7890625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6746.3681640625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6669.248046875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6622.33447265625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.27      0.24      0.26       561\n",
      "           7       0.28      0.13      0.18       506\n",
      "           8       0.28      0.14      0.19       591\n",
      "           9       0.22      0.08      0.11       431\n",
      "          10       0.41      0.66      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469407.0625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12482.322265625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9740.90625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8308.7333984375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7493.41845703125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7090.9169921875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6870.58642578125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6745.2138671875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6668.03271484375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6621.060546875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.56      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.21      0.08      0.12       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.25      0.12      0.17       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.24      0.09      0.13       431\n",
      "          10       0.39      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469395.78125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12481.455078125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9739.908203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8307.5498046875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7492.296875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7089.73681640625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6869.4453125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6744.11083984375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6666.97998046875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6619.84326171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.18      0.06      0.09       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.29      0.14      0.19       506\n",
      "           8       0.27      0.14      0.18       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.40      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469384.46875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12480.583984375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9738.90625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8306.3359375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7491.15771484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7088.57763671875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6868.35546875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6743.083984375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6665.97412109375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6618.666015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.80      0.56      1035\n",
      "           2       0.16      0.05      0.08       406\n",
      "           3       0.22      0.07      0.11       493\n",
      "           4       0.26      0.23      0.24       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.25      0.11      0.15       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.41      0.68      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469373.1875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12479.708984375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9737.90234375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8305.1162109375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7490.0302734375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7087.43115234375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6867.27734375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6742.02734375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6664.9013671875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6617.560546875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.18      0.06      0.09       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.27      0.22      0.24       561\n",
      "           7       0.26      0.14      0.18       506\n",
      "           8       0.22      0.10      0.14       591\n",
      "           9       0.18      0.07      0.10       431\n",
      "          10       0.41      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469361.875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12478.830078125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9736.8896484375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8303.8935546875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7488.8896484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7086.34375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6866.146484375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6740.93359375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6663.8056640625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6616.50048828125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.27      0.13      0.18       506\n",
      "           8       0.25      0.11      0.15       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.39      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469350.59375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12477.9462890625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9735.8671875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8302.6494140625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7487.71875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7085.25439453125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6865.041015625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6739.78857421875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6662.7490234375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6615.5419921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.17      0.06      0.09       493\n",
      "           4       0.26      0.25      0.26       561\n",
      "           7       0.27      0.13      0.18       506\n",
      "           8       0.27      0.13      0.17       591\n",
      "           9       0.20      0.07      0.10       431\n",
      "          10       0.42      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469339.28125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12477.05859375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9734.83203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8301.40625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7486.53271484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7084.13134765625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6863.89208984375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6738.63330078125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6661.66650390625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6614.5068359375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.18      0.05      0.08       406\n",
      "           3       0.21      0.07      0.11       493\n",
      "           4       0.27      0.26      0.26       561\n",
      "           7       0.28      0.13      0.18       506\n",
      "           8       0.28      0.14      0.18       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.39      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469328.0\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12476.1669921875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9733.7919921875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8300.154296875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7485.34130859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7083.0107421875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6862.771484375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6737.4501953125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6660.56103515625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6613.5888671875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.78      0.56      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.17      0.07      0.10       493\n",
      "           4       0.26      0.25      0.26       561\n",
      "           7       0.26      0.13      0.17       506\n",
      "           8       0.27      0.12      0.16       591\n",
      "           9       0.20      0.07      0.11       431\n",
      "          10       0.39      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469316.71875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12475.271484375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9732.7451171875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8298.904296875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7484.13623046875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7081.90576171875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6861.69482421875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6736.2890625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6659.373046875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6612.70068359375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.16      0.06      0.08       406\n",
      "           3       0.20      0.07      0.11       493\n",
      "           4       0.28      0.23      0.26       561\n",
      "           7       0.26      0.13      0.17       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.21      0.08      0.12       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.36, 0.36] [0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.3, 0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3]\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "f1s = []\n",
    "for n in range(1, 50):\n",
    "    prepare_augmented_train_data_for_bigartm(train_set, 'aug_train_{}.txt'.format(n), top_tokens, n, 1)\n",
    "    bv_aug_train = artm.BatchVectorizer(data_path='aug_train_{}.txt'.format(n), data_format='vowpal_wabbit', \n",
    "                                        batch_size=10000, target_folder='aug_train_{}_batches'.format(n))\n",
    "    new_model = artm.ARTM(num_topics=100, dictionary=dictionary, \n",
    "                      class_ids={'@default_class': 1.0, '@score': 5})\n",
    "    new_model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=100))\n",
    "    new_model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@score'))\n",
    "    new_model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary))\n",
    "\n",
    "    new_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_default', \n",
    "                                                               tau=10000, class_ids=['@default_class']))\n",
    "    new_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_score', \n",
    "                                                               tau=100, class_ids=['@score']))\n",
    "\n",
    "    for i in range(10):\n",
    "        new_model.fit_offline(bv_aug_train, num_collection_passes=1)\n",
    "        sparsity = new_model.score_tracker['sparsity'].value[-1]\n",
    "        perplexity = new_model.score_tracker['perplexity'].value[-1]\n",
    "        print('Iteration {}: sparsity = {}, perplexity = {}'.format(i, sparsity, perplexity))\n",
    "\n",
    "    X_train_pd = new_model.transform(bv_train)\n",
    "    X_train = np.array([X_train_pd[i].values for i in range(len(train_scores))])\n",
    "    y_train = np.array(train_scores)\n",
    "    X_val_pd = new_model.transform(bv_val)\n",
    "    X_val = np.array([X_val_pd[i].values for i in range(len(val_scores))])\n",
    "    y_val = np.array(val_scores)\n",
    "\n",
    "    classifier = RandomForestClassifier(n_estimators=50)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    y_val_pred = classifier.predict(X_val)\n",
    "    results = {}\n",
    "    results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "    results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "    \n",
    "    print(results['val'])\n",
    "    \n",
    "    spl = results['val'].split()\n",
    "    iddd = spl.index('accuracy') + 1\n",
    "    accs.append(float(spl[iddd]))\n",
    "    f1s.append(float(spl[iddd + 12]))\n",
    "    print(accs, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accs)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f1s)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469814.4375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12511.7158203125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9771.0078125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8343.287109375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7525.9326171875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7124.005859375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6904.41357421875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6780.25390625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6705.14990234375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6660.06640625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.79      0.55      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.19      0.08      0.11       493\n",
      "           4       0.28      0.21      0.24       561\n",
      "           7       0.28      0.14      0.19       506\n",
      "           8       0.24      0.11      0.15       591\n",
      "           9       0.26      0.08      0.13       431\n",
      "          10       0.40      0.67      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.36] [0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469757.5\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12509.869140625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9769.0751953125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8340.2919921875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7522.50732421875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7120.447265625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6900.7666015625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6776.09423828125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6700.5244140625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6655.26708984375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.14      0.05      0.08       406\n",
      "           3       0.19      0.08      0.11       493\n",
      "           4       0.29      0.25      0.27       561\n",
      "           7       0.27      0.14      0.19       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.19      0.07      0.10       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37] [0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469700.625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12507.9697265625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9767.013671875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8337.2490234375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7519.0810546875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7116.828125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6897.16015625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6772.3603515625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6696.30419921875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6650.7041015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.18      0.06      0.09       493\n",
      "           4       0.28      0.24      0.26       561\n",
      "           7       0.24      0.14      0.18       506\n",
      "           8       0.27      0.12      0.17       591\n",
      "           9       0.17      0.06      0.09       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36] [0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469643.78125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12506.0146484375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9764.8271484375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8334.130859375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7515.65576171875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7113.1044921875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6893.26611328125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6768.400390625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6692.015625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6646.39013671875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.18      0.06      0.09       406\n",
      "           3       0.19      0.08      0.11       493\n",
      "           4       0.29      0.24      0.27       561\n",
      "           7       0.26      0.15      0.19       506\n",
      "           8       0.29      0.15      0.20       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469586.96875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12504.0009765625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9762.517578125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8330.958984375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7512.28369140625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7109.43505859375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6889.22265625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6764.07958984375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6687.3056640625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6641.0380859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.14      0.05      0.08       406\n",
      "           3       0.17      0.06      0.09       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.30      0.15      0.20       506\n",
      "           8       0.24      0.11      0.15       591\n",
      "           9       0.17      0.06      0.09       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469530.15625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12501.9365234375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9760.0947265625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8327.6953125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7508.94287109375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7105.95703125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6885.18115234375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6759.4677734375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6682.302734375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6635.95703125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.19      0.06      0.09       406\n",
      "           3       0.18      0.07      0.10       493\n",
      "           4       0.30      0.25      0.27       561\n",
      "           7       0.25      0.12      0.17       506\n",
      "           8       0.27      0.12      0.17       591\n",
      "           9       0.19      0.07      0.10       431\n",
      "          10       0.39      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469473.40625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12499.8095703125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9757.5439453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8324.3388671875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7505.64892578125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7102.69921875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6881.6923828125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6755.41796875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6677.7529296875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6631.17578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55      1035\n",
      "           2       0.15      0.06      0.08       406\n",
      "           3       0.21      0.07      0.11       493\n",
      "           4       0.28      0.27      0.27       561\n",
      "           7       0.28      0.15      0.19       506\n",
      "           8       0.27      0.13      0.17       591\n",
      "           9       0.24      0.07      0.11       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469416.6875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12497.6328125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9754.8857421875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8320.8896484375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7502.36865234375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7099.4111328125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6878.37451171875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6751.8349609375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6673.70947265625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6626.994140625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.16      0.07      0.09       406\n",
      "           3       0.20      0.08      0.12       493\n",
      "           4       0.27      0.22      0.24       561\n",
      "           7       0.29      0.14      0.19       506\n",
      "           8       0.24      0.12      0.16       591\n",
      "           9       0.22      0.07      0.11       431\n",
      "          10       0.40      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469359.96875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12495.40234375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9752.1162109375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8317.345703125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7499.0810546875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7096.013671875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6874.86279296875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6748.36083984375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6670.4384765625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6623.962890625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.21      0.08      0.11       493\n",
      "           4       0.28      0.24      0.26       561\n",
      "           7       0.27      0.15      0.20       506\n",
      "           8       0.26      0.11      0.16       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469303.3125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12493.123046875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9749.2431640625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8313.716796875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7495.80908203125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7092.638671875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6871.18310546875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6744.36328125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6666.375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6619.84228515625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.20      0.08      0.11       493\n",
      "           4       0.31      0.25      0.28       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.27      0.11      0.16       591\n",
      "           9       0.21      0.07      0.11       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469246.6875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12490.7890625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9746.244140625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8309.984375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7492.56787109375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7089.44677734375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6867.56982421875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6740.2861328125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6661.96142578125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6615.27734375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.80      0.56      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.19      0.07      0.11       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.25      0.12      0.17       506\n",
      "           8       0.26      0.12      0.16       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.40      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469190.0625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12488.404296875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9743.1318359375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8306.1162109375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7489.17041015625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7086.4072265625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6864.275390625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6736.58203125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6658.0830078125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6611.291015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.19      0.06      0.09       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.27      0.16      0.20       506\n",
      "           8       0.27      0.13      0.18       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.41      0.67      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469133.5\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12485.97265625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9739.9140625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8302.103515625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7485.62646484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7083.25830078125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6861.31103515625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6733.81689453125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6655.27099609375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6608.2294921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.79      0.55      1035\n",
      "           2       0.15      0.05      0.07       406\n",
      "           3       0.18      0.07      0.10       493\n",
      "           4       0.28      0.24      0.26       561\n",
      "           7       0.26      0.13      0.17       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.22      0.07      0.11       431\n",
      "          10       0.40      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469076.96875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12483.4921875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9736.6015625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8297.9736328125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7481.86181640625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7079.99755859375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6858.07568359375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6730.7119140625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6652.8125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6605.8017578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.54      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.20      0.07      0.11       493\n",
      "           4       0.26      0.25      0.25       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.26      0.12      0.16       591\n",
      "           9       0.18      0.06      0.09       431\n",
      "          10       0.40      0.64      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469020.4375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12480.9697265625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9733.1767578125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8293.720703125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7478.00732421875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7076.5224609375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6854.59228515625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6726.98193359375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6648.8681640625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6601.7978515625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.19      0.06      0.10       493\n",
      "           4       0.29      0.27      0.28       561\n",
      "           7       0.28      0.15      0.20       506\n",
      "           8       0.26      0.13      0.17       591\n",
      "           9       0.21      0.07      0.10       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468963.96875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12478.3974609375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9729.6552734375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8289.31640625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7474.0380859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7072.9755859375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6851.01708984375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6723.21728515625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6645.30810546875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6598.55810546875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.80      0.55      1035\n",
      "           2       0.13      0.04      0.07       406\n",
      "           3       0.21      0.08      0.11       493\n",
      "           4       0.29      0.25      0.27       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.20      0.07      0.10       431\n",
      "          10       0.41      0.64      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468907.53125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12475.78515625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9726.05078125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8284.822265625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7469.96044921875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7069.40087890625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6847.556640625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6719.599609375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6641.5146484375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6594.9111328125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.29      0.25      0.27       561\n",
      "           7       0.28      0.14      0.19       506\n",
      "           8       0.23      0.10      0.14       591\n",
      "           9       0.20      0.06      0.10       431\n",
      "          10       0.39      0.66      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468851.09375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12473.1240234375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9722.3408203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8280.24609375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7465.81298828125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7065.58349609375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6843.8955078125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6715.962890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6637.69287109375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6591.0400390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.12      0.04      0.06       406\n",
      "           3       0.17      0.07      0.10       493\n",
      "           4       0.27      0.24      0.25       561\n",
      "           7       0.27      0.15      0.19       506\n",
      "           8       0.26      0.13      0.17       591\n",
      "           9       0.22      0.07      0.11       431\n",
      "          10       0.40      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468794.71875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12470.421875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9718.5498046875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8275.6005859375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7461.55322265625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7061.638671875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6840.00927734375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6712.224609375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6633.8056640625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6586.9189453125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.55      1035\n",
      "           2       0.16      0.05      0.08       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.26      0.23      0.25       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.27      0.12      0.16       591\n",
      "           9       0.23      0.10      0.14       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468738.34375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12467.6796875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9714.658203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8270.869140625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7457.19677734375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7057.54833984375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6835.9462890625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6708.1435546875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6629.89111328125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6582.970703125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.55      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.27      0.15      0.19       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.40      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468682.03125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12464.8984375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9710.7001953125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8266.0576171875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7452.84521484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7053.4189453125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6831.771484375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6703.853515625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6625.55419921875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6578.43603515625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.13      0.05      0.07       406\n",
      "           3       0.18      0.06      0.10       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.29      0.14      0.19       506\n",
      "           8       0.25      0.10      0.14       591\n",
      "           9       0.21      0.07      0.10       431\n",
      "          10       0.39      0.66      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468625.71875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12462.076171875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9706.6552734375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8261.134765625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7448.470703125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7049.2880859375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6827.52880859375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6699.458984375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6621.23828125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6574.20263671875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.19      0.08      0.11       406\n",
      "           3       0.24      0.09      0.13       493\n",
      "           4       0.28      0.24      0.26       561\n",
      "           7       0.28      0.16      0.20       506\n",
      "           8       0.28      0.12      0.17       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.29      0.28      0.25      5000\n",
      "weighted avg       0.32      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468569.46875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12459.2138671875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9702.5361328125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8256.1123046875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7444.0390625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7045.3037109375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6823.4736328125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6695.12890625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6617.22265625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6570.54248046875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.15      0.05      0.08       406\n",
      "           3       0.24      0.09      0.13       493\n",
      "           4       0.25      0.22      0.23       561\n",
      "           7       0.29      0.16      0.21       506\n",
      "           8       0.26      0.12      0.17       591\n",
      "           9       0.19      0.06      0.09       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.24      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468513.21875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12456.314453125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9698.3349609375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8251.0244140625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7439.52978515625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7041.275390625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6819.654296875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6691.28173828125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6613.51953125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6567.03955078125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.16      0.06      0.08       406\n",
      "           3       0.18      0.06      0.09       493\n",
      "           4       0.25      0.24      0.25       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.25      0.11      0.16       591\n",
      "           9       0.23      0.08      0.12       431\n",
      "          10       0.39      0.63      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468457.03125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12453.37890625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9694.064453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8245.8505859375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7434.9521484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7037.3076171875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6815.87158203125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6687.4609375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6610.05810546875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6564.2548828125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.79      0.56      1035\n",
      "           2       0.13      0.04      0.06       406\n",
      "           3       0.22      0.09      0.12       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.31      0.16      0.21       506\n",
      "           8       0.27      0.13      0.17       591\n",
      "           9       0.20      0.08      0.11       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468400.84375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12450.4033203125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9689.720703125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8240.5966796875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7430.2099609375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7033.19091796875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6811.77392578125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6683.193359375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6605.55419921875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6560.03466796875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.79      0.55      1035\n",
      "           2       0.12      0.04      0.06       406\n",
      "           3       0.22      0.08      0.12       493\n",
      "           4       0.25      0.25      0.25       561\n",
      "           7       0.31      0.16      0.21       506\n",
      "           8       0.27      0.11      0.16       591\n",
      "           9       0.25      0.09      0.13       431\n",
      "          10       0.41      0.63      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468344.6875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12447.390625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9685.294921875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8235.2529296875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7425.3837890625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7029.076171875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6807.900390625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6679.134765625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6601.3896484375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6556.00830078125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.78      0.54      1035\n",
      "           2       0.11      0.04      0.06       406\n",
      "           3       0.20      0.06      0.10       493\n",
      "           4       0.24      0.22      0.23       561\n",
      "           7       0.26      0.13      0.18       506\n",
      "           8       0.27      0.13      0.18       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.40      0.63      0.49       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.26      0.26      0.24      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468288.59375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12444.3466796875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9680.796875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8229.828125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7420.505859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7024.94970703125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6804.03662109375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6675.35400390625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6597.68115234375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6552.45751953125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.16      0.06      0.08       406\n",
      "           3       0.23      0.09      0.13       493\n",
      "           4       0.27      0.27      0.27       561\n",
      "           7       0.27      0.13      0.18       506\n",
      "           8       0.25      0.10      0.15       591\n",
      "           9       0.23      0.08      0.12       431\n",
      "          10       0.41      0.66      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468232.5\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12441.267578125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9676.228515625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8224.318359375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7415.55712890625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7020.82373046875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6800.3935546875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6671.705078125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6593.89892578125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6548.35009765625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.80      0.56      1035\n",
      "           2       0.13      0.05      0.07       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.26      0.25      0.25       561\n",
      "           7       0.24      0.13      0.17       506\n",
      "           8       0.26      0.11      0.15       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.41      0.66      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468176.46875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12438.15234375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9671.591796875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8218.728515625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7410.48828125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7016.42138671875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6796.63330078125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6668.43603515625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6590.78955078125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6544.90380859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.55      1035\n",
      "           2       0.14      0.05      0.07       406\n",
      "           3       0.21      0.08      0.11       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.27      0.12      0.16       591\n",
      "           9       0.24      0.10      0.14       431\n",
      "          10       0.41      0.66      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468120.4375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12435.0029296875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9666.890625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8213.0703125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7405.27587890625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7011.75341796875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6792.5478515625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6664.69384765625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6587.6103515625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6542.2822265625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.55      1035\n",
      "           2       0.15      0.05      0.08       406\n",
      "           3       0.21      0.08      0.12       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.26      0.13      0.18       506\n",
      "           8       0.24      0.10      0.15       591\n",
      "           9       0.19      0.07      0.10       431\n",
      "          10       0.40      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468064.4375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12431.826171875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9662.1357421875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8207.3447265625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7400.0205078125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7006.92919921875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6788.20849609375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6660.671875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6583.72265625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6538.912109375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.56      1035\n",
      "           2       0.15      0.05      0.08       406\n",
      "           3       0.22      0.08      0.12       493\n",
      "           4       0.28      0.29      0.29       561\n",
      "           7       0.25      0.14      0.18       506\n",
      "           8       0.27      0.12      0.17       591\n",
      "           9       0.21      0.08      0.12       431\n",
      "          10       0.40      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468008.46875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12428.6123046875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9657.3203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8201.556640625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7394.77587890625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7002.04443359375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6783.69970703125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6656.43310546875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6579.2763671875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6534.62158203125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.79      0.56      1035\n",
      "           2       0.12      0.03      0.05       406\n",
      "           3       0.23      0.09      0.13       493\n",
      "           4       0.26      0.24      0.25       561\n",
      "           7       0.27      0.14      0.18       506\n",
      "           8       0.26      0.12      0.16       591\n",
      "           9       0.23      0.09      0.13       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467952.5625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12425.369140625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9652.4365234375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8195.6865234375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7389.42626953125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6997.2373046875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6779.3427734375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6652.41845703125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6575.0888671875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6530.025390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.14      0.05      0.08       406\n",
      "           3       0.20      0.08      0.11       493\n",
      "           4       0.25      0.25      0.25       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.24      0.12      0.16       591\n",
      "           9       0.24      0.08      0.12       431\n",
      "          10       0.41      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467896.65625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12422.0927734375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9647.49609375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8189.78173828125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7383.97265625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6992.1005859375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6774.4931640625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6648.0263671875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6570.81884765625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6525.4951171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.21      0.08      0.12       493\n",
      "           4       0.26      0.25      0.25       561\n",
      "           7       0.30      0.15      0.20       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.23      0.09      0.13       431\n",
      "          10       0.39      0.63      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467840.78125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12418.787109375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9642.4990234375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8183.8193359375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7378.5029296875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6986.84912109375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6769.26318359375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6642.8125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6565.85498046875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6520.396484375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.21      0.08      0.11       493\n",
      "           4       0.25      0.24      0.25       561\n",
      "           7       0.30      0.15      0.20       506\n",
      "           8       0.23      0.11      0.15       591\n",
      "           9       0.24      0.09      0.13       431\n",
      "          10       0.40      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467784.9375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12415.451171875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9637.435546875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8177.8251953125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7373.013671875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6981.625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6764.1123046875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6637.7373046875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6561.08447265625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6515.646484375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.55      1035\n",
      "           2       0.16      0.05      0.08       406\n",
      "           3       0.24      0.09      0.13       493\n",
      "           4       0.29      0.26      0.27       561\n",
      "           7       0.27      0.14      0.18       506\n",
      "           8       0.23      0.10      0.14       591\n",
      "           9       0.18      0.06      0.09       431\n",
      "          10       0.40      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467729.125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12412.0859375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9632.3193359375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8171.77001953125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7367.4755859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6976.298828125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6758.73388671875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6632.17626953125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6555.8837890625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6510.8330078125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55      1035\n",
      "           2       0.16      0.06      0.08       406\n",
      "           3       0.23      0.09      0.13       493\n",
      "           4       0.24      0.25      0.25       561\n",
      "           7       0.31      0.17      0.22       506\n",
      "           8       0.27      0.13      0.18       591\n",
      "           9       0.24      0.08      0.12       431\n",
      "          10       0.41      0.64      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.27      0.25      5000\n",
      "weighted avg       0.32      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467673.34375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12408.6923828125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9627.1494140625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8165.6279296875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7361.90966796875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6970.8701171875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6753.294921875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6626.7734375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6550.89013671875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6506.34130859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.14      0.05      0.08       406\n",
      "           3       0.22      0.08      0.11       493\n",
      "           4       0.25      0.24      0.25       561\n",
      "           7       0.27      0.14      0.19       506\n",
      "           8       0.24      0.11      0.15       591\n",
      "           9       0.26      0.09      0.13       431\n",
      "          10       0.41      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467617.59375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12405.2666015625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9621.9326171875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8159.42578125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7356.390625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6965.74267578125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6748.16943359375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6621.62255859375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6546.0400390625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6502.029296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.54      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.24      0.09      0.13       493\n",
      "           4       0.29      0.26      0.27       561\n",
      "           7       0.28      0.14      0.19       506\n",
      "           8       0.25      0.12      0.16       591\n",
      "           9       0.26      0.09      0.13       431\n",
      "          10       0.39      0.63      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467561.875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12401.8173828125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9616.6513671875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8153.19970703125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7350.787109375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6960.70458984375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6743.19677734375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6616.32470703125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6540.85595703125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6497.4033203125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.75      0.55      1035\n",
      "           2       0.14      0.05      0.07       406\n",
      "           3       0.22      0.09      0.13       493\n",
      "           4       0.27      0.27      0.27       561\n",
      "           7       0.28      0.14      0.19       506\n",
      "           8       0.25      0.12      0.17       591\n",
      "           9       0.24      0.09      0.13       431\n",
      "          10       0.40      0.63      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467506.1875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12398.33984375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9611.3271484375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8146.9287109375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7345.16796875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6955.5634765625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6738.19921875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6611.30419921875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6535.751953125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6492.4921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.11      0.04      0.06       406\n",
      "           3       0.21      0.08      0.11       493\n",
      "           4       0.27      0.26      0.27       561\n",
      "           7       0.27      0.15      0.19       506\n",
      "           8       0.27      0.13      0.18       591\n",
      "           9       0.18      0.06      0.08       431\n",
      "          10       0.39      0.63      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467450.53125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12394.8369140625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9605.95703125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8140.60205078125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7339.40380859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6950.34814453125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6733.259765625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6605.97900390625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6530.1005859375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6486.900390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.14      0.06      0.08       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.26      0.25      0.25       561\n",
      "           7       0.26      0.14      0.18       506\n",
      "           8       0.27      0.13      0.18       591\n",
      "           9       0.24      0.09      0.13       431\n",
      "          10       0.40      0.62      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467394.90625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12391.3046875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9600.5341796875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8134.21044921875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7333.337890625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6944.453125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6727.54833984375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6600.427734375\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6524.47509765625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6480.666015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.15      0.05      0.08       406\n",
      "           3       0.22      0.09      0.13       493\n",
      "           4       0.26      0.25      0.25       561\n",
      "           7       0.30      0.15      0.20       506\n",
      "           8       0.25      0.13      0.17       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.40      0.64      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467339.3125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12387.74609375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9595.0673828125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8127.77197265625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7327.20361328125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6938.537109375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6721.72998046875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6594.80029296875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6519.0654296875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6476.0302734375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.55      1035\n",
      "           2       0.13      0.05      0.07       406\n",
      "           3       0.19      0.07      0.11       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.26      0.14      0.19       506\n",
      "           8       0.27      0.15      0.19       591\n",
      "           9       0.22      0.07      0.10       431\n",
      "          10       0.40      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467283.75\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12384.1630859375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9589.5419921875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8121.30322265625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7321.0546875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6932.68017578125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6716.13232421875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6589.47705078125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6513.833984375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6471.23388671875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.56      1035\n",
      "           2       0.13      0.05      0.07       406\n",
      "           3       0.24      0.09      0.13       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.28      0.14      0.19       506\n",
      "           8       0.26      0.12      0.17       591\n",
      "           9       0.21      0.07      0.11       431\n",
      "          10       0.40      0.65      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467228.1875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12380.5537109375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9583.982421875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8114.78076171875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7314.9130859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6926.837890625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6710.54443359375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6584.25439453125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6508.849609375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6466.56787109375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.21      0.08      0.11       493\n",
      "           4       0.26      0.25      0.26       561\n",
      "           7       0.31      0.17      0.22       506\n",
      "           8       0.25      0.13      0.17       591\n",
      "           9       0.25      0.09      0.13       431\n",
      "          10       0.40      0.62      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467172.6875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12376.9189453125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9578.373046875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8108.22216796875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7308.80322265625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6921.05029296875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6705.20947265625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6579.0546875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6503.8701171875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6462.02978515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55      1035\n",
      "           2       0.12      0.05      0.07       406\n",
      "           3       0.23      0.08      0.12       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.29      0.15      0.20       506\n",
      "           8       0.23      0.11      0.15       591\n",
      "           9       0.25      0.10      0.15       431\n",
      "          10       0.40      0.65      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467117.21875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12373.2626953125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9572.732421875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8101.61669921875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7302.7333984375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6915.2021484375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6699.90771484375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6574.15966796875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6499.25\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6457.70263671875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.22      0.08      0.12       493\n",
      "           4       0.26      0.27      0.27       561\n",
      "           7       0.25      0.14      0.18       506\n",
      "           8       0.25      0.13      0.17       591\n",
      "           9       0.22      0.08      0.11       431\n",
      "          10       0.40      0.62      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.37, 0.36, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.37, 0.36, 0.36, 0.37, 0.36, 0.35, 0.37, 0.36, 0.37, 0.36, 0.37, 0.37, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.3, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.3, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "f1s = []\n",
    "for n in range(1, 50):\n",
    "    prepare_augmented_train_data_for_bigartm(train_set, 'aug_train_{}.txt'.format(n), top_tokens, n, 5)\n",
    "    bv_aug_train = artm.BatchVectorizer(data_path='aug_train_{}.txt'.format(n), data_format='vowpal_wabbit', \n",
    "                                        batch_size=10000, target_folder='aug_train_{}_batches'.format(n))\n",
    "    new_model = artm.ARTM(num_topics=100, dictionary=dictionary, \n",
    "                      class_ids={'@default_class': 1.0, '@score': 5})\n",
    "    new_model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=100))\n",
    "    new_model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@score'))\n",
    "    new_model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary))\n",
    "\n",
    "    new_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_default', \n",
    "                                                               tau=10000, class_ids=['@default_class']))\n",
    "    new_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_score', \n",
    "                                                               tau=100, class_ids=['@score']))\n",
    "\n",
    "    for i in range(10):\n",
    "        new_model.fit_offline(bv_aug_train, num_collection_passes=1)\n",
    "        sparsity = new_model.score_tracker['sparsity'].value[-1]\n",
    "        perplexity = new_model.score_tracker['perplexity'].value[-1]\n",
    "        print('Iteration {}: sparsity = {}, perplexity = {}'.format(i, sparsity, perplexity))\n",
    "\n",
    "    X_train_pd = new_model.transform(bv_train)\n",
    "    X_train = np.array([X_train_pd[i].values for i in range(len(train_scores))])\n",
    "    y_train = np.array(train_scores)\n",
    "    X_val_pd = new_model.transform(bv_val)\n",
    "    X_val = np.array([X_val_pd[i].values for i in range(len(val_scores))])\n",
    "    y_val = np.array(val_scores)\n",
    "\n",
    "    classifier = RandomForestClassifier(n_estimators=50)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    y_val_pred = classifier.predict(X_val)\n",
    "    results = {}\n",
    "    results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "    results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "    \n",
    "    print(results['val'])\n",
    "    \n",
    "    spl = results['val'].split()\n",
    "    iddd = spl.index('accuracy') + 1\n",
    "    accs.append(float(spl[iddd]))\n",
    "    f1s.append(float(spl[iddd + 12]))\n",
    "    print(accs, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accs)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f1s)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468954.375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13230.2265625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9794.7236328125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8370.8720703125\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7553.13330078125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7146.71435546875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6918.171875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6787.30126953125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6708.38330078125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6663.0302734375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6641.01318359375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6636.52490234375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6645.28955078125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6664.2314453125\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6690.861328125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6722.79833984375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6758.4228515625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6796.00146484375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6834.12841796875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6871.53515625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.53       979\n",
      "           2       0.22      0.10      0.14       458\n",
      "           3       0.15      0.08      0.11       497\n",
      "           4       0.23      0.23      0.23       537\n",
      "           7       0.35      0.22      0.27       533\n",
      "           8       0.23      0.13      0.17       560\n",
      "           9       0.27      0.15      0.19       465\n",
      "          10       0.44      0.62      0.52       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.36] [0.32]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468841.28125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13227.4267578125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9791.609375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8365.51171875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7547.11083984375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7140.912109375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6912.55322265625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6781.2841796875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6701.86767578125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6656.60546875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6635.10302734375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6630.8017578125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6639.33056640625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6657.65380859375\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6683.71142578125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6715.119140625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6749.84228515625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6786.81982421875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6824.43017578125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6861.568359375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.74      0.53       979\n",
      "           2       0.22      0.11      0.14       458\n",
      "           3       0.12      0.07      0.09       497\n",
      "           4       0.24      0.24      0.24       537\n",
      "           7       0.34      0.22      0.26       533\n",
      "           8       0.23      0.14      0.17       560\n",
      "           9       0.23      0.11      0.15       465\n",
      "          10       0.44      0.60      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35] [0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468728.3125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13224.4765625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9788.162109375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8359.962890625\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7541.20068359375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7135.19921875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6906.70849609375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6775.20361328125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6695.59619140625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6650.31298828125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6628.91015625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6624.92236328125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6633.63037109375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6652.1044921875\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6678.47314453125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6710.02880859375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6744.708984375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6781.4853515625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6819.03173828125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6856.46826171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.73      0.53       979\n",
      "           2       0.22      0.10      0.14       458\n",
      "           3       0.17      0.09      0.12       497\n",
      "           4       0.22      0.23      0.23       537\n",
      "           7       0.36      0.24      0.29       533\n",
      "           8       0.21      0.12      0.15       560\n",
      "           9       0.21      0.11      0.14       465\n",
      "          10       0.44      0.60      0.51       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35] [0.32, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468615.4375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13221.3720703125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9784.3974609375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8354.1875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7535.27734375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7129.3134765625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6900.88525390625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6769.52685546875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6689.7236328125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6644.1953125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6622.6328125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6619.353515625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6629.21044921875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6648.3486328125\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6675.345703125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6708.00537109375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6743.91162109375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6781.41259765625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6819.03173828125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6855.7919921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.73      0.53       979\n",
      "           2       0.20      0.09      0.13       458\n",
      "           3       0.16      0.09      0.12       497\n",
      "           4       0.24      0.26      0.25       537\n",
      "           7       0.36      0.22      0.27       533\n",
      "           8       0.22      0.12      0.16       560\n",
      "           9       0.28      0.16      0.20       465\n",
      "          10       0.44      0.60      0.51       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36] [0.32, 0.31, 0.31, 0.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468502.71875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13218.1162109375\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9780.3193359375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8348.14453125\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7529.28857421875\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7123.2880859375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6894.73828125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6763.8642578125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6684.3232421875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6637.9521484375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6615.5556640625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6611.83642578125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6622.1240234375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6642.34423828125\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6670.04052734375\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6702.6533203125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6738.16455078125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6774.77099609375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6811.0576171875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6847.20458984375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.20      0.09      0.12       458\n",
      "           3       0.17      0.09      0.12       497\n",
      "           4       0.24      0.26      0.25       537\n",
      "           7       0.33      0.22      0.26       533\n",
      "           8       0.19      0.10      0.13       560\n",
      "           9       0.23      0.11      0.15       465\n",
      "          10       0.44      0.60      0.51       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468390.09375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13214.71875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9775.9248046875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8341.80078125\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7523.26220703125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7117.63623046875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6889.021484375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6758.14697265625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6678.892578125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6632.9228515625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6611.07568359375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6607.39599609375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6617.11181640625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6636.19873046875\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6662.45751953125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6693.79833984375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6728.45849609375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6764.95263671875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6801.64697265625\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6838.1416015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.53       979\n",
      "           2       0.19      0.08      0.11       458\n",
      "           3       0.17      0.09      0.12       497\n",
      "           4       0.25      0.25      0.25       537\n",
      "           7       0.35      0.23      0.28       533\n",
      "           8       0.22      0.12      0.15       560\n",
      "           9       0.26      0.14      0.18       465\n",
      "          10       0.44      0.61      0.51       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468277.625\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13211.1689453125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9771.2158203125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8335.15234375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7517.05615234375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7112.0576171875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6883.49609375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6752.18701171875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6672.427734375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6626.33203125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6604.21337890625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6599.94189453125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6608.9736328125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6627.732421875\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6653.703125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6685.0712890625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6719.83349609375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6756.38525390625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6793.04345703125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6829.40380859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55       979\n",
      "           2       0.20      0.09      0.13       458\n",
      "           3       0.20      0.11      0.14       497\n",
      "           4       0.25      0.27      0.26       537\n",
      "           7       0.34      0.20      0.25       533\n",
      "           8       0.24      0.12      0.16       560\n",
      "           9       0.30      0.15      0.20       465\n",
      "          10       0.43      0.60      0.50       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.30      0.29      0.27      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468165.25\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13207.470703125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9766.2158203125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8328.2255859375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7510.61865234375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7106.53564453125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6878.40185546875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6746.6103515625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6666.19775390625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6619.55810546875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6597.09375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6592.49658203125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6601.26708984375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6620.244140625\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6646.6435546875\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6677.962890625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6712.1005859375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6748.041015625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6784.67236328125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6821.39404296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.53       979\n",
      "           2       0.20      0.08      0.12       458\n",
      "           3       0.18      0.09      0.12       497\n",
      "           4       0.24      0.27      0.25       537\n",
      "           7       0.35      0.23      0.27       533\n",
      "           8       0.22      0.12      0.16       560\n",
      "           9       0.27      0.14      0.19       465\n",
      "          10       0.45      0.61      0.52       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 468053.0\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13203.64453125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9760.931640625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8321.078125\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7504.13427734375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7100.6181640625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6872.806640625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6741.1865234375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6660.552734375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6613.55712890625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6590.81982421875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6586.138671875\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6595.0947265625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6614.54248046875\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6641.25927734375\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6672.54931640625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6706.47265625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6741.93505859375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6777.77587890625\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6813.630859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.73      0.53       979\n",
      "           2       0.17      0.07      0.10       458\n",
      "           3       0.19      0.10      0.13       497\n",
      "           4       0.25      0.27      0.26       537\n",
      "           7       0.35      0.21      0.26       533\n",
      "           8       0.24      0.13      0.17       560\n",
      "           9       0.25      0.14      0.18       465\n",
      "          10       0.44      0.62      0.52       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467940.875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13199.6767578125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9755.3662109375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8313.6748046875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7497.49658203125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7094.26708984375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6866.67431640625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6735.330078125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6654.8349609375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6608.07666015625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6585.3564453125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6580.92724609375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6590.4287109375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6610.18359375\n",
      "Iteration 14: sparsity = 0.9725000262260437, perplexity = 6636.6728515625\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6667.25732421875\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6700.58935546875\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6735.232421875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6770.54296875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6806.275390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.75      0.54       979\n",
      "           2       0.19      0.08      0.12       458\n",
      "           3       0.16      0.08      0.10       497\n",
      "           4       0.24      0.24      0.24       537\n",
      "           7       0.33      0.20      0.25       533\n",
      "           8       0.22      0.12      0.16       560\n",
      "           9       0.25      0.15      0.19       465\n",
      "          10       0.43      0.60      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467828.875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13195.5849609375\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9749.55859375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8305.994140625\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7490.54443359375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7087.64453125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6859.96337890625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6728.5107421875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6648.14111328125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6601.70361328125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6579.08447265625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6574.98974609375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6584.71337890625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6604.5703125\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6630.97412109375\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6661.109375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6693.52587890625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6727.5712890625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6762.5478515625\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6798.1552734375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.76      0.54       979\n",
      "           2       0.21      0.10      0.13       458\n",
      "           3       0.14      0.07      0.10       497\n",
      "           4       0.25      0.26      0.26       537\n",
      "           7       0.34      0.20      0.25       533\n",
      "           8       0.22      0.13      0.17       560\n",
      "           9       0.24      0.12      0.16       465\n",
      "          10       0.44      0.60      0.50       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467717.0\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13191.3681640625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9743.4873046875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8298.02734375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7483.2880859375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7080.77587890625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6853.2783203125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6721.615234375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6641.05078125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6595.03369140625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6573.18212890625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6569.35498046875\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6579.0048828125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6598.46630859375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6624.5732421875\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6654.2490234375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6686.34130859375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6720.28125\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6755.1103515625\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6790.544921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.74      0.54       979\n",
      "           2       0.19      0.09      0.12       458\n",
      "           3       0.18      0.09      0.12       497\n",
      "           4       0.25      0.26      0.26       537\n",
      "           7       0.34      0.22      0.26       533\n",
      "           8       0.22      0.12      0.16       560\n",
      "           9       0.26      0.13      0.17       465\n",
      "          10       0.43      0.62      0.51       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467605.25\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13187.0302734375\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9737.166015625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8289.740234375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7475.609375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7073.775390625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6846.37109375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6715.40625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6635.4736328125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6590.0849609375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6568.5302734375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6564.72802734375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6574.5126953125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6594.49072265625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6621.26708984375\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6652.103515625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6685.35986328125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6720.2724609375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6756.12451171875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6792.22998046875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.19      0.08      0.12       458\n",
      "           3       0.15      0.08      0.11       497\n",
      "           4       0.24      0.24      0.24       537\n",
      "           7       0.35      0.20      0.26       533\n",
      "           8       0.23      0.14      0.17       560\n",
      "           9       0.26      0.13      0.17       465\n",
      "          10       0.44      0.61      0.51       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467493.59375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13182.5693359375\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9730.609375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8281.099609375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7467.39111328125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7066.216796875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6839.0283203125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6708.306640625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6628.75439453125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6583.49755859375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6562.52099609375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6558.84619140625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6568.87646484375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6589.857421875\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6618.10791015625\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6650.12744140625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6684.15869140625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6719.486328125\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6755.55859375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6791.63330078125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.75      0.54       979\n",
      "           2       0.18      0.07      0.10       458\n",
      "           3       0.15      0.08      0.10       497\n",
      "           4       0.22      0.24      0.23       537\n",
      "           7       0.33      0.20      0.25       533\n",
      "           8       0.23      0.13      0.16       560\n",
      "           9       0.28      0.14      0.18       465\n",
      "          10       0.44      0.61      0.51       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467382.09375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13177.9931640625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9723.8134765625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8272.130859375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7458.9326171875\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7058.29443359375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6831.4990234375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6701.42333984375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6622.759765625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6577.7294921875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6556.658203125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6553.70361328125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6563.73095703125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6584.521484375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6613.17578125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6646.33349609375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6681.57421875\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6717.7802734375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6754.95068359375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6791.77294921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.73      0.53       979\n",
      "           2       0.18      0.09      0.12       458\n",
      "           3       0.13      0.06      0.09       497\n",
      "           4       0.22      0.23      0.23       537\n",
      "           7       0.32      0.19      0.24       533\n",
      "           8       0.23      0.14      0.17       560\n",
      "           9       0.26      0.13      0.17       465\n",
      "          10       0.43      0.60      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.26      5000\n",
      "weighted avg       0.30      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467270.6875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13173.3095703125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9716.8076171875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8262.9130859375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7450.17236328125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7050.2099609375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6823.83154296875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6694.54541015625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6616.70703125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6572.40380859375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6551.1533203125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6547.84521484375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6558.0322265625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6578.91650390625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6607.40673828125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6640.833984375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6676.84619140625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6714.06982421875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6751.64111328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6788.41259765625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.75      0.54       979\n",
      "           2       0.18      0.08      0.11       458\n",
      "           3       0.17      0.09      0.11       497\n",
      "           4       0.25      0.25      0.25       537\n",
      "           7       0.34      0.22      0.26       533\n",
      "           8       0.21      0.13      0.16       560\n",
      "           9       0.24      0.14      0.18       465\n",
      "          10       0.44      0.60      0.50       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.28      0.27      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467159.40625\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13168.5146484375\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9709.5908203125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8253.490234375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7441.18017578125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7041.9912109375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6815.84130859375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6686.806640625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6609.583984375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6565.41015625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6544.65478515625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6541.3837890625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6551.5302734375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6572.43310546875\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6601.00732421875\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6634.56640625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6670.859375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6708.25\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6745.8203125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6782.60986328125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.76      0.54       979\n",
      "           2       0.20      0.09      0.13       458\n",
      "           3       0.18      0.10      0.13       497\n",
      "           4       0.25      0.25      0.25       537\n",
      "           7       0.34      0.21      0.26       533\n",
      "           8       0.23      0.15      0.18       560\n",
      "           9       0.28      0.15      0.19       465\n",
      "          10       0.44      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.29      0.27      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 467048.25\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13163.6181640625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9702.1689453125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8243.9013671875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7432.04541015625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7033.5595703125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6807.89892578125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6678.83349609375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6601.56005859375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6557.193359375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6536.1083984375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6533.34130859375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6544.189453125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6565.51806640625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6594.8251953125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6629.1044921875\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6666.548828125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6704.8916015625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6743.1240234375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6780.7275390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.53       979\n",
      "           2       0.18      0.08      0.11       458\n",
      "           3       0.19      0.10      0.13       497\n",
      "           4       0.26      0.26      0.26       537\n",
      "           7       0.32      0.21      0.25       533\n",
      "           8       0.21      0.13      0.16       560\n",
      "           9       0.26      0.14      0.18       465\n",
      "          10       0.44      0.59      0.51       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.28      0.27      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466937.21875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13158.6220703125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9694.546875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8234.14453125\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7422.7392578125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7024.85986328125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6799.47607421875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6670.75048828125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6593.4365234375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6549.37841796875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6528.59716796875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6526.087890625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6537.62158203125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6559.49462890625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6589.0849609375\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6624.0458984375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6661.974609375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6700.84228515625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6739.7060546875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6778.4951171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.73      0.52       979\n",
      "           2       0.17      0.08      0.10       458\n",
      "           3       0.16      0.08      0.11       497\n",
      "           4       0.24      0.24      0.24       537\n",
      "           7       0.35      0.22      0.27       533\n",
      "           8       0.24      0.14      0.18       560\n",
      "           9       0.23      0.12      0.16       465\n",
      "          10       0.44      0.60      0.51       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466826.3125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13153.521484375\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9686.7275390625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8224.1806640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7413.3369140625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7016.18115234375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6790.87255859375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6662.9765625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6586.513671875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6543.3369140625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6523.326171875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6521.0859375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6533.134765625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6555.884765625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6586.240234375\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6621.75244140625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6660.0703125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6699.44677734375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6738.65283203125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6777.65673828125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.73      0.53       979\n",
      "           2       0.21      0.10      0.13       458\n",
      "           3       0.18      0.10      0.13       497\n",
      "           4       0.25      0.23      0.24       537\n",
      "           7       0.31      0.20      0.24       533\n",
      "           8       0.22      0.14      0.17       560\n",
      "           9       0.23      0.13      0.16       465\n",
      "          10       0.43      0.58      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466715.53125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13148.3310546875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9678.7177734375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8214.03515625\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7403.73095703125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 7007.47265625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6782.4736328125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6654.95751953125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6578.5830078125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6535.4541015625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6515.57763671875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6513.55908203125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6525.72998046875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6548.72998046875\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6579.0849609375\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6614.0703125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6651.4423828125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6689.85498046875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6728.4873046875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6766.64453125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.77      0.53       979\n",
      "           2       0.22      0.09      0.13       458\n",
      "           3       0.18      0.09      0.12       497\n",
      "           4       0.25      0.24      0.24       537\n",
      "           7       0.31      0.20      0.24       533\n",
      "           8       0.24      0.14      0.18       560\n",
      "           9       0.23      0.12      0.16       465\n",
      "          10       0.43      0.57      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466604.84375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13143.0390625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9670.5361328125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8203.7119140625\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7393.8515625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6998.3720703125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6773.66748046875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6646.68896484375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6570.97314453125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6527.62255859375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6508.11376953125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6506.10595703125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6517.59326171875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6539.98583984375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6569.70654296875\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6603.93408203125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6640.20751953125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6676.92626953125\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6714.21240234375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6750.76904296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.76      0.54       979\n",
      "           2       0.21      0.09      0.13       458\n",
      "           3       0.17      0.09      0.11       497\n",
      "           4       0.26      0.24      0.25       537\n",
      "           7       0.32      0.21      0.26       533\n",
      "           8       0.20      0.13      0.16       560\n",
      "           9       0.26      0.14      0.18       465\n",
      "          10       0.44      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466494.28125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13137.66015625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9662.16796875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8193.1904296875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7383.9697265625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6989.60009765625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6765.60107421875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6638.82080078125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6563.97705078125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6521.40576171875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6502.7119140625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6501.24951171875\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6513.41357421875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6535.58984375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6564.67919921875\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6598.203125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6633.9248046875\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6670.68505859375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6708.06884765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6744.75439453125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.18      0.08      0.11       458\n",
      "           3       0.15      0.07      0.10       497\n",
      "           4       0.24      0.24      0.24       537\n",
      "           7       0.31      0.19      0.24       533\n",
      "           8       0.21      0.12      0.16       560\n",
      "           9       0.23      0.11      0.15       465\n",
      "          10       0.43      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466383.84375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13132.19921875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9653.63671875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8182.490234375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7374.06298828125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6980.6015625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6757.36572265625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6630.48779296875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6556.2626953125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6514.4404296875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6495.66015625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6494.5908203125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6506.9072265625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6528.99169921875\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6557.3271484375\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6590.1005859375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6625.27294921875\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6661.67919921875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6699.05859375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6736.12744140625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.74      0.53       979\n",
      "           2       0.19      0.09      0.13       458\n",
      "           3       0.17      0.09      0.11       497\n",
      "           4       0.25      0.23      0.24       537\n",
      "           7       0.32      0.21      0.26       533\n",
      "           8       0.20      0.12      0.15       560\n",
      "           9       0.26      0.14      0.18       465\n",
      "          10       0.44      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466273.53125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13126.6416015625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9644.9365234375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8171.62646484375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7364.0205078125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6971.28125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6748.3740234375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6622.3076171875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6547.95751953125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6506.7275390625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6488.708984375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6488.080078125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6500.5419921875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6522.40185546875\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6550.09814453125\n",
      "Iteration 15: sparsity = 0.9725000262260437, perplexity = 6581.91943359375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6616.53125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6652.89697265625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6689.73779296875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6726.53857421875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.18      0.08      0.11       458\n",
      "           3       0.18      0.09      0.12       497\n",
      "           4       0.23      0.24      0.24       537\n",
      "           7       0.33      0.21      0.25       533\n",
      "           8       0.23      0.14      0.17       560\n",
      "           9       0.25      0.12      0.17       465\n",
      "          10       0.43      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466163.34375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13121.005859375\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9636.0703125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8160.6181640625\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7354.03369140625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6962.4384765625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6740.3828125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6615.1318359375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6541.5283203125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6500.79541015625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6482.8974609375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6482.19287109375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6494.51611328125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6516.65380859375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6544.7509765625\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6576.14404296875\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6610.1455078125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6646.37890625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6683.26513671875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6719.5546875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.73      0.52       979\n",
      "           2       0.17      0.07      0.10       458\n",
      "           3       0.20      0.11      0.14       497\n",
      "           4       0.25      0.24      0.24       537\n",
      "           7       0.32      0.19      0.24       533\n",
      "           8       0.22      0.14      0.18       560\n",
      "           9       0.26      0.13      0.17       465\n",
      "          10       0.44      0.59      0.51       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 466053.25\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13115.2890625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9627.0390625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8149.4677734375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7344.09521484375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6953.6572265625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6732.55029296875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6607.85205078125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6534.5966796875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6493.6767578125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6475.3759765625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6475.052734375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6487.470703125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6509.1328125\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6537.03466796875\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6568.66015625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6601.67041015625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6637.10791015625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6673.83984375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6710.33984375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.76      0.53       979\n",
      "           2       0.18      0.08      0.11       458\n",
      "           3       0.15      0.08      0.10       497\n",
      "           4       0.23      0.21      0.22       537\n",
      "           7       0.33      0.20      0.25       533\n",
      "           8       0.23      0.14      0.17       560\n",
      "           9       0.26      0.14      0.19       465\n",
      "          10       0.44      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465943.28125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13109.4892578125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9617.859375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8138.16357421875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7334.0361328125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6945.24951171875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6725.15234375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6600.66748046875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6527.669921875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6487.18896484375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6468.744140625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6468.1025390625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6480.5224609375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6502.2783203125\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6530.33935546875\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6562.09228515625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6595.37158203125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6629.93017578125\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6666.03857421875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6702.087890625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.76      0.53       979\n",
      "           2       0.19      0.08      0.11       458\n",
      "           3       0.16      0.09      0.12       497\n",
      "           4       0.23      0.21      0.22       537\n",
      "           7       0.33      0.21      0.26       533\n",
      "           8       0.25      0.15      0.19       560\n",
      "           9       0.25      0.12      0.16       465\n",
      "          10       0.42      0.59      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465833.4375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13103.61328125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9608.52734375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8126.70068359375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7323.55810546875\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6936.15869140625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6717.642578125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6593.5791015625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6520.509765625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6480.4423828125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6461.8427734375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6460.90380859375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6473.1796875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6494.5400390625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6522.20166015625\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6553.51171875\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6586.8515625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6621.88671875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6657.6640625\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6693.724609375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.18      0.08      0.11       458\n",
      "           3       0.18      0.09      0.12       497\n",
      "           4       0.23      0.22      0.22       537\n",
      "           7       0.34      0.21      0.26       533\n",
      "           8       0.22      0.13      0.16       560\n",
      "           9       0.26      0.14      0.18       465\n",
      "          10       0.43      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465723.71875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13097.6591796875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9599.0517578125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8115.126953125\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7313.048828125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6926.8154296875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6709.9248046875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6586.57666015625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6513.40771484375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6473.21044921875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6454.47802734375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6453.75537109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6466.95947265625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6488.7314453125\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6515.443359375\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6546.3662109375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6580.0703125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6615.38134765625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6651.767578125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6687.84130859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.53       979\n",
      "           2       0.22      0.09      0.13       458\n",
      "           3       0.16      0.09      0.12       497\n",
      "           4       0.23      0.23      0.23       537\n",
      "           7       0.31      0.20      0.24       533\n",
      "           8       0.27      0.17      0.20       560\n",
      "           9       0.26      0.12      0.17       465\n",
      "          10       0.43      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465614.125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13091.6337890625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9589.4306640625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8103.4208984375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7302.44677734375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6916.9794921875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6701.24755859375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6578.806640625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6506.3583984375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6466.76025390625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6448.05126953125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6446.14208984375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6458.7666015625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6481.0205078125\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6509.08056640625\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6541.02783203125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6575.611328125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6611.9228515625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6648.99609375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6685.18603515625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.74      0.52       979\n",
      "           2       0.17      0.07      0.10       458\n",
      "           3       0.19      0.10      0.13       497\n",
      "           4       0.24      0.23      0.23       537\n",
      "           7       0.35      0.21      0.26       533\n",
      "           8       0.24      0.15      0.18       560\n",
      "           9       0.25      0.13      0.17       465\n",
      "          10       0.43      0.58      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465504.625\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13085.5361328125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9579.69140625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8091.591796875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7291.70458984375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6907.59912109375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6692.95849609375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6571.14306640625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6498.998046875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6459.45068359375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6441.267578125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6438.76953125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6449.90234375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6471.46533203125\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6499.49609375\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6531.08984375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6565.1982421875\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6600.72119140625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6636.935546875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6672.83740234375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.76      0.54       979\n",
      "           2       0.19      0.09      0.12       458\n",
      "           3       0.19      0.09      0.12       497\n",
      "           4       0.25      0.23      0.24       537\n",
      "           7       0.31      0.20      0.24       533\n",
      "           8       0.21      0.14      0.17       560\n",
      "           9       0.28      0.14      0.19       465\n",
      "          10       0.44      0.60      0.51       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465395.25\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13079.37109375\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9569.8232421875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8079.62255859375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7280.9306640625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6898.1767578125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6684.77734375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6563.53759765625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6491.3681640625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6451.81494140625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6433.7421875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6431.59912109375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6442.1455078125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6462.623046875\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6489.818359375\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6520.73583984375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6554.3828125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6589.17724609375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6624.2890625\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6659.61572265625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.74      0.53       979\n",
      "           2       0.20      0.08      0.11       458\n",
      "           3       0.17      0.10      0.12       497\n",
      "           4       0.23      0.23      0.23       537\n",
      "           7       0.31      0.20      0.24       533\n",
      "           8       0.25      0.18      0.21       560\n",
      "           9       0.26      0.13      0.18       465\n",
      "          10       0.44      0.58      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465286.0\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13073.1279296875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9559.8544921875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8067.5947265625\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7270.03857421875\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6888.65283203125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6676.53662109375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6556.53369140625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6484.29736328125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6444.146484375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6426.13525390625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6424.14404296875\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6434.73291015625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6454.0625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6480.39794921875\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6510.84912109375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6543.6416015625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6577.7470703125\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6612.05517578125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6645.91064453125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.16      0.07      0.10       458\n",
      "           3       0.17      0.09      0.11       497\n",
      "           4       0.21      0.22      0.22       537\n",
      "           7       0.31      0.20      0.24       533\n",
      "           8       0.21      0.14      0.17       560\n",
      "           9       0.26      0.13      0.17       465\n",
      "          10       0.43      0.56      0.49       971\n",
      "\n",
      "    accuracy                           0.34      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.34      0.30      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465176.875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13066.8212890625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9549.7470703125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8055.49951171875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7259.16796875\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6879.294921875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6668.36279296875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6549.271484375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6477.34765625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6436.96923828125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6418.78369140625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6417.0078125\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6427.37060546875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6446.08251953125\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6471.44091796875\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6501.35205078125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6533.53076171875\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6566.73095703125\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6600.20361328125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6633.0078125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.54       979\n",
      "           2       0.18      0.08      0.11       458\n",
      "           3       0.17      0.10      0.13       497\n",
      "           4       0.24      0.24      0.24       537\n",
      "           7       0.29      0.20      0.23       533\n",
      "           8       0.22      0.15      0.18       560\n",
      "           9       0.25      0.12      0.16       465\n",
      "          10       0.43      0.57      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.30      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 465067.84375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13060.44921875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9539.5361328125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8043.33544921875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7248.27099609375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6869.83251953125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6660.4501953125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6541.97802734375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6469.8310546875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6428.9296875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6409.9716796875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6408.06201171875\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6418.599609375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6437.515625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6462.59130859375\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6491.7529296875\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6523.07568359375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6555.451171875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6587.93017578125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6619.7158203125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.73      0.53       979\n",
      "           2       0.17      0.07      0.10       458\n",
      "           3       0.17      0.09      0.12       497\n",
      "           4       0.24      0.24      0.24       537\n",
      "           7       0.32      0.21      0.25       533\n",
      "           8       0.24      0.15      0.18       560\n",
      "           9       0.23      0.12      0.16       465\n",
      "          10       0.41      0.56      0.47       971\n",
      "\n",
      "    accuracy                           0.34      5000\n",
      "   macro avg       0.27      0.27      0.26      5000\n",
      "weighted avg       0.30      0.34      0.30      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464958.9375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13054.01171875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9529.2109375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8031.09423828125\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7237.3828125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6860.25732421875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6652.09326171875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6534.6953125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6463.3447265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6422.8720703125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6404.26318359375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6402.31005859375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6412.98046875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6431.97021484375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6457.12353515625\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6486.880859375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6518.54150390625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6550.68212890625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6582.78857421875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6614.10595703125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.77      0.54       979\n",
      "           2       0.20      0.07      0.11       458\n",
      "           3       0.18      0.09      0.12       497\n",
      "           4       0.24      0.25      0.24       537\n",
      "           7       0.35      0.23      0.27       533\n",
      "           8       0.27      0.17      0.21       560\n",
      "           9       0.24      0.12      0.16       465\n",
      "          10       0.42      0.57      0.49       971\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464850.15625\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13047.509765625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9518.77734375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8018.78759765625\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7226.37255859375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6850.51220703125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6643.3310546875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6527.12939453125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6456.26123046875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6416.2333984375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6398.37548828125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6397.2373046875\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6408.3818359375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6427.72021484375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6453.11376953125\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6482.65576171875\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6514.71240234375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6547.1484375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6579.19140625\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6610.2529296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.75      0.54       979\n",
      "           2       0.20      0.08      0.12       458\n",
      "           3       0.15      0.08      0.10       497\n",
      "           4       0.22      0.23      0.22       537\n",
      "           7       0.36      0.23      0.28       533\n",
      "           8       0.25      0.17      0.20       560\n",
      "           9       0.22      0.11      0.15       465\n",
      "          10       0.42      0.56      0.48       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464741.46875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13040.947265625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9508.2392578125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 8006.435546875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7215.376953125\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6840.9677734375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6635.1005859375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6520.0126953125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6450.45654296875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6410.74365234375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6393.32666015625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6392.896484375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6404.72509765625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6423.99365234375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6449.6220703125\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6479.33251953125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6511.3251953125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6544.662109375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6577.2001953125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6608.212890625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.75      0.54       979\n",
      "           2       0.21      0.09      0.12       458\n",
      "           3       0.19      0.10      0.13       497\n",
      "           4       0.22      0.21      0.22       537\n",
      "           7       0.32      0.21      0.26       533\n",
      "           8       0.23      0.16      0.19       560\n",
      "           9       0.27      0.12      0.17       465\n",
      "          10       0.43      0.58      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.29      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464632.9375\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13034.32421875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9497.6064453125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7994.02685546875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7204.482421875\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6831.4033203125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6626.84521484375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6513.1552734375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6445.076171875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6406.42919921875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6389.361328125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6388.77978515625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6401.71630859375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6422.6123046875\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6449.53662109375\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6480.630859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6513.927734375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6547.8701171875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6581.3251953125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6613.24951171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.22      0.08      0.11       458\n",
      "           3       0.16      0.09      0.11       497\n",
      "           4       0.21      0.22      0.22       537\n",
      "           7       0.34      0.19      0.25       533\n",
      "           8       0.24      0.16      0.19       560\n",
      "           9       0.25      0.14      0.18       465\n",
      "          10       0.43      0.57      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464524.5\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13027.6416015625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9486.8740234375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7981.55126953125\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7193.73681640625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6821.95751953125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6618.5771484375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6506.00732421875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6439.1083984375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6401.6298828125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6385.580078125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6386.07568359375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6400.7353515625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6424.052734375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6452.67333984375\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6485.07763671875\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6520.37939453125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6556.697265625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6592.50439453125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6627.69921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.21      0.08      0.11       458\n",
      "           3       0.17      0.10      0.13       497\n",
      "           4       0.23      0.23      0.23       537\n",
      "           7       0.29      0.17      0.22       533\n",
      "           8       0.25      0.16      0.20       560\n",
      "           9       0.23      0.12      0.16       465\n",
      "          10       0.43      0.57      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464416.15625\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13020.904296875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9476.060546875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7968.9794921875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7183.0419921875\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6812.98681640625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6610.869140625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6499.07568359375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6432.92431640625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6396.61279296875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6381.83642578125\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6382.84228515625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6398.521484375\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6423.54052734375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6454.32763671875\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6488.66162109375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6525.05517578125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6562.0263671875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6597.27685546875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6631.775390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.76      0.54       979\n",
      "           2       0.19      0.06      0.09       458\n",
      "           3       0.16      0.09      0.12       497\n",
      "           4       0.23      0.25      0.24       537\n",
      "           7       0.33      0.20      0.25       533\n",
      "           8       0.26      0.17      0.20       560\n",
      "           9       0.25      0.12      0.16       465\n",
      "          10       0.42      0.58      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464307.96875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13014.103515625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9465.1416015625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7956.32421875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7172.00244140625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6803.6787109375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6603.41650390625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6493.04248046875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6428.189453125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6392.8857421875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6379.5224609375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6382.9755859375\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6399.61328125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6425.533203125\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6457.2861328125\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6492.79736328125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6529.9443359375\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6566.8427734375\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6601.25341796875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6633.02783203125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.75      0.52       979\n",
      "           2       0.19      0.07      0.10       458\n",
      "           3       0.16      0.09      0.12       497\n",
      "           4       0.22      0.22      0.22       537\n",
      "           7       0.31      0.20      0.25       533\n",
      "           8       0.21      0.13      0.16       560\n",
      "           9       0.23      0.11      0.15       465\n",
      "          10       0.42      0.57      0.48       971\n",
      "\n",
      "    accuracy                           0.34      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.34      0.30      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.34] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464199.875\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13007.2529296875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9454.1484375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7943.59228515625\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7160.806640625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6793.69287109375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6594.4296875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6485.302734375\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6421.59814453125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6387.46435546875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6374.966796875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6379.76123046875\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6397.6982421875\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6424.5576171875\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6456.68701171875\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6492.00341796875\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6528.5791015625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6563.96875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6597.2451171875\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6627.517578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.18      0.06      0.09       458\n",
      "           3       0.16      0.09      0.12       497\n",
      "           4       0.24      0.23      0.23       537\n",
      "           7       0.31      0.20      0.24       533\n",
      "           8       0.24      0.16      0.19       560\n",
      "           9       0.28      0.15      0.19       465\n",
      "          10       0.43      0.58      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.34, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 464091.90625\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 13000.3408203125\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9443.0830078125\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7930.81591796875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7149.6259765625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6784.1181640625\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6586.33154296875\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6478.3994140625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6416.07666015625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6383.427734375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6371.97998046875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6377.80419921875\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6396.6611328125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6423.99609375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6455.744140625\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6490.57275390625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6525.6796875\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6559.72265625\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6591.3916015625\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6620.63818359375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.21      0.08      0.11       458\n",
      "           3       0.18      0.09      0.12       497\n",
      "           4       0.23      0.23      0.23       537\n",
      "           7       0.34      0.21      0.26       533\n",
      "           8       0.25      0.17      0.20       560\n",
      "           9       0.25      0.13      0.17       465\n",
      "          10       0.43      0.58      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.29      0.28      0.27      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.34, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 463984.03125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 12993.384765625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9431.9521484375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7917.9873046875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7138.56103515625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6774.5537109375\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6577.94189453125\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6471.09326171875\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6409.81591796875\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6378.47802734375\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6368.0322265625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6374.6337890625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6394.40673828125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6422.44775390625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6454.6806640625\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6488.5361328125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6522.65966796875\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6555.45751953125\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6586.01220703125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6614.189453125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.75      0.53       979\n",
      "           2       0.17      0.05      0.08       458\n",
      "           3       0.20      0.10      0.14       497\n",
      "           4       0.21      0.23      0.22       537\n",
      "           7       0.32      0.20      0.24       533\n",
      "           8       0.24      0.16      0.19       560\n",
      "           9       0.20      0.09      0.13       465\n",
      "          10       0.42      0.58      0.49       971\n",
      "\n",
      "    accuracy                           0.34      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.34      0.30      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.34, 0.35, 0.35, 0.34] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 463876.28125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 12986.373046875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9420.7255859375\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7905.1123046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7127.3916015625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6765.09326171875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6569.67724609375\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6463.8369140625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6403.974609375\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6374.16357421875\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6364.84765625\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6371.97900390625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6392.345703125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6420.74462890625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6453.47412109375\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6487.53662109375\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6521.525390625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6553.92919921875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6584.43798828125\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6611.87646484375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.75      0.53       979\n",
      "           2       0.18      0.07      0.10       458\n",
      "           3       0.18      0.09      0.12       497\n",
      "           4       0.22      0.24      0.23       537\n",
      "           7       0.32      0.21      0.25       533\n",
      "           8       0.23      0.15      0.18       560\n",
      "           9       0.23      0.11      0.15       465\n",
      "          10       0.42      0.57      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.26      5000\n",
      "weighted avg       0.30      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.34, 0.35, 0.35, 0.34, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 463768.65625\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 12979.3056640625\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9409.4404296875\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7892.16943359375\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7115.916015625\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6754.921875\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6560.4072265625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6455.40283203125\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6396.88916015625\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6368.61767578125\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6360.8193359375\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6368.6025390625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6389.07861328125\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6417.7822265625\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6451.048828125\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6485.91650390625\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6520.99853515625\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6554.5419921875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6586.1943359375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6615.12451171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.54       979\n",
      "           2       0.19      0.07      0.11       458\n",
      "           3       0.18      0.10      0.13       497\n",
      "           4       0.22      0.23      0.23       537\n",
      "           7       0.31      0.20      0.25       533\n",
      "           8       0.24      0.16      0.19       560\n",
      "           9       0.27      0.12      0.17       465\n",
      "          10       0.43      0.59      0.50       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.34, 0.35, 0.35, 0.34, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8487499952316284, perplexity = 463661.125\n",
      "Iteration 1: sparsity = 0.9662500023841858, perplexity = 12972.1904296875\n",
      "Iteration 2: sparsity = 0.9700000286102295, perplexity = 9398.0869140625\n",
      "Iteration 3: sparsity = 0.9700000286102295, perplexity = 7879.2138671875\n",
      "Iteration 4: sparsity = 0.9712499976158142, perplexity = 7104.365234375\n",
      "Iteration 5: sparsity = 0.9712499976158142, perplexity = 6744.8251953125\n",
      "Iteration 6: sparsity = 0.9712499976158142, perplexity = 6551.541015625\n",
      "Iteration 7: sparsity = 0.9712499976158142, perplexity = 6447.40625\n",
      "Iteration 8: sparsity = 0.9712499976158142, perplexity = 6390.033203125\n",
      "Iteration 9: sparsity = 0.9712499976158142, perplexity = 6363.1015625\n",
      "Iteration 10: sparsity = 0.9712499976158142, perplexity = 6356.9326171875\n",
      "Iteration 11: sparsity = 0.9712499976158142, perplexity = 6366.00244140625\n",
      "Iteration 12: sparsity = 0.9712499976158142, perplexity = 6387.31640625\n",
      "Iteration 13: sparsity = 0.9712499976158142, perplexity = 6416.32568359375\n",
      "Iteration 14: sparsity = 0.9712499976158142, perplexity = 6449.8681640625\n",
      "Iteration 15: sparsity = 0.9712499976158142, perplexity = 6485.58203125\n",
      "Iteration 16: sparsity = 0.9725000262260437, perplexity = 6521.11376953125\n",
      "Iteration 17: sparsity = 0.9725000262260437, perplexity = 6554.57373046875\n",
      "Iteration 18: sparsity = 0.9725000262260437, perplexity = 6585.36083984375\n",
      "Iteration 19: sparsity = 0.9725000262260437, perplexity = 6613.55224609375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.76      0.54       979\n",
      "           2       0.22      0.08      0.11       458\n",
      "           3       0.21      0.11      0.15       497\n",
      "           4       0.21      0.22      0.21       537\n",
      "           7       0.32      0.20      0.25       533\n",
      "           8       0.25      0.16      0.20       560\n",
      "           9       0.25      0.12      0.16       465\n",
      "          10       0.43      0.57      0.49       971\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.29      0.28      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.36, 0.35, 0.35, 0.36, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.35, 0.36, 0.35, 0.34, 0.35, 0.34, 0.36, 0.35, 0.35, 0.35, 0.35, 0.35, 0.34, 0.35, 0.35, 0.34, 0.35, 0.35, 0.35] [0.32, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.32, 0.31, 0.31, 0.32, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.3, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.31, 0.31, 0.31]\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "f1s = []\n",
    "for n in range(1, 50):\n",
    "    prepare_augmented_train_data_for_bigartm(train_set, 'aug_train_{}.txt'.format(n), top_tokens, n, 10)\n",
    "    bv_aug_train = artm.BatchVectorizer(data_path='aug_train_{}.txt'.format(n), data_format='vowpal_wabbit', \n",
    "                                        batch_size=10000, target_folder='aug_train_{}_batches'.format(n))\n",
    "    new_model = artm.ARTM(num_topics=100, dictionary=dictionary, \n",
    "                      class_ids={'@default_class': 1.0, '@score': 5})\n",
    "    new_model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=100))\n",
    "    new_model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@score'))\n",
    "    new_model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary))\n",
    "\n",
    "    new_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_default', \n",
    "                                                               tau=10000, class_ids=['@default_class']))\n",
    "    new_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_score', \n",
    "                                                               tau=100, class_ids=['@score']))\n",
    "\n",
    "    for i in range(20):\n",
    "        new_model.fit_offline(bv_aug_train, num_collection_passes=1)\n",
    "        sparsity = new_model.score_tracker['sparsity'].value[-1]\n",
    "        perplexity = new_model.score_tracker['perplexity'].value[-1]\n",
    "        print('Iteration {}: sparsity = {}, perplexity = {}'.format(i, sparsity, perplexity))\n",
    "\n",
    "    X_train_pd = new_model.transform(bv_train)\n",
    "    X_train = np.array([X_train_pd[i].values for i in range(len(train_scores))])\n",
    "    y_train = np.array(train_scores)\n",
    "    X_val_pd = new_model.transform(bv_val)\n",
    "    X_val = np.array([X_val_pd[i].values for i in range(len(val_scores))])\n",
    "    y_val = np.array(val_scores)\n",
    "\n",
    "    classifier = RandomForestClassifier(n_estimators=50)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    y_val_pred = classifier.predict(X_val)\n",
    "    results = {}\n",
    "    results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "    results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "    \n",
    "    print(results['val'])\n",
    "    \n",
    "    spl = results['val'].split()\n",
    "    iddd = spl.index('accuracy') + 1\n",
    "    accs.append(float(spl[iddd]))\n",
    "    f1s.append(float(spl[iddd + 12]))\n",
    "    print(accs, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accs)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f1s)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зафиксируем 50 слов в документе и посмотрим на зависимость от количества документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 469305.4375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12474.373046875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9731.689453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8297.6328125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7482.9150390625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7080.81298828125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6860.64794921875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6735.201171875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6658.17529296875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6611.7451171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.16      0.05      0.08       406\n",
      "           3       0.23      0.07      0.11       493\n",
      "           4       0.26      0.25      0.26       561\n",
      "           7       0.28      0.14      0.18       506\n",
      "           8       0.28      0.14      0.19       591\n",
      "           9       0.22      0.07      0.11       431\n",
      "          10       0.41      0.66      0.51       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.32      0.37      0.31      5000\n",
      "\n",
      "[0.37] [0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468747.0625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12445.2568359375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9690.8154296875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8246.8369140625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7436.880859375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 7038.59912109375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6817.49951171875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6687.40478515625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6607.95703125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6561.10888671875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.79      0.56      1035\n",
      "           2       0.14      0.04      0.06       406\n",
      "           3       0.23      0.09      0.13       493\n",
      "           4       0.27      0.26      0.27       561\n",
      "           7       0.26      0.13      0.18       506\n",
      "           8       0.27      0.12      0.17       591\n",
      "           9       0.19      0.07      0.10       431\n",
      "          10       0.40      0.66      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.27      0.27      0.24      5000\n",
      "weighted avg       0.31      0.37      0.31      5000\n",
      "\n",
      "[0.37, 0.37] [0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 468188.0625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12417.4951171875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9647.939453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8195.00390625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7387.5390625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6989.9072265625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6770.45458984375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6642.29541015625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6566.24609375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6524.935546875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.56      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.25      0.09      0.14       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.27      0.15      0.19       506\n",
      "           8       0.25      0.13      0.17       591\n",
      "           9       0.25      0.09      0.13       431\n",
      "          10       0.40      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.29      0.27      0.25      5000\n",
      "weighted avg       0.32      0.37      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37] [0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467624.0625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12394.423828125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9607.72265625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8147.35400390625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7344.50439453125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6952.27392578125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6735.13037109375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6606.61572265625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6528.908203125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6484.82080078125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55      1035\n",
      "           2       0.18      0.08      0.11       406\n",
      "           3       0.20      0.08      0.11       493\n",
      "           4       0.28      0.28      0.28       561\n",
      "           7       0.25      0.12      0.17       506\n",
      "           8       0.27      0.14      0.18       591\n",
      "           9       0.20      0.07      0.10       431\n",
      "          10       0.40      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36] [0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 467061.75\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12369.5810546875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9567.0478515625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8094.982421875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7296.697265625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6909.423828125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6694.49169921875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6568.89111328125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6494.23291015625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6453.1435546875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.76      0.54      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.21      0.09      0.13       493\n",
      "           4       0.25      0.23      0.24       561\n",
      "           7       0.27      0.15      0.19       506\n",
      "           8       0.25      0.12      0.17       591\n",
      "           9       0.20      0.08      0.11       431\n",
      "          10       0.39      0.61      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 466502.8125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12343.1982421875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9525.8564453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 8045.9326171875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7252.7685546875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6872.85400390625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6663.9482421875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6543.24267578125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6471.08642578125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6431.46337890625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.75      0.54      1035\n",
      "           2       0.14      0.06      0.09       406\n",
      "           3       0.24      0.08      0.11       493\n",
      "           4       0.29      0.28      0.28       561\n",
      "           7       0.25      0.13      0.17       506\n",
      "           8       0.27      0.14      0.19       591\n",
      "           9       0.23      0.08      0.12       431\n",
      "          10       0.41      0.64      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 465959.34375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12323.56640625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9486.8994140625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7996.2255859375\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7205.25244140625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6829.931640625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6625.20751953125\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6506.27685546875\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6433.908203125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6392.703125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55      1035\n",
      "           2       0.15      0.05      0.08       406\n",
      "           3       0.21      0.08      0.11       493\n",
      "           4       0.26      0.27      0.27       561\n",
      "           7       0.27      0.14      0.19       506\n",
      "           8       0.28      0.15      0.20       591\n",
      "           9       0.25      0.08      0.12       431\n",
      "          10       0.41      0.64      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 465414.65625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12297.900390625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9447.453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7948.935546875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7161.08837890625\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6789.8125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6589.51416015625\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6474.51025390625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6405.44140625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6366.62255859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.78      0.55      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.22      0.08      0.11       493\n",
      "           4       0.28      0.28      0.28       561\n",
      "           7       0.25      0.13      0.17       506\n",
      "           8       0.22      0.11      0.14       591\n",
      "           9       0.27      0.09      0.13       431\n",
      "          10       0.39      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 464875.5\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12273.3798828125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9402.5048828125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7896.091796875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7115.01318359375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6749.619140625\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6554.2421875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6445.76806640625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6384.46826171875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6353.7744140625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55      1035\n",
      "           2       0.13      0.05      0.08       406\n",
      "           3       0.22      0.08      0.12       493\n",
      "           4       0.26      0.23      0.24       561\n",
      "           7       0.24      0.13      0.17       506\n",
      "           8       0.23      0.12      0.16       591\n",
      "           9       0.22      0.06      0.10       431\n",
      "          10       0.39      0.64      0.49       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.26      0.26      0.24      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 464339.96875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12248.619140625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9358.216796875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7845.650390625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7071.22216796875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6712.779296875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6521.0458984375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6415.275390625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6356.4501953125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6327.509765625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.75      0.55      1035\n",
      "           2       0.12      0.05      0.08       406\n",
      "           3       0.20      0.08      0.12       493\n",
      "           4       0.25      0.24      0.25       561\n",
      "           7       0.29      0.16      0.20       506\n",
      "           8       0.25      0.13      0.17       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.40      0.63      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.36      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 463800.09375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12226.849609375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9318.4267578125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7801.142578125\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 7032.8779296875\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6680.908203125\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6494.1435546875\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6392.25830078125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6334.919921875\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6304.94921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.75      0.54      1035\n",
      "           2       0.15      0.07      0.09       406\n",
      "           3       0.20      0.09      0.12       493\n",
      "           4       0.24      0.25      0.24       561\n",
      "           7       0.25      0.12      0.17       506\n",
      "           8       0.28      0.14      0.19       591\n",
      "           9       0.27      0.10      0.14       431\n",
      "          10       0.41      0.64      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 463267.625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12208.603515625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9281.318359375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7756.83935546875\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 6993.59521484375\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6649.84326171875\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6472.63037109375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6379.74267578125\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6329.55517578125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6304.2470703125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.73      0.54      1035\n",
      "           2       0.17      0.08      0.11       406\n",
      "           3       0.18      0.08      0.11       493\n",
      "           4       0.25      0.25      0.25       561\n",
      "           7       0.27      0.16      0.20       506\n",
      "           8       0.25      0.13      0.17       591\n",
      "           9       0.21      0.07      0.11       431\n",
      "          10       0.40      0.62      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 462737.40625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12183.6884765625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9239.3505859375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7714.11962890625\n",
      "Iteration 4: sparsity = 0.9700000286102295, perplexity = 6956.69970703125\n",
      "Iteration 5: sparsity = 0.9700000286102295, perplexity = 6617.0771484375\n",
      "Iteration 6: sparsity = 0.9700000286102295, perplexity = 6441.6484375\n",
      "Iteration 7: sparsity = 0.9700000286102295, perplexity = 6348.806640625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6298.36572265625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6272.82421875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.74      0.53      1035\n",
      "           2       0.12      0.05      0.07       406\n",
      "           3       0.24      0.10      0.14       493\n",
      "           4       0.27      0.25      0.26       561\n",
      "           7       0.25      0.14      0.18       506\n",
      "           8       0.27      0.14      0.18       591\n",
      "           9       0.20      0.07      0.10       431\n",
      "          10       0.40      0.61      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 462216.0\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12158.302734375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9196.955078125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7669.8515625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6919.474609375\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6587.25439453125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6418.3544921875\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6330.6181640625\n",
      "Iteration 8: sparsity = 0.9700000286102295, perplexity = 6284.2939453125\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6263.4091796875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.76      0.55      1035\n",
      "           2       0.13      0.05      0.08       406\n",
      "           3       0.21      0.09      0.12       493\n",
      "           4       0.24      0.24      0.24       561\n",
      "           7       0.23      0.14      0.17       506\n",
      "           8       0.27      0.14      0.19       591\n",
      "           9       0.21      0.07      0.11       431\n",
      "          10       0.40      0.62      0.49       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.26      0.24      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 461676.53125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12138.052734375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9158.2666015625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7629.97265625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6884.46923828125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6554.4267578125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6387.70556640625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6302.2529296875\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6257.37353515625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6237.8505859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.16      0.05      0.08       406\n",
      "           3       0.19      0.07      0.10       493\n",
      "           4       0.25      0.24      0.25       561\n",
      "           7       0.27      0.16      0.20       506\n",
      "           8       0.27      0.16      0.20       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.41      0.62      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 461145.75\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12113.6396484375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9120.69140625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7591.4130859375\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6849.8330078125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6522.2705078125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6357.90673828125\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6274.142578125\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6231.98486328125\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 6214.68701171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.77      0.56      1035\n",
      "           2       0.12      0.04      0.06       406\n",
      "           3       0.23      0.09      0.13       493\n",
      "           4       0.24      0.22      0.23       561\n",
      "           7       0.24      0.15      0.18       506\n",
      "           8       0.24      0.14      0.17       591\n",
      "           9       0.18      0.08      0.11       431\n",
      "          10       0.40      0.59      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.26      0.26      0.24      5000\n",
      "weighted avg       0.29      0.35      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 460637.03125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12093.4013671875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9084.3564453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7555.07421875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6819.24560546875\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6493.39306640625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6329.81103515625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6246.49853515625\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6205.322265625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6188.732421875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.75      0.55      1035\n",
      "           2       0.14      0.06      0.08       406\n",
      "           3       0.21      0.09      0.12       493\n",
      "           4       0.25      0.25      0.25       561\n",
      "           7       0.24      0.15      0.19       506\n",
      "           8       0.25      0.14      0.18       591\n",
      "           9       0.20      0.07      0.11       431\n",
      "          10       0.39      0.59      0.47       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.26      0.26      0.24      5000\n",
      "weighted avg       0.30      0.35      0.30      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 460120.625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12071.2099609375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9042.68359375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7513.5185546875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6783.25634765625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6459.79833984375\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6298.818359375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6217.2734375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6177.90087890625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6164.6748046875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.76      0.56      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.25      0.10      0.14       493\n",
      "           4       0.24      0.25      0.25       561\n",
      "           7       0.23      0.13      0.17       506\n",
      "           8       0.27      0.15      0.19       591\n",
      "           9       0.21      0.08      0.11       431\n",
      "          10       0.40      0.62      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 459605.4375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12050.2529296875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 9003.9658203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7473.63232421875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6750.0703125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6431.0673828125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6273.0771484375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6195.50390625\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6159.90771484375\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6150.546875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.76      0.55      1035\n",
      "           2       0.17      0.06      0.09       406\n",
      "           3       0.22      0.09      0.13       493\n",
      "           4       0.23      0.22      0.22       561\n",
      "           7       0.25      0.16      0.20       506\n",
      "           8       0.29      0.17      0.22       591\n",
      "           9       0.24      0.09      0.13       431\n",
      "          10       0.41      0.63      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 459089.90625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12023.57421875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8964.3505859375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7434.7802734375\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6718.13916015625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6401.90478515625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6243.8837890625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6164.818359375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6127.86572265625\n",
      "Iteration 9: sparsity = 0.9700000286102295, perplexity = 6116.1376953125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.74      0.55      1035\n",
      "           2       0.11      0.04      0.06       406\n",
      "           3       0.20      0.08      0.12       493\n",
      "           4       0.27      0.26      0.27       561\n",
      "           7       0.27      0.17      0.21       506\n",
      "           8       0.29      0.16      0.21       591\n",
      "           9       0.19      0.07      0.11       431\n",
      "          10       0.40      0.61      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 458577.125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 12001.802734375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8924.404296875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7395.5693359375\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6683.564453125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6370.36376953125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6214.38427734375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6135.685546875\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6097.9833984375\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 6084.4931640625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.75      0.54      1035\n",
      "           2       0.11      0.04      0.06       406\n",
      "           3       0.21      0.08      0.12       493\n",
      "           4       0.24      0.25      0.25       561\n",
      "           7       0.26      0.17      0.21       506\n",
      "           8       0.28      0.15      0.19       591\n",
      "           9       0.22      0.09      0.12       431\n",
      "          10       0.41      0.61      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 458066.40625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11978.2861328125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8884.11328125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7356.40771484375\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6651.337890625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6344.6591796875\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6194.38916015625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6121.4619140625\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6087.947265625\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 6076.44921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.77      0.55      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.22      0.09      0.12       493\n",
      "           4       0.27      0.24      0.26       561\n",
      "           7       0.27      0.17      0.21       506\n",
      "           8       0.28      0.16      0.21       591\n",
      "           9       0.20      0.08      0.11       431\n",
      "          10       0.41      0.60      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 457564.3125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11956.341796875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8847.634765625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7317.7451171875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6613.85107421875\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6308.77294921875\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6160.79736328125\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6090.1240234375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6057.65966796875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 6047.0224609375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.75      0.55      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.23      0.11      0.14       493\n",
      "           4       0.27      0.26      0.26       561\n",
      "           7       0.22      0.15      0.18       506\n",
      "           8       0.23      0.13      0.16       591\n",
      "           9       0.22      0.08      0.12       431\n",
      "          10       0.41      0.62      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 457062.78125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11927.279296875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8806.998046875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7280.0576171875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6583.01806640625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6282.8798828125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6138.2021484375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6069.58984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8: sparsity = 0.96875, perplexity = 6038.35986328125\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 6027.64599609375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.74      0.54      1035\n",
      "           2       0.14      0.05      0.07       406\n",
      "           3       0.23      0.12      0.16       493\n",
      "           4       0.24      0.24      0.24       561\n",
      "           7       0.23      0.15      0.18       506\n",
      "           8       0.26      0.16      0.20       591\n",
      "           9       0.20      0.08      0.12       431\n",
      "          10       0.41      0.59      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 456574.0\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11902.810546875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8766.9052734375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7240.73779296875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6548.11474609375\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6251.2578125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6109.95751953125\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6042.208984375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 6010.44189453125\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5998.83447265625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.75      0.55      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.23      0.10      0.14       493\n",
      "           4       0.28      0.25      0.26       561\n",
      "           7       0.23      0.16      0.19       506\n",
      "           8       0.27      0.15      0.19       591\n",
      "           9       0.21      0.09      0.13       431\n",
      "          10       0.41      0.60      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 456072.0625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11873.7294921875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8723.591796875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7198.0712890625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6510.71337890625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6217.78759765625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6078.974609375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 6012.1494140625\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5979.9111328125\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5967.27001953125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.75      0.55      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.25      0.11      0.15       493\n",
      "           4       0.28      0.24      0.26       561\n",
      "           7       0.27      0.19      0.22       506\n",
      "           8       0.28      0.17      0.21       591\n",
      "           9       0.21      0.09      0.13       431\n",
      "          10       0.42      0.62      0.50       977\n",
      "\n",
      "    accuracy                           0.37      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.32      0.37      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 455570.5\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11843.681640625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8679.8486328125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7154.34814453125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6472.38623046875\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6182.60205078125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6043.94580078125\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5975.3583984375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5940.28076171875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5925.18408203125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.73      0.54      1035\n",
      "           2       0.17      0.08      0.11       406\n",
      "           3       0.25      0.11      0.15       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.28      0.18      0.22       506\n",
      "           8       0.28      0.18      0.22       591\n",
      "           9       0.23      0.10      0.14       431\n",
      "          10       0.41      0.60      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.26      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 455082.0625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11819.2060546875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8640.3818359375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7115.783203125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6439.4228515625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6153.4453125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 6016.2197265625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5948.13818359375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5912.2255859375\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5895.052734375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.74      0.54      1035\n",
      "           2       0.16      0.06      0.09       406\n",
      "           3       0.21      0.11      0.14       493\n",
      "           4       0.28      0.23      0.25       561\n",
      "           7       0.24      0.17      0.20       506\n",
      "           8       0.27      0.18      0.21       591\n",
      "           9       0.17      0.07      0.10       431\n",
      "          10       0.41      0.60      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 454598.625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11793.2119140625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8601.505859375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7076.17822265625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6404.689453125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6123.37353515625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5989.98486328125\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5925.2216796875\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5892.31982421875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5876.4912109375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.74      0.54      1035\n",
      "           2       0.17      0.09      0.12       406\n",
      "           3       0.24      0.11      0.15       493\n",
      "           4       0.26      0.22      0.23       561\n",
      "           7       0.22      0.15      0.18       506\n",
      "           8       0.23      0.15      0.18       591\n",
      "           9       0.20      0.07      0.10       431\n",
      "          10       0.40      0.59      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.26      0.25      5000\n",
      "weighted avg       0.30      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 454114.125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11766.0205078125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8561.689453125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7039.689453125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6373.39501953125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6095.3291015625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5963.7568359375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5899.953125\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5867.24658203125\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5851.35009765625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.73      0.54      1035\n",
      "           2       0.16      0.08      0.10       406\n",
      "           3       0.25      0.13      0.17       493\n",
      "           4       0.26      0.20      0.23       561\n",
      "           7       0.27      0.16      0.20       506\n",
      "           8       0.23      0.16      0.19       591\n",
      "           9       0.19      0.09      0.12       431\n",
      "          10       0.41      0.60      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 453635.65625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11746.51171875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8525.4404296875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 7003.5439453125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6342.01123046875\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6066.919921875\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5936.93994140625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5873.68505859375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5841.5146484375\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5826.15087890625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.75      0.55      1035\n",
      "           2       0.17      0.07      0.10       406\n",
      "           3       0.26      0.12      0.17       493\n",
      "           4       0.28      0.24      0.26       561\n",
      "           7       0.25      0.17      0.20       506\n",
      "           8       0.25      0.17      0.20       591\n",
      "           9       0.18      0.08      0.11       431\n",
      "          10       0.42      0.59      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 453159.09375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11717.9052734375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8483.4208984375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6960.3798828125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6299.97900390625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 6024.74365234375\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5895.3349609375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5832.51025390625\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5800.544921875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5784.5869140625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.74      0.55      1035\n",
      "           2       0.17      0.08      0.10       406\n",
      "           3       0.25      0.11      0.15       493\n",
      "           4       0.27      0.22      0.24       561\n",
      "           7       0.26      0.18      0.21       506\n",
      "           8       0.25      0.18      0.21       591\n",
      "           9       0.18      0.09      0.12       431\n",
      "          10       0.41      0.59      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 452672.6875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11692.7255859375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8442.94921875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6920.08056640625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6263.5068359375\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5991.02587890625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5863.34716796875\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5801.38330078125\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5769.3154296875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5752.04541015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.74      0.54      1035\n",
      "           2       0.15      0.06      0.09       406\n",
      "           3       0.26      0.12      0.16       493\n",
      "           4       0.27      0.23      0.25       561\n",
      "           7       0.22      0.16      0.19       506\n",
      "           8       0.25      0.17      0.20       591\n",
      "           9       0.17      0.08      0.11       431\n",
      "          10       0.43      0.61      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 452198.46875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11668.9765625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8403.09765625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6878.8193359375\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6226.0927734375\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5958.50830078125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5834.02099609375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5773.8916015625\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5742.84619140625\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5726.9267578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.75      0.55      1035\n",
      "           2       0.15      0.06      0.08       406\n",
      "           3       0.26      0.13      0.17       493\n",
      "           4       0.26      0.19      0.22       561\n",
      "           7       0.22      0.15      0.18       506\n",
      "           8       0.27      0.19      0.23       591\n",
      "           9       0.19      0.09      0.12       431\n",
      "          10       0.42      0.60      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 451725.375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11641.4482421875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8361.77734375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6838.06884765625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6188.80078125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5923.47900390625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5799.525390625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5739.48046875\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5708.4677734375\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5692.89599609375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.41      0.72      0.52      1035\n",
      "           2       0.16      0.07      0.10       406\n",
      "           3       0.26      0.12      0.17       493\n",
      "           4       0.25      0.17      0.20       561\n",
      "           7       0.22      0.15      0.18       506\n",
      "           8       0.26      0.18      0.21       591\n",
      "           9       0.24      0.10      0.14       431\n",
      "          10       0.41      0.60      0.49       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.30      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 451259.53125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11612.2646484375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8324.0146484375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6801.56396484375\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6155.29150390625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5892.166015625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5769.45068359375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5710.421875\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5679.787109375\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5663.52490234375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.72      0.54      1035\n",
      "           2       0.18      0.08      0.11       406\n",
      "           3       0.26      0.13      0.17       493\n",
      "           4       0.25      0.21      0.23       561\n",
      "           7       0.25      0.19      0.22       506\n",
      "           8       0.27      0.19      0.23       591\n",
      "           9       0.19      0.08      0.11       431\n",
      "          10       0.41      0.59      0.48       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 450778.4375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11580.4658203125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8285.7841796875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6767.52001953125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6126.6767578125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5866.73583984375\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5745.791015625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5687.658203125\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5657.36279296875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5641.58642578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.73      0.53      1035\n",
      "           2       0.17      0.08      0.11       406\n",
      "           3       0.28      0.13      0.18       493\n",
      "           4       0.27      0.19      0.22       561\n",
      "           7       0.25      0.19      0.21       506\n",
      "           8       0.26      0.18      0.21       591\n",
      "           9       0.21      0.09      0.12       431\n",
      "          10       0.42      0.61      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 450303.3125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11551.3203125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8245.1083984375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6728.75830078125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6092.2939453125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5833.24951171875\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5711.83984375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5652.3916015625\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5620.69384765625\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5603.7001953125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.73      0.54      1035\n",
      "           2       0.17      0.08      0.11       406\n",
      "           3       0.26      0.12      0.16       493\n",
      "           4       0.25      0.19      0.21       561\n",
      "           7       0.23      0.17      0.20       506\n",
      "           8       0.26      0.16      0.19       591\n",
      "           9       0.20      0.09      0.12       431\n",
      "          10       0.41      0.62      0.49       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.25      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 449833.375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11521.0625\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8204.26953125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6689.89111328125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6057.27783203125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5799.96337890625\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5679.62841796875\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5621.4716796875\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5590.75634765625\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5574.81005859375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.73      0.54      1035\n",
      "           2       0.14      0.06      0.09       406\n",
      "           3       0.25      0.11      0.15       493\n",
      "           4       0.25      0.19      0.21       561\n",
      "           7       0.24      0.18      0.21       506\n",
      "           8       0.25      0.18      0.21       591\n",
      "           9       0.18      0.07      0.11       431\n",
      "          10       0.42      0.61      0.50       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 449378.46875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11494.4208984375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8167.6708984375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6654.71142578125\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 6024.40478515625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5767.43505859375\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5646.42822265625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5588.04052734375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5557.521484375\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5542.0888671875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.72      0.54      1035\n",
      "           2       0.19      0.09      0.12       406\n",
      "           3       0.25      0.12      0.16       493\n",
      "           4       0.24      0.19      0.21       561\n",
      "           7       0.23      0.16      0.19       506\n",
      "           8       0.25      0.18      0.21       591\n",
      "           9       0.20      0.09      0.13       431\n",
      "          10       0.42      0.61      0.50       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 448921.75\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11471.3115234375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8131.58203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6619.5087890625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5991.72216796875\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5735.69091796875\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5614.4765625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5556.1845703125\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5526.1796875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5510.5791015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.73      0.54      1035\n",
      "           2       0.18      0.09      0.12       406\n",
      "           3       0.25      0.12      0.16       493\n",
      "           4       0.27      0.17      0.21       561\n",
      "           7       0.24      0.18      0.20       506\n",
      "           8       0.24      0.17      0.20       591\n",
      "           9       0.18      0.08      0.11       431\n",
      "          10       0.41      0.60      0.49       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 448461.09375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11442.703125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8094.65283203125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6584.5244140625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5959.6884765625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5703.0830078125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5580.2890625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5519.34033203125\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5486.658203125\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5468.873046875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.54      1035\n",
      "           2       0.16      0.07      0.10       406\n",
      "           3       0.22      0.10      0.14       493\n",
      "           4       0.25      0.19      0.21       561\n",
      "           7       0.24      0.17      0.20       506\n",
      "           8       0.26      0.19      0.22       591\n",
      "           9       0.22      0.09      0.13       431\n",
      "          10       0.43      0.61      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.27      0.27      0.26      5000\n",
      "weighted avg       0.31      0.36      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 448004.09375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11411.7783203125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8051.0771484375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6542.1123046875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5920.1630859375\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5663.77001953125\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5540.462890625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5478.41552734375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5444.87060546875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5426.61865234375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.73      0.55      1035\n",
      "           2       0.20      0.09      0.13       406\n",
      "           3       0.26      0.13      0.18       493\n",
      "           4       0.25      0.18      0.21       561\n",
      "           7       0.25      0.18      0.21       506\n",
      "           8       0.22      0.15      0.18       591\n",
      "           9       0.22      0.10      0.14       431\n",
      "          10       0.41      0.61      0.49       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.36      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35, 0.36, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 447547.1875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11380.888671875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 8008.99169921875\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6500.11474609375\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5880.63525390625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5624.43701171875\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5499.65576171875\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5434.70458984375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5397.47021484375\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5377.18798828125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.73      0.54      1035\n",
      "           2       0.17      0.08      0.11       406\n",
      "           3       0.22      0.09      0.13       493\n",
      "           4       0.28      0.20      0.24       561\n",
      "           7       0.25      0.19      0.22       506\n",
      "           8       0.24      0.17      0.20       591\n",
      "           9       0.19      0.10      0.13       431\n",
      "          10       0.41      0.59      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35, 0.36, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.31]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 447096.28125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11357.0751953125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 7971.79345703125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6464.669921875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5848.89990234375\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5594.86279296875\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5471.2890625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5407.08203125\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5370.72705078125\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5351.54443359375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.74      0.54      1035\n",
      "           2       0.19      0.09      0.12       406\n",
      "           3       0.24      0.11      0.15       493\n",
      "           4       0.29      0.22      0.25       561\n",
      "           7       0.22      0.16      0.19       506\n",
      "           8       0.25      0.17      0.20       591\n",
      "           9       0.25      0.11      0.15       431\n",
      "          10       0.43      0.61      0.50       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.29      0.28      0.26      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35, 0.36, 0.36, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.31, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 446640.125\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11324.1171875\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 7928.4423828125\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6424.21826171875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5810.94921875\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5557.865234375\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5434.95068359375\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5370.76904296875\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5335.9453125\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5317.87841796875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.71      0.53      1035\n",
      "           2       0.20      0.10      0.13       406\n",
      "           3       0.25      0.12      0.16       493\n",
      "           4       0.26      0.19      0.22       561\n",
      "           7       0.25      0.17      0.20       506\n",
      "           8       0.26      0.19      0.22       591\n",
      "           9       0.20      0.10      0.14       431\n",
      "          10       0.42      0.60      0.49       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.35      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35, 0.36, 0.36, 0.35, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 446186.59375\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11296.27734375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 7890.18603515625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6387.47607421875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5776.267578125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5523.7841796875\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5402.0166015625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5339.94287109375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5306.771484375\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5289.42578125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.73      0.53      1035\n",
      "           2       0.19      0.09      0.12       406\n",
      "           3       0.24      0.10      0.14       493\n",
      "           4       0.28      0.21      0.24       561\n",
      "           7       0.25      0.19      0.21       506\n",
      "           8       0.26      0.19      0.22       591\n",
      "           9       0.19      0.08      0.12       431\n",
      "          10       0.43      0.62      0.51       977\n",
      "\n",
      "    accuracy                           0.36      5000\n",
      "   macro avg       0.28      0.28      0.26      5000\n",
      "weighted avg       0.32      0.36      0.32      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35, 0.36, 0.36, 0.35, 0.36, 0.35, 0.36] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 445748.6875\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11272.958984375\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 7856.15869140625\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6356.232421875\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5748.28173828125\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5496.83349609375\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5375.38916015625\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5313.423828125\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5280.390625\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5263.08349609375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.72      0.53      1035\n",
      "           2       0.19      0.09      0.13       406\n",
      "           3       0.24      0.11      0.15       493\n",
      "           4       0.26      0.18      0.21       561\n",
      "           7       0.23      0.19      0.21       506\n",
      "           8       0.24      0.17      0.20       591\n",
      "           9       0.17      0.07      0.10       431\n",
      "          10       0.41      0.58      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.27      0.27      0.25      5000\n",
      "weighted avg       0.30      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35, 0.36, 0.36, 0.35, 0.36, 0.35, 0.36, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.31]\n",
      "Iteration 0: sparsity = 0.8525000214576721, perplexity = 445302.5625\n",
      "Iteration 1: sparsity = 0.9649999737739563, perplexity = 11243.83203125\n",
      "Iteration 2: sparsity = 0.96875, perplexity = 7819.80615234375\n",
      "Iteration 3: sparsity = 0.96875, perplexity = 6323.853515625\n",
      "Iteration 4: sparsity = 0.96875, perplexity = 5720.33837890625\n",
      "Iteration 5: sparsity = 0.96875, perplexity = 5471.787109375\n",
      "Iteration 6: sparsity = 0.96875, perplexity = 5352.0361328125\n",
      "Iteration 7: sparsity = 0.96875, perplexity = 5291.04833984375\n",
      "Iteration 8: sparsity = 0.96875, perplexity = 5258.0546875\n",
      "Iteration 9: sparsity = 0.96875, perplexity = 5241.6416015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.73      0.54      1035\n",
      "           2       0.20      0.10      0.13       406\n",
      "           3       0.26      0.13      0.17       493\n",
      "           4       0.28      0.20      0.23       561\n",
      "           7       0.25      0.18      0.21       506\n",
      "           8       0.24      0.17      0.20       591\n",
      "           9       0.18      0.08      0.11       431\n",
      "          10       0.41      0.59      0.48       977\n",
      "\n",
      "    accuracy                           0.35      5000\n",
      "   macro avg       0.28      0.27      0.26      5000\n",
      "weighted avg       0.31      0.35      0.31      5000\n",
      "\n",
      "[0.37, 0.37, 0.37, 0.36, 0.35, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.37, 0.36, 0.36, 0.35, 0.35, 0.36, 0.36, 0.36, 0.36, 0.35, 0.36, 0.36, 0.35, 0.35, 0.35, 0.35, 0.36, 0.36, 0.35, 0.36, 0.35, 0.36, 0.35, 0.35] [0.31, 0.31, 0.31, 0.31, 0.3, 0.31, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.3, 0.31, 0.3, 0.3, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.32, 0.32, 0.31, 0.31, 0.31, 0.31, 0.31, 0.32, 0.31, 0.32, 0.32, 0.32, 0.31, 0.31]\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "f1s = []\n",
    "for n in range(1, 50):\n",
    "    prepare_augmented_train_data_for_bigartm(train_set, 'aug_train_{}.txt'.format(n), top_tokens, 50, n)\n",
    "    bv_aug_train = artm.BatchVectorizer(data_path='aug_train_{}.txt'.format(n), data_format='vowpal_wabbit', \n",
    "                                        batch_size=10000, target_folder='aug_train_{}_batches'.format(n))\n",
    "    new_model = artm.ARTM(num_topics=100, dictionary=dictionary, \n",
    "                      class_ids={'@default_class': 1.0, '@score': 5})\n",
    "    new_model.scores.add(artm.TopTokensScore(name='top-tokens', num_tokens=100))\n",
    "    new_model.scores.add(artm.SparsityPhiScore(name='sparsity', class_id='@score'))\n",
    "    new_model.scores.add(artm.PerplexityScore(name='perplexity', dictionary=dictionary))\n",
    "\n",
    "    new_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_default', \n",
    "                                                               tau=10000, class_ids=['@default_class']))\n",
    "    new_model.regularizers.add(artm.DecorrelatorPhiRegularizer(name='decorrelator_phi_score', \n",
    "                                                               tau=100, class_ids=['@score']))\n",
    "\n",
    "    for i in range(10):\n",
    "        new_model.fit_offline(bv_aug_train, num_collection_passes=1)\n",
    "        sparsity = new_model.score_tracker['sparsity'].value[-1]\n",
    "        perplexity = new_model.score_tracker['perplexity'].value[-1]\n",
    "        print('Iteration {}: sparsity = {}, perplexity = {}'.format(i, sparsity, perplexity))\n",
    "\n",
    "    X_train_pd = new_model.transform(bv_train)\n",
    "    X_train = np.array([X_train_pd[i].values for i in range(len(train_scores))])\n",
    "    y_train = np.array(train_scores)\n",
    "    X_val_pd = new_model.transform(bv_val)\n",
    "    X_val = np.array([X_val_pd[i].values for i in range(len(val_scores))])\n",
    "    y_val = np.array(val_scores)\n",
    "\n",
    "    classifier = RandomForestClassifier(n_estimators=50)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_train_pred = classifier.predict(X_train)\n",
    "    y_val_pred = classifier.predict(X_val)\n",
    "    results = {}\n",
    "    results['train'] = classification_report(y_train, y_train_pred, zero_division=1)\n",
    "    results['val'] = classification_report(y_val, y_val_pred, zero_division=1)\n",
    "    \n",
    "    print(results['val'])\n",
    "    \n",
    "    spl = results['val'].split()\n",
    "    iddd = spl.index('accuracy') + 1\n",
    "    accs.append(float(spl[iddd]))\n",
    "    f1s.append(float(spl[iddd + 12]))\n",
    "    print(accs, f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accs)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(f1s)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.score_tracker['sparsity'].value)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff6d4e9c460>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log(model.score_tracker['perplexity'].value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
